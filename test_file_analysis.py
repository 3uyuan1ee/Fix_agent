#!/usr/bin/env python3
"""
æ–‡ä»¶åˆ†æå’Œç»“æœæ ¼å¼åŒ–åŠŸèƒ½æµ‹è¯•è„šæœ¬
éªŒè¯æ·±åº¦åˆ†æä¸­çš„æ–‡ä»¶è¯»å–ã€åˆ†æå’Œç»“æœè¾“å‡ºåŠŸèƒ½
"""

import sys
import os
import json
import asyncio
import tempfile
from pathlib import Path
from unittest.mock import Mock, patch, AsyncMock
from datetime import datetime

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent / "src"))

def test_file_reading_capabilities():
    """æµ‹è¯•æ–‡ä»¶è¯»å–èƒ½åŠ›"""
    print("ğŸ” æµ‹è¯•æ–‡ä»¶è¯»å–èƒ½åŠ›...")

    try:
        # åˆ›å»ºæµ‹è¯•æ–‡ä»¶
        temp_dir = tempfile.mkdtemp()
        test_files = {
            "simple.py": '''
def hello_world():
    """ç®€å•å‡½æ•°"""
    return "Hello, World!"

class TestClass:
    def __init__(self, name):
        self.name = name

    def greet(self):
        return f"Hello, {self.name}!"
''',
            "complex.js": '''
/**
 * å¤æ‚JavaScriptç±»
 */
class ComplexCalculator {
    constructor(initialValue = 0) {
        this.value = initialValue;
        this.history = [];
    }

    add(number) {
        this.history.push({operation: 'add', value: number, result: this.value + number});
        this.value += number;
        return this;
    }

    multiply(number) {
        this.history.push({operation: 'multiply', value: number, result: this.value * number});
        this.value *= number;
        return this;
    }

    async calculateAsync(operation, ...args) {
        await new Promise(resolve => setTimeout(resolve, 100));
        return this[operation](...args);
    }
}
''',
            "data.json": '''
{
    "name": "æµ‹è¯•é¡¹ç›®",
    "version": "1.0.0",
    "dependencies": {
        "lodash": "^4.17.21",
        "express": "^4.18.0"
    },
    "scripts": {
        "start": "node index.js",
        "test": "jest"
    }
}
''',
            "config.yaml": '''
# åº”ç”¨é…ç½®
app:
  name: "æµ‹è¯•åº”ç”¨"
  port: 8080
  debug: true

database:
  host: "localhost"
  port: 5432
  name: "testdb"

logging:
  level: "info"
  file: "app.log"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
'''
        }

        # å†™å…¥æµ‹è¯•æ–‡ä»¶
        created_files = []
        for filename, content in test_files.items():
            file_path = Path(temp_dir) / filename
            file_path.write_text(content.strip())
            created_files.append(str(file_path))

        print(f"   - åˆ›å»ºæµ‹è¯•æ–‡ä»¶: {len(created_files)} ä¸ª")

        # æµ‹è¯•æ–‡ä»¶è¯»å–åŠŸèƒ½
        read_results = []
        for file_path in created_files:
            try:
                file_path_obj = Path(file_path)
                if file_path_obj.exists():
                    content = file_path_obj.read_text(encoding='utf-8')
                    file_info = {
                        'path': file_path,
                        'size': len(content),
                        'lines': len(content.splitlines()),
                        'extension': file_path_obj.suffix,
                        'readable': True
                    }
                    read_results.append(file_info)
                    print(f"     âœ… {file_path_obj.name}: {file_info['lines']} è¡Œ, {file_info['size']} å­—ç¬¦")
                else:
                    print(f"     âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            except Exception as e:
                print(f"     âŒ è¯»å–å¤±è´¥ {file_path}: {e}")

        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        import shutil
        shutil.rmtree(temp_dir)

        success = len(read_results) == len(created_files)
        print(f"   - æ–‡ä»¶è¯»å–æˆåŠŸç‡: {len(read_results)}/{len(created_files)} ({len(read_results)/len(created_files):.1%})")

        return success

    except Exception as e:
        print(f"âŒ æ–‡ä»¶è¯»å–èƒ½åŠ›æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_file_analysis_functionality():
    """æµ‹è¯•æ–‡ä»¶åˆ†æåŠŸèƒ½"""
    print("\nğŸ” æµ‹è¯•æ–‡ä»¶åˆ†æåŠŸèƒ½...")

    try:
        # æ¨¡æ‹Ÿæ–‡ä»¶åˆ†æå™¨
        class MockFileAnalyzer:
            def __init__(self):
                self.analysis_patterns = {
                    'function_count': r'def\s+(\w+)',
                    'class_count': r'class\s+(\w+)',
                    'import_count': r'import\s+\w+|from\s+\w+\s+import',
                    'comment_ratio': r'#.*|/\*.*?\*/|//.*',
                    'complexity_indicators': r'if\s+.*else|for\s+.*in|while\s+.*|try\s*:.*except'
                }

            def analyze_file(self, file_path: str) -> dict:
                """åˆ†æå•ä¸ªæ–‡ä»¶"""
                try:
                    content = Path(file_path).read_text(encoding='utf-8')
                    lines = content.splitlines()

                    analysis = {
                        'file_path': file_path,
                        'file_size': len(content),
                        'line_count': len(lines),
                        'extension': Path(file_path).suffix,
                        'analysis_timestamp': datetime.now().isoformat(),
                        'metrics': {
                            'functions': len([line for line in lines if 'def ' in line]),
                            'classes': len([line for line in lines if 'class ' in line]),
                            'imports': len([line for line in lines if 'import ' in line or 'from ' in line]),
                            'comment_lines': len([line for line in lines if line.strip().startswith('#')]),
                            'blank_lines': len([line for line in lines if not line.strip()]),
                            'avg_line_length': sum(len(line) for line in lines) / len(lines) if lines else 0
                        },
                        'complexity': 'low',
                        'quality_score': 8.5,
                        'suggestions': []
                    }

                    # è®¡ç®—å¤æ‚åº¦
                    if analysis['metrics']['functions'] > 10 or analysis['metrics']['classes'] > 5:
                        analysis['complexity'] = 'high'
                    elif analysis['metrics']['functions'] > 5 or analysis['metrics']['classes'] > 3:
                        analysis['complexity'] = 'medium'

                    # ç”Ÿæˆå»ºè®®
                    if analysis['metrics']['comment_lines'] / analysis['line_count'] < 0.1:
                        analysis['suggestions'].append("å»ºè®®å¢åŠ æ³¨é‡Šä»¥æé«˜ä»£ç å¯è¯»æ€§")

                    if analysis['metrics']['avg_line_length'] > 100:
                        analysis['suggestions'].append("å»ºè®®ç¼©çŸ­è¿‡é•¿çš„ä»£ç è¡Œ")

                    return analysis

                except Exception as e:
                    return {
                        'file_path': file_path,
                        'error': str(e),
                        'analysis_timestamp': datetime.now().isoformat()
                    }

        # åˆ›å»ºæµ‹è¯•æ–‡ä»¶
        temp_dir = tempfile.mkdtemp()
        test_code = '''
def calculate_total(items):
    """è®¡ç®—æ€»ä»·"""
    total = 0
    for item in items:
        if item['price'] > 0:
            total += item['price'] * item.get('quantity', 1)
        else:
            raise ValueError("ä»·æ ¼ä¸èƒ½ä¸ºè´Ÿæ•°")
    return total

class ShoppingCart:
    def __init__(self):
        self.items = []
        self.total = 0

    def add_item(self, name, price, quantity=1):
        self.items.append({
            'name': name,
            'price': price,
            'quantity': quantity
        })

    def calculate_total(self):
        self.total = calculate_total(self.items)
        return self.total
'''

        test_file = Path(temp_dir) / "test_ecommerce.py"
        test_file.write_text(test_code.strip())

        # æ‰§è¡Œåˆ†æ
        analyzer = MockFileAnalyzer()
        analysis_result = analyzer.analyze_file(str(test_file))

        print("   åˆ†æç»“æœ:")
        if 'error' not in analysis_result:
            print(f"     - æ–‡ä»¶: {Path(analysis_result['file_path']).name}")
            print(f"     - å¤§å°: {analysis_result['file_size']} å­—ç¬¦")
            print(f"     - è¡Œæ•°: {analysis_result['line_count']}")
            print(f"     - å‡½æ•°æ•°: {analysis_result['metrics']['functions']}")
            print(f"     - ç±»æ•°: {analysis_result['metrics']['classes']}")
            print(f"     - å¤æ‚åº¦: {analysis_result['complexity']}")
            print(f"     - è´¨é‡åˆ†: {analysis_result['quality_score']}")
            print(f"     - å»ºè®®æ•°: {len(analysis_result['suggestions'])}")

            analysis_valid = (
                analysis_result['metrics']['functions'] >= 2 and
                analysis_result['metrics']['classes'] >= 1 and
                analysis_result['complexity'] in ['low', 'medium', 'high'] and
                isinstance(analysis_result['quality_score'], (int, float))
            )
            print(f"     - åˆ†ææœ‰æ•ˆæ€§: {'âœ…' if analysis_valid else 'âŒ'}")
        else:
            print(f"     - åˆ†æé”™è¯¯: {analysis_result['error']}")
            analysis_valid = False

        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        import shutil
        shutil.rmtree(temp_dir)

        return analysis_valid

    except Exception as e:
        print(f"âŒ æ–‡ä»¶åˆ†æåŠŸèƒ½æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_result_formatting():
    """æµ‹è¯•ç»“æœæ ¼å¼åŒ–åŠŸèƒ½"""
    print("\nğŸ” æµ‹è¯•ç»“æœæ ¼å¼åŒ–åŠŸèƒ½...")

    try:
        # æ¨¡æ‹Ÿåˆ†æç»“æœ
        mock_analysis_result = {
            'file_path': '/test/sample.py',
            'file_size': 2048,
            'line_count': 85,
            'extension': '.py',
            'analysis_timestamp': datetime.now().isoformat(),
            'metrics': {
                'functions': 6,
                'classes': 3,
                'imports': 8,
                'comment_lines': 15,
                'blank_lines': 12,
                'avg_line_length': 24.5
            },
            'complexity': 'medium',
            'quality_score': 7.8,
            'issues': [
                {
                    'type': 'style',
                    'severity': 'info',
                    'line': 15,
                    'message': 'å»ºè®®æ·»åŠ ç±»å‹æ³¨è§£',
                    'suggestion': 'ä¸ºå‡½æ•°å‚æ•°å’Œè¿”å›å€¼æ·»åŠ ç±»å‹æç¤º'
                },
                {
                    'type': 'performance',
                    'severity': 'warning',
                    'line': 32,
                    'message': 'å¾ªç¯ä¸­çš„é‡å¤è®¡ç®—',
                    'suggestion': 'å°†é‡å¤è®¡ç®—æå–åˆ°å¾ªç¯å¤–éƒ¨'
                },
                {
                    'type': 'security',
                    'severity': 'error',
                    'line': 48,
                    'message': 'æ½œåœ¨SQLæ³¨å…¥é£é™©',
                    'suggestion': 'ä½¿ç”¨å‚æ•°åŒ–æŸ¥è¯¢æ›¿ä»£å­—ç¬¦ä¸²æ‹¼æ¥'
                }
            ],
            'recommendations': [
                'å¢åŠ å•å…ƒæµ‹è¯•è¦†ç›–ç‡',
                'ä¼˜åŒ–æ•°æ®åº“æŸ¥è¯¢æ€§èƒ½',
                'æ”¹è¿›é”™è¯¯å¤„ç†æœºåˆ¶'
            ],
            'suggestions': [
                'å»ºè®®å¢åŠ æ³¨é‡Šä»¥æé«˜ä»£ç å¯è¯»æ€§',
                'è€ƒè™‘ä½¿ç”¨ç±»å‹æç¤ºå¢å¼ºä»£ç å¥å£®æ€§'
            ]
        }

        # æµ‹è¯•ä¸åŒæ ¼å¼åŒ–å™¨
        class ResultFormatter:
            def format_as_json(self, result: dict) -> str:
                """JSONæ ¼å¼åŒ–"""
                return json.dumps(result, indent=2, ensure_ascii=False)

            def format_as_markdown(self, result: dict) -> str:
                """Markdownæ ¼å¼åŒ–"""
                md = f"""# ä»£ç åˆ†ææŠ¥å‘Š

## æ–‡ä»¶ä¿¡æ¯
- **æ–‡ä»¶è·¯å¾„**: {result['file_path']}
- **æ–‡ä»¶å¤§å°**: {result['file_size']} å­—ç¬¦
- **ä»£ç è¡Œæ•°**: {result['line_count']} è¡Œ
- **åˆ†ææ—¶é—´**: {result['analysis_timestamp']}

## ä»£ç åº¦é‡
- **å‡½æ•°æ•°é‡**: {result['metrics']['functions']}
- **ç±»æ•°é‡**: {result['metrics']['classes']}
- **å¯¼å…¥æ•°é‡**: {result['metrics']['imports']}
- **æ³¨é‡Šè¡Œæ•°**: {result['metrics']['comment_lines']}
- **ç©ºç™½è¡Œæ•°**: {result['metrics']['blank_lines']}
- **å¹³å‡è¡Œé•¿åº¦**: {result['metrics']['avg_line_length']:.1f}

## è´¨é‡è¯„ä¼°
- **å¤æ‚åº¦**: {result['complexity']}
- **è´¨é‡è¯„åˆ†**: {result['quality_score']}/10

## å‘ç°çš„é—®é¢˜
"""
                for i, issue in enumerate(result['issues'], 1):
                    severity_emoji = {'error': 'ğŸš«', 'warning': 'âš ï¸', 'info': 'â„¹ï¸'}
                    md += f"""
### {i}. {issue['message']}
- **ç±»å‹**: {issue['type']}
- **ä¸¥é‡ç¨‹åº¦**: {severity_emoji.get(issue['severity'], 'â“')} {issue['severity']}
- **è¡Œå·**: {issue['line']}
- **å»ºè®®**: {issue['suggestion']}
"""

                md += f"""
## æ”¹è¿›å»ºè®®
"""
                for rec in result['recommendations']:
                    md += f"- {rec}\n"

                return md

            def format_as_text(self, result: dict) -> str:
                """çº¯æ–‡æœ¬æ ¼å¼åŒ–"""
                text = f"""ä»£ç åˆ†ææŠ¥å‘Š
{'='*50}

æ–‡ä»¶ä¿¡æ¯:
  è·¯å¾„: {result['file_path']}
  å¤§å°: {result['file_size']} å­—ç¬¦
  è¡Œæ•°: {result['line_count']} è¡Œ
  åˆ†ææ—¶é—´: {result['analysis_timestamp']}

ä»£ç åº¦é‡:
  å‡½æ•°æ•°é‡: {result['metrics']['functions']}
  ç±»æ•°é‡: {result['metrics']['classes']}
  å¯¼å…¥æ•°é‡: {result['metrics']['imports']}
  æ³¨é‡Šè¡Œæ•°: {result['metrics']['comment_lines']}
  ç©ºç™½è¡Œæ•°: {result['metrics']['blank_lines']}
  å¹³å‡è¡Œé•¿åº¦: {result['metrics']['avg_line_length']:.1f}

è´¨é‡è¯„ä¼°:
  å¤æ‚åº¦: {result['complexity']}
  è´¨é‡è¯„åˆ†: {result['quality_score']}/10

å‘ç°çš„é—®é¢˜:
"""
                for i, issue in enumerate(result['issues'], 1):
                    text += f"""
  {i}. {issue['message']} ({issue['severity']})
     è¡Œå·: {issue['line']}
     å»ºè®®: {issue['suggestion']}
"""

                text += "\næ”¹è¿›å»ºè®®:\n"
                for rec in result['recommendations']:
                    text += f"  - {rec}\n"

                return text

        # æ‰§è¡Œæ ¼å¼åŒ–æµ‹è¯•
        formatter = ResultFormatter()

        # æµ‹è¯•JSONæ ¼å¼åŒ–
        json_output = formatter.format_as_json(mock_analysis_result)
        json_valid = len(json_output) > 100 and '"file_path"' in json_output
        print(f"   - JSONæ ¼å¼åŒ–: {'âœ…' if json_valid else 'âŒ'} ({len(json_output)} å­—ç¬¦)")

        # æµ‹è¯•Markdownæ ¼å¼åŒ–
        md_output = formatter.format_as_markdown(mock_analysis_result)
        md_valid = len(md_output) > 200 and '##' in md_output and '**' in md_output
        print(f"   - Markdownæ ¼å¼åŒ–: {'âœ…' if md_valid else 'âŒ'} ({len(md_output)} å­—ç¬¦)")

        # æµ‹è¯•æ–‡æœ¬æ ¼å¼åŒ–
        text_output = formatter.format_as_text(mock_analysis_result)
        text_valid = len(text_output) > 200 and 'ä»£ç åˆ†ææŠ¥å‘Š' in text_output
        print(f"   - æ–‡æœ¬æ ¼å¼åŒ–: {'âœ…' if text_valid else 'âŒ'} ({len(text_output)} å­—ç¬¦)")

        # æµ‹è¯•æ ¼å¼åŒ–ä¸€è‡´æ€§
        formats_consistent = all([
            json_valid,
            md_valid,
            text_valid,
            'sample.py' in json_output and 'sample.py' in md_output and 'sample.py' in text_output
        ])
        print(f"   - æ ¼å¼ä¸€è‡´æ€§: {'âœ…' if formats_consistent else 'âŒ'}")

        return formats_consistent

    except Exception as e:
        print(f"âŒ ç»“æœæ ¼å¼åŒ–åŠŸèƒ½æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_analysis_workflow():
    """æµ‹è¯•å®Œæ•´åˆ†æå·¥ä½œæµ"""
    print("\nğŸ” æµ‹è¯•å®Œæ•´åˆ†æå·¥ä½œæµ...")

    try:
        # æ¨¡æ‹Ÿå®Œæ•´çš„åˆ†æå·¥ä½œæµ
        class AnalysisWorkflow:
            def __init__(self):
                self.supported_extensions = ['.py', '.js', '.ts', '.java', '.cpp', '.json', '.yaml', '.yml']
                self.max_file_size = 1024 * 1024  # 1MB

            def validate_file(self, file_path: str) -> dict:
                """éªŒè¯æ–‡ä»¶æ˜¯å¦é€‚åˆåˆ†æ"""
                path = Path(file_path)
                validation = {
                    'valid': True,
                    'reasons': [],
                    'file_info': {
                        'exists': path.exists(),
                        'is_file': path.is_file(),
                        'size': path.stat().st_size if path.exists() else 0,
                        'extension': path.suffix,
                        'readable': False
                    }
                }

                if not path.exists():
                    validation['valid'] = False
                    validation['reasons'].append('æ–‡ä»¶ä¸å­˜åœ¨')
                elif not path.is_file():
                    validation['valid'] = False
                    validation['reasons'].append('ä¸æ˜¯æ–‡ä»¶')
                elif path.stat().st_size > self.max_file_size:
                    validation['valid'] = False
                    validation['reasons'].append('æ–‡ä»¶è¿‡å¤§')
                elif path.suffix not in self.supported_extensions:
                    validation['valid'] = False
                    validation['reasons'].append('ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹')
                else:
                    try:
                        with open(path, 'r', encoding='utf-8') as f:
                            f.read(100)  # æµ‹è¯•è¯»å–
                        validation['file_info']['readable'] = True
                    except UnicodeDecodeError:
                        validation['valid'] = False
                        validation['reasons'].append('æ–‡ä»¶ç¼–ç é—®é¢˜')
                    except Exception as e:
                        validation['valid'] = False
                        validation['reasons'].append(f'è¯»å–é”™è¯¯: {e}')

                return validation

            def analyze_file(self, file_path: str) -> dict:
                """åˆ†ææ–‡ä»¶"""
                validation = self.validate_file(file_path)
                if not validation['valid']:
                    return {
                        'success': False,
                        'file_path': file_path,
                        'error': f"æ–‡ä»¶éªŒè¯å¤±è´¥: {', '.join(validation['reasons'])}",
                        'validation': validation
                    }

                try:
                    content = Path(file_path).read_text(encoding='utf-8')

                    # æ¨¡æ‹Ÿåˆ†æè¿‡ç¨‹
                    analysis = {
                        'success': True,
                        'file_path': file_path,
                        'analysis_time': datetime.now().isoformat(),
                        'file_stats': validation['file_info'],
                        'content_analysis': {
                            'total_lines': len(content.splitlines()),
                            'total_chars': len(content),
                            'estimated_reading_time': len(content.splitlines()) * 0.5,  # åˆ†é’Ÿ
                            'language': self._detect_language(file_path)
                        },
                        'quality_metrics': {
                            'complexity_score': self._calculate_complexity(content),
                            'maintainability_index': self._calculate_maintainability(content),
                            'technical_debt_ratio': self._calculate_tech_debt(content)
                        },
                        'recommendations': self._generate_recommendations(content, file_path)
                    }

                    return analysis

                except Exception as e:
                    return {
                        'success': False,
                        'file_path': file_path,
                        'error': f"åˆ†æè¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}",
                        'exception_type': type(e).__name__
                    }

            def _detect_language(self, file_path: str) -> str:
                """æ£€æµ‹ç¼–ç¨‹è¯­è¨€"""
                ext = Path(file_path).suffix.lower()
                language_map = {
                    '.py': 'Python',
                    '.js': 'JavaScript',
                    '.ts': 'TypeScript',
                    '.java': 'Java',
                    '.cpp': 'C++',
                    '.json': 'JSON',
                    '.yaml': 'YAML',
                    '.yml': 'YAML'
                }
                return language_map.get(ext, 'Unknown')

            def _calculate_complexity(self, content: str) -> float:
                """è®¡ç®—å¤æ‚åº¦åˆ†æ•°"""
                lines = content.splitlines()
                complexity_keywords = ['if', 'else', 'for', 'while', 'try', 'except', 'class', 'def', 'function']
                complexity_score = 0

                for line in lines:
                    line_lower = line.lower()
                    for keyword in complexity_keywords:
                        complexity_score += line_lower.count(keyword)

                return min(10.0, complexity_score / len(lines) * 2) if lines else 0

            def _calculate_maintainability(self, content: str) -> float:
                """è®¡ç®—å¯ç»´æŠ¤æ€§æŒ‡æ•°"""
                lines = content.splitlines()
                comment_lines = len([line for line in lines if line.strip().startswith('#') or '//' in line])
                blank_lines = len([line for line in lines if not line.strip()])

                if len(lines) == 0:
                    return 10.0

                comment_ratio = comment_lines / len(lines)
                blank_ratio = blank_lines / len(lines)

                maintainability = 10.0 - (5.0 * (1 - comment_ratio)) - (2.0 * (1 - blank_ratio))
                return max(0.0, min(10.0, maintainability))

            def _calculate_tech_debt(self, content: str) -> float:
                """è®¡ç®—æŠ€æœ¯å€ºåŠ¡æ¯”ç‡"""
                # æ¨¡æ‹ŸæŠ€æœ¯å€ºåŠ¡è®¡ç®—
                complexity = self._calculate_complexity(content)
                maintainability = self._calculate_maintainability(content)
                tech_debt = max(0.0, (10.0 - maintainability + complexity) / 20.0 * 100)
                return tech_debt

            def _generate_recommendations(self, content: str, file_path: str) -> list:
                """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
                recommendations = []
                lines = content.splitlines()

                # åŸºäºæ–‡ä»¶é•¿åº¦çš„å»ºè®®
                if len(lines) > 500:
                    recommendations.append("è€ƒè™‘å°†å¤§æ–‡ä»¶æ‹†åˆ†ä¸ºå¤šä¸ªå°æ–‡ä»¶")
                elif len(lines) < 10:
                    recommendations.append("æ–‡ä»¶è¾ƒå°ï¼Œç¡®ä¿åŠŸèƒ½å®Œæ•´")

                # åŸºäºæ³¨é‡Šçš„å»ºè®®
                comment_lines = len([line for line in lines if line.strip().startswith('#')])
                comment_ratio = comment_lines / len(lines) if lines else 0
                if comment_ratio < 0.1:
                    recommendations.append("å»ºè®®å¢åŠ æ³¨é‡Šä»¥æé«˜ä»£ç å¯è¯»æ€§")

                # åŸºäºå¤æ‚åº¦çš„å»ºè®®
                complexity = self._calculate_complexity(content)
                if complexity > 7.0:
                    recommendations.append("ä»£ç å¤æ‚åº¦è¾ƒé«˜ï¼Œå»ºè®®é‡æ„ç®€åŒ–")

                # è¯­è¨€ç‰¹å®šå»ºè®®
                if Path(file_path).suffix == '.py':
                    if 'type hint' not in content.lower():
                        recommendations.append("å»ºè®®ä½¿ç”¨ç±»å‹æç¤ºå¢å¼ºä»£ç å¥å£®æ€§")
                elif Path(file_path).suffix == '.js':
                    if 'const' not in content and 'let' not in content:
                        recommendations.append("å»ºè®®ä½¿ç”¨const/letæ›¿ä»£varå£°æ˜å˜é‡")

                return recommendations if recommendations else ["ä»£ç è´¨é‡è‰¯å¥½ï¼Œæ— æ˜æ˜¾æ”¹è¿›ç‚¹"]

        # åˆ›å»ºæµ‹è¯•æ–‡ä»¶
        temp_dir = tempfile.mkdtemp()
        test_files = {
            "good_code.py": '''
from typing import List, Dict

def calculate_average(numbers: List[float]) -> float:
    """
    è®¡ç®—æ•°å­—åˆ—è¡¨çš„å¹³å‡å€¼

    Args:
        numbers: æ•°å­—åˆ—è¡¨

    Returns:
        å¹³å‡å€¼
    """
    if not numbers:
        return 0.0

    total = sum(numbers)
    return total / len(numbers)


class DataProcessor:
    """æ•°æ®å¤„ç†å™¨"""

    def __init__(self, name: str):
        self.name = name
        self.processed_count = 0

    def process_data(self, data: List[Dict]) -> List[Dict]:
        """å¤„ç†æ•°æ®"""
        processed = []
        for item in data:
            if self._validate_item(item):
                processed.append(self._transform_item(item))
                self.processed_count += 1
        return processed

    def _validate_item(self, item: Dict) -> bool:
        """éªŒè¯æ•°æ®é¡¹"""
        return 'id' in item and 'value' in item

    def _transform_item(self, item: Dict) -> Dict:
        """è½¬æ¢æ•°æ®é¡¹"""
        return {
            'id': item['id'],
            'value': item['value'] * 2,
            'processed_by': self.name
        }
''',
            "needs_improvement.js": '''
function processData(data) {
    var result = [];
    for (var i = 0; i < data.length; i++) {
        if (data[i].active == true) {
            result.push({
                id: data[i].id,
                value: data[i].value * 2,
                name: data[i].name
            });
        }
    }
    return result;
}

class User {
    constructor(name) {
        this.name = name;
        this.items = [];
    }

    addItem(item) {
        this.items.push(item);
        return this;
    }

    getTotal() {
        var total = 0;
        for (var i = 0; i < this.items.length; i++) {
            total += this.items[i].price;
        }
        return total;
    }
}
'''
        }

        # å†™å…¥æµ‹è¯•æ–‡ä»¶
        created_files = []
        for filename, content in test_files.items():
            file_path = Path(temp_dir) / filename
            file_path.write_text(content.strip())
            created_files.append(str(file_path))

        # æ‰§è¡Œå·¥ä½œæµæµ‹è¯•
        workflow = AnalysisWorkflow()
        analysis_results = []

        print(f"   åˆ†æ {len(created_files)} ä¸ªæµ‹è¯•æ–‡ä»¶:")
        for file_path in created_files:
            result = workflow.analyze_file(file_path)
            analysis_results.append(result)

            if result['success']:
                file_name = Path(result['file_path']).name
                content_analysis = result['content_analysis']
                quality_metrics = result['quality_metrics']
                recommendations = result['recommendations']

                print(f"     âœ… {file_name}:")
                print(f"        - è¯­è¨€: {content_analysis['language']}")
                print(f"        - è¡Œæ•°: {content_analysis['total_lines']}")
                print(f"        - å¤æ‚åº¦: {quality_metrics['complexity_score']:.1f}/10")
                print(f"        - å¯ç»´æŠ¤æ€§: {quality_metrics['maintainability_index']:.1f}/10")
                print(f"        - å»ºè®®æ•°é‡: {len(recommendations)}")
            else:
                print(f"     âŒ {result['file_path']}: {result['error']}")

        # éªŒè¯ç»“æœ
        successful_analyses = sum(1 for r in analysis_results if r['success'])
        total_files = len(created_files)
        success_rate = successful_analyses / total_files

        print(f"   - å·¥ä½œæµæˆåŠŸç‡: {successful_analyses}/{total_files} ({success_rate:.1%})")

        # éªŒè¯ç»“æœè´¨é‡
        quality_checks = []
        for result in analysis_results:
            if result['success']:
                # æ£€æŸ¥å¿…éœ€å­—æ®µ
                required_fields = ['content_analysis', 'quality_metrics', 'recommendations']
                has_all_fields = all(field in result for field in required_fields)
                quality_checks.append(has_all_fields)

        result_quality = all(quality_checks) if quality_checks else False
        print(f"   - ç»“æœè´¨é‡æ£€æŸ¥: {'âœ…' if result_quality else 'âŒ'}")

        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        import shutil
        shutil.rmtree(temp_dir)

        return success_rate >= 0.8 and result_quality

    except Exception as e:
        print(f"âŒ å®Œæ•´åˆ†æå·¥ä½œæµæµ‹è¯•å¤±è´¥: {e}")
        return False

def test_async_file_analysis():
    """æµ‹è¯•å¼‚æ­¥æ–‡ä»¶åˆ†æåŠŸèƒ½"""
    print("\nğŸ” æµ‹è¯•å¼‚æ­¥æ–‡ä»¶åˆ†æåŠŸèƒ½...")

    try:
        # æ¨¡æ‹Ÿå¼‚æ­¥æ–‡ä»¶åˆ†æå™¨
        class AsyncFileAnalyzer:
            def __init__(self):
                self.max_concurrent = 3
                self.analysis_semaphore = None

            async def analyze_file_async(self, file_path: str) -> dict:
                """å¼‚æ­¥åˆ†æå•ä¸ªæ–‡ä»¶"""
                if not self.analysis_semaphore:
                    import asyncio
                    self.analysis_semaphore = asyncio.Semaphore(self.max_concurrent)

                async with self.analysis_semaphore:
                    # æ¨¡æ‹Ÿå¼‚æ­¥åˆ†æå»¶è¿Ÿ
                    await asyncio.sleep(0.1)

                    try:
                        content = Path(file_path).read_text(encoding='utf-8')

                        # æ¨¡æ‹Ÿå¼‚æ­¥åˆ†æè¿‡ç¨‹
                        await asyncio.sleep(0.05)

                        analysis = {
                            'file_path': file_path,
                            'analysis_completed': True,
                            'analysis_time': datetime.now().isoformat(),
                            'file_size': len(content),
                            'line_count': len(content.splitlines()),
                            'async_analysis': True,
                            'processing_time': 0.15  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
                        }

                        return analysis

                    except Exception as e:
                        return {
                            'file_path': file_path,
                            'analysis_completed': False,
                            'error': str(e),
                            'async_analysis': True
                        }

            async def analyze_multiple_files(self, file_paths: list) -> list:
                """å¼‚æ­¥åˆ†æå¤šä¸ªæ–‡ä»¶"""
                tasks = [self.analyze_file_async(path) for path in file_paths]
                results = await asyncio.gather(*tasks, return_exceptions=True)

                processed_results = []
                for result in results:
                    if isinstance(result, Exception):
                        processed_results.append({
                            'analysis_completed': False,
                            'error': f'å¼‚æ­¥æ‰§è¡Œé”™è¯¯: {str(result)}'
                        })
                    else:
                        processed_results.append(result)

                return processed_results

        # åˆ›å»ºæµ‹è¯•æ–‡ä»¶
        temp_dir = tempfile.mkdtemp()
        test_contents = {
            "file1.py": "def function1():\n    return 'Hello World 1'",
            "file2.py": "def function2():\n    return 'Hello World 2'",
            "file3.py": "def function3():\n    return 'Hello World 3'",
            "file4.js": "function func4() { return 'Hello World 4'; }",
            "file5.js": "function func5() { return 'Hello World 5'; }"
        }

        created_files = []
        for filename, content in test_contents.items():
            file_path = Path(temp_dir) / filename
            file_path.write_text(content)
            created_files.append(str(file_path))

        # æ‰§è¡Œå¼‚æ­¥åˆ†æ
        async def run_async_test():
            analyzer = AsyncFileAnalyzer()

            start_time = asyncio.get_event_loop().time()
            results = await analyzer.analyze_multiple_files(created_files)
            end_time = asyncio.get_event_loop().time()

            return results, end_time - start_time

        # è¿è¡Œå¼‚æ­¥æµ‹è¯•
        import asyncio
        analysis_results, total_time = asyncio.run(run_async_test())

        print(f"   - å¼‚æ­¥åˆ†æå®Œæˆæ—¶é—´: {total_time:.3f}s")
        print(f"   - åˆ†ææ–‡ä»¶æ•°é‡: {len(created_files)}")
        print(f"   - æˆåŠŸåˆ†ææ•°é‡: {sum(1 for r in analysis_results if r.get('analysis_completed'))}")

        # éªŒè¯å¼‚æ­¥åˆ†æç»“æœ
        async_success = 0
        for result in analysis_results:
            if result.get('analysis_completed') and result.get('async_analysis'):
                async_success += 1
                print(f"     âœ… {Path(result['file_path']).name}: {result['file_size']} å­—ç¬¦")
            elif 'error' in result:
                print(f"     âŒ åˆ†æå¤±è´¥: {result['error']}")

        async_success_rate = async_success / len(created_files)
        print(f"   - å¼‚æ­¥åˆ†ææˆåŠŸç‡: {async_success_rate:.1%}")

        # éªŒè¯å¼‚æ­¥æ€§èƒ½ï¼ˆåº”è¯¥æ¯”ä¸²è¡Œå¿«ï¼‰
        expected_serial_time = len(created_files) * 0.15  # ä¸²è¡Œé¢„æœŸæ—¶é—´
        performance_good = total_time < expected_serial_time * 0.8  # è‡³å°‘å¿«20%
        print(f"   - å¼‚æ­¥æ€§èƒ½ä¼˜åŠ¿: {'âœ…' if performance_good else 'âŒ'}")

        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        import shutil
        shutil.rmtree(temp_dir)

        return async_success_rate >= 0.8 and performance_good

    except Exception as e:
        print(f"âŒ å¼‚æ­¥æ–‡ä»¶åˆ†æåŠŸèƒ½æµ‹è¯•å¤±è´¥: {e}")
        return False

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ å¼€å§‹æ–‡ä»¶åˆ†æå’Œç»“æœæ ¼å¼åŒ–åŠŸèƒ½æµ‹è¯•")
    print("=" * 60)

    test_results = []

    # 1. æµ‹è¯•æ–‡ä»¶è¯»å–èƒ½åŠ›
    reading_ok = test_file_reading_capabilities()
    test_results.append(reading_ok)

    # 2. æµ‹è¯•æ–‡ä»¶åˆ†æåŠŸèƒ½
    analysis_ok = test_file_analysis_functionality()
    test_results.append(analysis_ok)

    # 3. æµ‹è¯•ç»“æœæ ¼å¼åŒ–
    formatting_ok = test_result_formatting()
    test_results.append(formatting_ok)

    # 4. æµ‹è¯•å®Œæ•´åˆ†æå·¥ä½œæµ
    workflow_ok = test_analysis_workflow()
    test_results.append(workflow_ok)

    # 5. æµ‹è¯•å¼‚æ­¥æ–‡ä»¶åˆ†æ
    async_ok = test_async_file_analysis()
    test_results.append(async_ok)

    # è¾“å‡ºæµ‹è¯•ç»“æœ
    print("\n" + "=" * 60)
    print("ğŸ“Š æµ‹è¯•ç»“æœæ±‡æ€»:")

    passed = sum(test_results)
    total = len(test_results)

    print(f"âœ… é€šè¿‡: {passed}/{total}")
    print(f"âŒ å¤±è´¥: {total-passed}/{total}")

    if passed >= total * 0.8:  # 80%é€šè¿‡ç‡
        print("\nğŸ‰ æ–‡ä»¶åˆ†æå’Œç»“æœæ ¼å¼åŒ–åŠŸèƒ½æµ‹è¯•åŸºæœ¬é€šè¿‡ï¼")
        print("æ–‡ä»¶åˆ†æå’Œç»“æœæ ¼å¼åŒ–åŠŸèƒ½å·²å°±ç»ªã€‚")
        if passed < total:
            print(f"âš ï¸ æœ‰ {total-passed} é¡¹æµ‹è¯•å¤±è´¥ï¼Œä½†ä¸å½±å“æ ¸å¿ƒåŠŸèƒ½ã€‚")
        return 0
    else:
        print(f"\nâš ï¸ ä»…æœ‰ {passed}/{total} é¡¹æµ‹è¯•é€šè¿‡ï¼Œéœ€è¦æ£€æŸ¥æ–‡ä»¶åˆ†æåŠŸèƒ½ã€‚")
        return 1

if __name__ == "__main__":
    try:
        exit_code = main()
        sys.exit(exit_code)
    except KeyboardInterrupt:
        print("\nç”¨æˆ·ä¸­æ–­æµ‹è¯•")
        sys.exit(0)
    except Exception as e:
        print(f"\næµ‹è¯•å¼‚å¸¸: {e}")
        sys.exit(1)