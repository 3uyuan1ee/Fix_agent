#!/usr/bin/env python3
"""
æ–‡ä»¶åˆ†æå’Œç»“æœæ ¼å¼åŒ–åŠŸèƒ½æµ‹è¯•è„šæœ¬
ç”¨äºéªŒè¯DeepAnalyzerçš„æ–‡ä»¶åˆ†æèƒ½åŠ›å’Œå¤šç§ç»“æœæ ¼å¼åŒ–è¾“å‡º
"""

import sys
import os
import json
import asyncio
import tempfile
from pathlib import Path
from unittest.mock import Mock, patch, AsyncMock

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent / "src"))

def test_file_analysis_setup():
    """æµ‹è¯•æ–‡ä»¶åˆ†æç¯å¢ƒè®¾ç½®"""
    print("ğŸ” æµ‹è¯•æ–‡ä»¶åˆ†æç¯å¢ƒè®¾ç½®...")

    try:
        from src.tools.deep_analyzer import DeepAnalyzer, DeepAnalysisRequest, DeepAnalysisResult

        # åˆ›å»ºä¸´æ—¶æµ‹è¯•æ–‡ä»¶
        test_files = {}

        # Pythonæµ‹è¯•æ–‡ä»¶
        test_files['python'] = {
            'name': 'test_python.py',
            'content': '''def calculate_factorial(n):
    """è®¡ç®—é˜¶ä¹˜"""
    if n <= 1:
        return 1
    return n * calculate_factorial(n - 1)

def fibonacci(n):
    """æ–æ³¢é‚£å¥‘æ•°åˆ—"""
    if n <= 0:
        return 0
    elif n == 1:
        return 1
    return fibonacci(n - 1) + fibonacci(n - 2)

def main():
    for i in range(10):
        print(f"fib({i}) = {fibonacci(i)}")
        print(f"fact({i}) = {calculate_factorial(i)}")

if __name__ == "__main__":
    main()
'''
        }

        # JavaScriptæµ‹è¯•æ–‡ä»¶
        test_files['javascript'] = {
            'name': 'test_javascript.js',
            'content': '''function calculateFactorial(n) {
    if (n <= 1) return 1;
    return n * calculateFactorial(n - 1);
}

function fibonacci(n) {
    if (n <= 0) return 0;
    if (n === 1) return 1;
    return fibonacci(n - 1) + fibonacci(n - 2);
}

function main() {
    for (let i = 0; i < 10; i++) {
        console.log(`fib(${i}) = ${fibonacci(i)}`);
        console.log(`fact(${i}) = ${calculateFactorial(i)}`);
    }
}

main();
'''
        }

        # åˆ›å»ºä¸´æ—¶ç›®å½•å’Œæ–‡ä»¶
        temp_dir = tempfile.mkdtemp()
        created_files = {}

        for lang, file_info in test_files.items():
            file_path = os.path.join(temp_dir, file_info['name'])
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(file_info['content'])
            created_files[lang] = file_path

        print("âœ… æµ‹è¯•æ–‡ä»¶åˆ›å»ºæˆåŠŸ")
        print(f"   - ä¸´æ—¶ç›®å½•: {temp_dir}")
        print(f"   - Pythonæ–‡ä»¶: {created_files['python']}")
        print(f"   - JavaScriptæ–‡ä»¶: {created_files['javascript']}")

        return created_files, temp_dir, DeepAnalyzer, DeepAnalysisRequest, DeepAnalysisResult

    except Exception as e:
        print(f"âŒ æ–‡ä»¶åˆ†æç¯å¢ƒè®¾ç½®å¤±è´¥: {e}")
        return None, None, None, None, None

def test_file_reading_capabilities():
    """æµ‹è¯•æ–‡ä»¶è¯»å–èƒ½åŠ›"""
    print("\nğŸ” æµ‹è¯•æ–‡ä»¶è¯»å–èƒ½åŠ›...")

    try:
        from src.tools.deep_analyzer import DeepAnalyzer

        # ä½¿ç”¨Mockæ¥é¿å…LLMæä¾›è€…æ£€æŸ¥
        with patch('src.tools.deep_analyzer.LLMClient') as MockLLMClientClass, \
             patch('src.tools.deep_analyzer.PromptManager') as MockPromptManagerClass:

            # Mock LLMå®¢æˆ·ç«¯
            class MockLLMClient:
                def create_request(self, messages, **kwargs):
                    class MockRequest:
                        def __init__(self):
                            self.messages = messages
                            self.model = kwargs.get('model', 'test-model')
                    return MockRequest()

            # Mock Promptç®¡ç†å™¨
            class MockPromptManager:
                def render_template(self, template_name, variables):
                    class MockResult:
                        def __init__(self):
                            self.success = True
                            self.content = f"åˆ†æè¯·æ±‚: {variables.get('analysis_type', 'unknown')}"
                    return MockResult()

            MockLLMClientClass.return_value = MockLLMClient()
            MockPromptManagerClass.return_value = MockPromptManager()

            # åˆ›å»ºDeepAnalyzerå®ä¾‹
            analyzer = DeepAnalyzer()

        # åˆ›å»ºæµ‹è¯•æ–‡ä»¶
        test_file = Path("test_file_reading.py")
        test_content = '''def hello_world():
    print("Hello, World!")

def add_numbers(a, b):
    return a + b

class Calculator:
    def __init__(self):
        self.result = 0

    def calculate(self, x, y):
        self.result = x + y
        return self.result
'''

        with open(test_file, 'w', encoding='utf-8') as f:
            f.write(test_content)

        # æµ‹è¯•æ–‡ä»¶è¯»å–
        content = analyzer._read_file_content(str(test_file))

        if content and content.strip():
            print("âœ… æ–‡ä»¶è¯»å–æˆåŠŸ")
            print(f"   - æ–‡ä»¶å¤§å°: {len(content)} å­—ç¬¦")
            print(f"   - å†…å®¹è¡Œæ•°: {len(content.splitlines())} è¡Œ")
            print(f"   - åŒ…å«ç±»: {1 if 'class' in content else 0}")
            print(f"   - åŒ…å«å‡½æ•°: {content.count('def ')}")

            # æ¸…ç†æµ‹è¯•æ–‡ä»¶
            test_file.unlink()
            return True
        else:
            print("âŒ æ–‡ä»¶è¯»å–å¤±è´¥æˆ–å†…å®¹ä¸ºç©º")
            test_file.unlink()
            return False

    except Exception as e:
        print(f"âŒ æ–‡ä»¶è¯»å–æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_analysis_request_creation():
    """æµ‹è¯•åˆ†æè¯·æ±‚åˆ›å»º"""
    print("\nğŸ” æµ‹è¯•åˆ†æè¯·æ±‚åˆ›å»º...")

    try:
        from src.tools.deep_analyzer import DeepAnalysisRequest

        # æµ‹è¯•ä¸åŒç±»å‹çš„åˆ†æè¯·æ±‚
        test_requests = [
            {
                'file_path': 'test.py',
                'analysis_type': 'comprehensive',
                'temperature': 0.3,
                'max_tokens': 4000,
                'user_instructions': 'è¯·è¿›è¡Œå…¨é¢åˆ†æ'
            },
            {
                'file_path': 'test.js',
                'analysis_type': 'security',
                'temperature': 0.1,
                'max_tokens': 3000,
                'user_instructions': 'é‡ç‚¹å…³æ³¨å®‰å…¨é—®é¢˜'
            },
            {
                'file_path': 'test.java',
                'analysis_type': 'performance',
                'temperature': 0.5,
                'max_tokens': 5000,
                'user_instructions': 'åˆ†ææ€§èƒ½ç“¶é¢ˆ'
            }
        ]

        created_requests = []
        for req_data in test_requests:
            request = DeepAnalysisRequest(**req_data)
            created_requests.append(request)
            print(f"âœ… åˆ›å»ºåˆ†æè¯·æ±‚: {request.analysis_type} - {request.file_path}")

        print(f"   - æ€»å…±åˆ›å»ºè¯·æ±‚æ•°: {len(created_requests)}")
        return len(created_requests) == len(test_requests)

    except Exception as e:
        print(f"âŒ åˆ†æè¯·æ±‚åˆ›å»ºæµ‹è¯•å¤±è´¥: {e}")
        return False

def test_mock_file_analysis():
    """æµ‹è¯•Mockæ–‡ä»¶åˆ†ææµç¨‹"""
    print("\nğŸ” æµ‹è¯•Mockæ–‡ä»¶åˆ†ææµç¨‹...")

    try:
        from src.tools.deep_analyzer import DeepAnalyzer, DeepAnalysisRequest
        import asyncio

        # Mock LLMå®¢æˆ·ç«¯
        class MockLLMClient:
            def __init__(self):
                self.request_count = 0

            def create_request(self, messages, **kwargs):
                self.request_count += 1
                class MockRequest:
                    def __init__(self):
                        self.messages = messages
                        self.model = kwargs.get('model', 'mock-model')
                return MockRequest()

            async def complete(self, request):
                # æ¨¡æ‹Ÿåˆ†æå“åº”
                mock_response = '''{
    "summary": "è¿™æ˜¯ä¸€ä¸ªç»“æ„è‰¯å¥½çš„Pythonä»£ç ï¼Œå®ç°äº†åŸºæœ¬çš„è®¡ç®—åŠŸèƒ½",
    "issues": [
        {
            "type": "suggestion",
            "line": 1,
            "message": "å»ºè®®æ·»åŠ ç±»å‹æ³¨è§£ä»¥æé«˜ä»£ç å¯è¯»æ€§"
        },
        {
            "type": "performance",
            "line": 8,
            "message": "fibonacciå‡½æ•°ä½¿ç”¨äº†é€’å½’ï¼Œå¯¹å¤§æ•°å€¼å¯èƒ½æœ‰æ€§èƒ½é—®é¢˜"
        }
    ],
    "recommendations": [
        "ä¸ºæ‰€æœ‰å‡½æ•°æ·»åŠ ç±»å‹æ³¨è§£",
        "è€ƒè™‘ä½¿ç”¨è¿­ä»£æ–¹å¼å®ç°æ–æ³¢é‚£å¥‘æ•°åˆ—",
        "æ·»åŠ è¾“å…¥éªŒè¯å’Œé”™è¯¯å¤„ç†",
        "æ·»åŠ å•å…ƒæµ‹è¯•"
    ],
    "strengths": [
        "ä»£ç ç»“æ„æ¸…æ™°",
        "å‡½æ•°èŒè´£å•ä¸€",
        "æœ‰é€‚å½“çš„æ–‡æ¡£å­—ç¬¦ä¸²"
    ],
    "overall_score": 8.2
}'''

                class MockResponse:
                    def __init__(self):
                        self.content = mock_response
                        self.model = request.model
                        self.provider = "mock"
                        self.usage = {"total_tokens": 250}
                return MockResponse()

        # Mock Promptç®¡ç†å™¨
        class MockPromptManager:
            def render_template(self, template_name, variables):
                content = f"åˆ†æ {variables.get('analysis_type', 'unknown')}: {variables.get('file_content', '')[:100]}..."
                class MockResult:
                    def __init__(self):
                        self.success = True
                        self.content = content
                return MockResult()

        # åˆ›å»ºDeepAnalyzerå®ä¾‹
        analyzer = DeepAnalyzer()
        analyzer.llm_client = MockLLMClient()
        analyzer.prompt_manager = MockPromptManager()

        # åˆ›å»ºæµ‹è¯•æ–‡ä»¶
        test_file = Path("test_analysis.py")
        test_content = '''def fibonacci(n):
    """æ–æ³¢é‚£å¥‘æ•°åˆ—"""
    if n <= 1:
        return n
    return fibonacci(n-1) + fibonacci(n-2)

def calculate_factorial(n):
    """è®¡ç®—é˜¶ä¹˜"""
    if n <= 1:
        return 1
    return n * calculate_factorial(n-1)

print(fibonacci(10))
print(calculate_factorial(5))
'''

        with open(test_file, 'w', encoding='utf-8') as f:
            f.write(test_content)

        # åˆ›å»ºåˆ†æè¯·æ±‚
        request = DeepAnalysisRequest(
            file_path=str(test_file),
            analysis_type="comprehensive",
            user_instructions="è¯·é‡ç‚¹å…³æ³¨ä»£ç è´¨é‡å’Œæ€§èƒ½"
        )

        # ä½¿ç”¨asyncioè¿è¡Œå¼‚æ­¥åˆ†æ
        async def run_analysis():
            return await analyzer.analyze_file(request)

        result = asyncio.run(run_analysis())

        if result.success:
            print("âœ… Mockæ–‡ä»¶åˆ†ææˆåŠŸ")
            print(f"   - åˆ†ææ–‡ä»¶: {result.file_path}")
            print(f"   - åˆ†æç±»å‹: {result.analysis_type}")
            print(f"   - æ‰§è¡Œæ—¶é—´: {result.execution_time:.2f}ç§’")
            print(f"   - ä½¿ç”¨æ¨¡å‹: {result.model_used}")
            print(f"   - Tokenä½¿ç”¨: {result.token_usage}")
            print(f"   - å†…å®¹é•¿åº¦: {len(result.content)} å­—ç¬¦")

            if result.structured_analysis:
                print("âœ… ç»“æ„åŒ–åˆ†æç»“æœ:")
                analysis = result.structured_analysis
                print(f"   - æ‘˜è¦: {analysis.get('summary', 'N/A')[:50]}...")
                print(f"   - é—®é¢˜æ•°é‡: {len(analysis.get('issues', []))}")
                print(f"   - å»ºè®®æ•°é‡: {len(analysis.get('recommendations', []))}")
                print(f"   - æ€»ä½“è¯„åˆ†: {analysis.get('overall_score', 'N/A')}")

            # æ¸…ç†æµ‹è¯•æ–‡ä»¶
            test_file.unlink()
            return True
        else:
            print(f"âŒ Mockæ–‡ä»¶åˆ†æå¤±è´¥: {result.error}")
            test_file.unlink()
            return False

    except Exception as e:
        print(f"âŒ Mockæ–‡ä»¶åˆ†ææµ‹è¯•å¤±è´¥: {e}")
        return False

def test_result_formatting_text():
    """æµ‹è¯•æ–‡æœ¬æ ¼å¼åŒ–è¾“å‡º"""
    print("\nğŸ” æµ‹è¯•æ–‡æœ¬æ ¼å¼åŒ–è¾“å‡º...")

    try:
        from src.tools.deep_analyzer import DeepAnalyzer, DeepAnalysisResult

        # åˆ›å»ºæµ‹è¯•ç»“æœ
        test_result = DeepAnalysisResult(
            file_path="test.py",
            analysis_type="comprehensive",
            success=True,
            content="åˆ†æå®Œæˆï¼Œå‘ç°3ä¸ªé—®é¢˜",
            structured_analysis={
                "summary": "ä»£ç è´¨é‡è‰¯å¥½ï¼Œä½†æœ‰ä¸€äº›æ”¹è¿›å»ºè®®",
                "issues": [
                    {"type": "style", "line": 5, "message": "ç¼ºå°‘ç±»å‹æ³¨è§£"},
                    {"type": "performance", "line": 10, "message": "å¯ä»¥ä½¿ç”¨æ›´é«˜æ•ˆçš„ç®—æ³•"}
                ],
                "recommendations": [
                    "æ·»åŠ ç±»å‹æ³¨è§£",
                    "ä¼˜åŒ–ç®—æ³•æ€§èƒ½",
                    "æ·»åŠ é”™è¯¯å¤„ç†"
                ],
                "overall_score": 8.5
            },
            execution_time=1.23,
            model_used="glm-4.5",
            token_usage={"total_tokens": 300}
        )

        # åˆ›å»ºåˆ†æå™¨å®ä¾‹
        analyzer = DeepAnalyzer()

        # æµ‹è¯•æ–‡æœ¬æ ¼å¼åŒ–
        text_output = analyzer.format_analysis_result(test_result, "text")

        if text_output and len(text_output) > 0:
            print("âœ… æ–‡æœ¬æ ¼å¼åŒ–æˆåŠŸ")
            print(f"   - è¾“å‡ºé•¿åº¦: {len(text_output)} å­—ç¬¦")
            print("   - æ ¼å¼åŒ–ç»“æœé¢„è§ˆ:")
            print("   " + "="*40)
            for line in text_output.split('\n')[:10]:  # æ˜¾ç¤ºå‰10è¡Œ
                if line.strip():
                    print(f"   {line}")
            if len(text_output.split('\n')) > 10:
                print("   ...")
            print("   " + "="*40)

            return True
        else:
            print("âŒ æ–‡æœ¬æ ¼å¼åŒ–å¤±è´¥")
            return False

    except Exception as e:
        print(f"âŒ æ–‡æœ¬æ ¼å¼åŒ–æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_result_formatting_json():
    """æµ‹è¯•JSONæ ¼å¼åŒ–è¾“å‡º"""
    print("\nğŸ” æµ‹è¯•JSONæ ¼å¼åŒ–è¾“å‡º...")

    try:
        from src.tools.deep_analyzer import DeepAnalyzer, DeepAnalysisResult

        # åˆ›å»ºæµ‹è¯•ç»“æœ
        test_result = DeepAnalysisResult(
            file_path="test.py",
            analysis_type="security",
            success=True,
            content="å®‰å…¨åˆ†æå®Œæˆ",
            structured_analysis={
                "summary": "å‘ç°1ä¸ªå®‰å…¨é£é™©",
                "security_issues": [
                    {"severity": "high", "line": 15, "description": "SQLæ³¨å…¥é£é™©"},
                    {"severity": "medium", "line": 25, "description": "ç¡¬ç¼–ç å¯†é’¥"}
                ],
                "recommendations": [
                    "ä½¿ç”¨å‚æ•°åŒ–æŸ¥è¯¢",
                    "å°†å¯†é’¥ç§»è‡³ç¯å¢ƒå˜é‡"
                ],
                "security_score": 6.5
            },
            execution_time=2.15,
            model_used="glm-4.5",
            token_usage={"total_tokens": 450}
        )

        # åˆ›å»ºåˆ†æå™¨å®ä¾‹
        analyzer = DeepAnalyzer()

        # æµ‹è¯•JSONæ ¼å¼åŒ–
        json_output = analyzer.format_analysis_result(test_result, "json")

        if json_output:
            # éªŒè¯JSONæœ‰æ•ˆæ€§
            try:
                json_data = json.loads(json_output)
                print("âœ… JSONæ ¼å¼åŒ–æˆåŠŸ")
                print(f"   - è¾“å‡ºé•¿åº¦: {len(json_output)} å­—ç¬¦")
                print(f"   - JSONå­—æ®µæ•°: {len(json_data)}")
                print("   - ä¸»è¦å­—æ®µ:")
                for key in ['file_path', 'analysis_type', 'success', 'structured_analysis']:
                    if key in json_data:
                        print(f"     - {key}: {type(json_data[key]).__name__}")

                return True
            except json.JSONDecodeError:
                print("âŒ JSONæ ¼å¼åŒ–ç»“æœæ— æ•ˆ")
                return False
        else:
            print("âŒ JSONæ ¼å¼åŒ–å¤±è´¥")
            return False

    except Exception as e:
        print(f"âŒ JSONæ ¼å¼åŒ–æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_result_formatting_markdown():
    """æµ‹è¯•Markdownæ ¼å¼åŒ–è¾“å‡º"""
    print("\nğŸ” æµ‹è¯•Markdownæ ¼å¼åŒ–è¾“å‡º...")

    try:
        from src.tools.deep_analyzer import DeepAnalyzer, DeepAnalysisResult

        # åˆ›å»ºæµ‹è¯•ç»“æœ
        test_result = DeepAnalysisResult(
            file_path="test.py",
            analysis_type="performance",
            success=True,
            content="æ€§èƒ½åˆ†æå®Œæˆ",
            structured_analysis={
                "summary": "æ€§èƒ½è‰¯å¥½ï¼Œæœ‰ä¼˜åŒ–ç©ºé—´",
                "performance_metrics": {
                    "time_complexity": "O(nÂ²)",
                    "space_complexity": "O(n)",
                    "bottlenecks": ["é€’å½’è°ƒç”¨", "é‡å¤è®¡ç®—"]
                },
                "optimization_suggestions": [
                    "ä½¿ç”¨åŠ¨æ€è§„åˆ’",
                    "ç¼“å­˜è®¡ç®—ç»“æœ",
                    "ç®—æ³•é‡æ„"
                ],
                "performance_score": 7.8
            },
            execution_time=1.87,
            model_used="glm-4.5",
            token_usage={"total_tokens": 380}
        )

        # åˆ›å»ºåˆ†æå™¨å®ä¾‹
        analyzer = DeepAnalyzer()

        # æµ‹è¯•Markdownæ ¼å¼åŒ–
        md_output = analyzer.format_analysis_result(test_result, "markdown")

        if md_output and len(md_output) > 0:
            print("âœ… Markdownæ ¼å¼åŒ–æˆåŠŸ")
            print(f"   - è¾“å‡ºé•¿åº¦: {len(md_output)} å­—ç¬¦")
            print("   - Markdownæ ¼å¼éªŒè¯:")
            print(f"     - åŒ…å«æ ‡é¢˜: {'#' in md_output}")
            print(f"     - åŒ…å«åˆ—è¡¨: {'-' in md_output or '*' in md_output}")
            print(f"     - åŒ…å«ä»£ç å—: {'```' in md_output}")
            print(f"     - åŒ…å«ç²—ä½“: {'**' in md_output}")

            # æ˜¾ç¤ºå‰å‡ è¡Œ
            print("   - æ ¼å¼åŒ–ç»“æœé¢„è§ˆ:")
            print("   " + "="*40)
            for line in md_output.split('\n')[:8]:
                if line.strip():
                    print(f"   {line}")
            print("   " + "="*40)

            return True
        else:
            print("âŒ Markdownæ ¼å¼åŒ–å¤±è´¥")
            return False

    except Exception as e:
        print(f"âŒ Markdownæ ¼å¼åŒ–æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_multi_format_output():
    """æµ‹è¯•å¤šæ ¼å¼è¾“å‡ºä¸€è‡´æ€§"""
    print("\nğŸ” æµ‹è¯•å¤šæ ¼å¼è¾“å‡ºä¸€è‡´æ€§...")

    try:
        from src.tools.deep_analyzer import DeepAnalyzer, DeepAnalysisResult

        # åˆ›å»ºæµ‹è¯•ç»“æœ
        test_result = DeepAnalysisResult(
            file_path="consistency_test.py",
            analysis_type="comprehensive",
            success=True,
            content="ä¸€è‡´æ€§æµ‹è¯•",
            structured_analysis={
                "summary": "å¤šæ ¼å¼è¾“å‡ºä¸€è‡´æ€§æµ‹è¯•",
                "issues": [{"type": "test", "message": "æµ‹è¯•é—®é¢˜"}],
                "recommendations": ["æµ‹è¯•å»ºè®®"],
                "score": 9.0
            },
            execution_time=0.95,
            model_used="test-model",
            token_usage={"total_tokens": 200}
        )

        # åˆ›å»ºåˆ†æå™¨å®ä¾‹
        analyzer = DeepAnalyzer()

        # æµ‹è¯•æ‰€æœ‰æ ¼å¼
        formats = ["text", "json", "markdown"]
        results = {}

        for fmt in formats:
            try:
                output = analyzer.format_analysis_result(test_result, fmt)
                if output:
                    results[fmt] = output
                    print(f"âœ… {fmt.upper()} æ ¼å¼åŒ–æˆåŠŸ")
                else:
                    print(f"âŒ {fmt.upper()} æ ¼å¼åŒ–å¤±è´¥")
            except Exception as e:
                print(f"âŒ {fmt.upper()} æ ¼å¼åŒ–å¼‚å¸¸: {e}")

        # éªŒè¯ä¸€è‡´æ€§
        if len(results) == len(formats):
            print("âœ… æ‰€æœ‰æ ¼å¼å‡æˆåŠŸç”Ÿæˆ")
            print(f"   - æˆåŠŸæ ¼å¼æ•°: {len(results)}/{len(formats)}")

            # éªŒè¯å…³é”®ä¿¡æ¯åœ¨æ‰€æœ‰æ ¼å¼ä¸­éƒ½å­˜åœ¨
            key_info = test_result.file_path
            consistency_check = all(key_info in output for output in results.values())

            print(f"   - å…³é”®ä¿¡æ¯ä¸€è‡´æ€§: {'é€šè¿‡' if consistency_check else 'å¤±è´¥'}")
            return len(results) == len(formats)
        else:
            print(f"âŒ æ ¼å¼åŒ–ä¸å®Œæ•´: {len(results)}/{len(formats)}")
            return False

    except Exception as e:
        print(f"âŒ å¤šæ ¼å¼è¾“å‡ºæµ‹è¯•å¤±è´¥: {e}")
        return False

def test_end_to_end_analysis():
    """æµ‹è¯•ç«¯åˆ°ç«¯åˆ†ææµç¨‹"""
    print("\nğŸ” æµ‹è¯•ç«¯åˆ°ç«¯åˆ†ææµç¨‹...")

    try:
        from src.tools.deep_analyzer import DeepAnalyzer, DeepAnalysisRequest
        import asyncio

        # Mockç»„ä»¶
        class MockLLMClient:
            def create_request(self, messages, **kwargs):
                class MockRequest:
                    def __init__(self):
                        self.messages = messages
                        self.model = kwargs.get('model', 'mock-model')
                return MockRequest()

            async def complete(self, request):
                response_content = '''{
    "summary": "ç«¯åˆ°ç«¯æµ‹è¯•ï¼šä»£ç ç»“æ„è‰¯å¥½ï¼ŒåŠŸèƒ½å®Œæ•´",
    "issues": [
        {
            "type": "improvement",
            "line": 1,
            "message": "å¯ä»¥æ·»åŠ æ›´å¤šé”™è¯¯å¤„ç†"
        }
    ],
    "recommendations": [
        "æ·»åŠ è¾“å…¥éªŒè¯",
        "å¢åŠ å¼‚å¸¸å¤„ç†",
        "æ·»åŠ æ—¥å¿—è®°å½•"
    ],
    "strengths": [
        "å‡½æ•°è®¾è®¡åˆç†",
        "ä»£ç é£æ ¼ä¸€è‡´"
    ],
    "overall_score": 8.8
}'''

                class MockResponse:
                    def __init__(self):
                        self.content = response_content
                        self.model = "mock-model"
                        self.provider = "mock"
                        self.usage = {"total_tokens": 180}
                return MockResponse()

        class MockPromptManager:
            def render_template(self, template_name, variables):
                class MockResult:
                    def __init__(self):
                        self.success = True
                        self.content = f"åˆ†æè¯·æ±‚: {variables.get('file_path', 'unknown')}"
                return MockResult()

        # åˆ›å»ºåˆ†æå™¨
        analyzer = DeepAnalyzer()
        analyzer.llm_client = MockLLMClient()
        analyzer.prompt_manager = MockPromptManager()

        # åˆ›å»ºæµ‹è¯•æ–‡ä»¶
        test_file = Path("end_to_end_test.py")
        test_content = '''import json
import os

def load_config(file_path):
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    try:
        with open(file_path, 'r') as f:
            return json.load(f)
    except FileNotFoundError:
        return {}

def get_env_var(key, default=None):
    """è·å–ç¯å¢ƒå˜é‡"""
    return os.getenv(key, default)

def main():
    config = load_config('config.json')
    api_key = get_env_var('API_KEY')
    print(f"é…ç½®: {config}")
    print(f"APIå¯†é’¥: {api_key}")

if __name__ == "__main__":
    main()
'''

        with open(test_file, 'w', encoding='utf-8') as f:
            f.write(test_content)

        # æ‰§è¡Œåˆ†æ
        request = DeepAnalysisRequest(
            file_path=str(test_file),
            analysis_type="comprehensive",
            user_instructions="è¯·è¿›è¡Œå…¨é¢åˆ†æï¼ŒåŒ…æ‹¬å®‰å…¨æ€§æ£€æŸ¥"
        )

        # ä½¿ç”¨asyncioè¿è¡Œå¼‚æ­¥åˆ†æ
        async def run_analysis():
            return await analyzer.analyze_file(request)

        result = asyncio.run(run_analysis())

        if result.success:
            print("âœ… ç«¯åˆ°ç«¯åˆ†ææˆåŠŸ")
            print(f"   - åˆ†ææ–‡ä»¶: {result.file_path}")
            print(f"   - åˆ†æç±»å‹: {result.analysis_type}")
            print(f"   - æ‰§è¡Œæ—¶é—´: {result.execution_time:.3f}ç§’")
            print(f"   - ä½¿ç”¨æ¨¡å‹: {result.model_used}")
            print(f"   - Tokenä½¿ç”¨: {result.token_usage}")

            # æµ‹è¯•ä¸åŒæ ¼å¼è¾“å‡º
            analyzer = DeepAnalyzer()
            for fmt in ["text", "json", "markdown"]:
                try:
                    output = analyzer.format_analysis_result(result, fmt)
                    if output:
                        print(f"   - {fmt.upper()} è¾“å‡º: {len(output)} å­—ç¬¦")
                except Exception as e:
                    print(f"   - {fmt.upper()} è¾“å‡ºå¤±è´¥: {e}")

            # æ¸…ç†æµ‹è¯•æ–‡ä»¶
            test_file.unlink()
            return True
        else:
            print(f"âŒ ç«¯åˆ°ç«¯åˆ†æå¤±è´¥: {result.error}")
            test_file.unlink()
            return False

    except Exception as e:
        print(f"âŒ ç«¯åˆ°ç«¯åˆ†ææµ‹è¯•å¤±è´¥: {e}")
        return False

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ å¼€å§‹æ–‡ä»¶åˆ†æå’Œç»“æœæ ¼å¼åŒ–åŠŸèƒ½æµ‹è¯•")
    print("=" * 60)

    test_results = []

    # 1. æµ‹è¯•æ–‡ä»¶åˆ†æç¯å¢ƒè®¾ç½®
    created_files, temp_dir, DeepAnalyzer, DeepAnalysisRequest, DeepAnalysisResult = test_file_analysis_setup()
    test_results.append(created_files is not None)

    # 2. æµ‹è¯•æ–‡ä»¶è¯»å–èƒ½åŠ›
    reading_ok = test_file_reading_capabilities()
    test_results.append(reading_ok)

    # 3. æµ‹è¯•åˆ†æè¯·æ±‚åˆ›å»º
    request_ok = test_analysis_request_creation()
    test_results.append(request_ok)

    # 4. æµ‹è¯•Mockæ–‡ä»¶åˆ†æ
    mock_analysis_ok = test_mock_file_analysis()
    test_results.append(mock_analysis_ok)

    # 5. æµ‹è¯•æ–‡æœ¬æ ¼å¼åŒ–
    text_formatting_ok = test_result_formatting_text()
    test_results.append(text_formatting_ok)

    # 6. æµ‹è¯•JSONæ ¼å¼åŒ–
    json_formatting_ok = test_result_formatting_json()
    test_results.append(json_formatting_ok)

    # 7. æµ‹è¯•Markdownæ ¼å¼åŒ–
    md_formatting_ok = test_result_formatting_markdown()
    test_results.append(md_formatting_ok)

    # 8. æµ‹è¯•å¤šæ ¼å¼è¾“å‡ºä¸€è‡´æ€§
    multi_format_ok = test_multi_format_output()
    test_results.append(multi_format_ok)

    # 9. æµ‹è¯•ç«¯åˆ°ç«¯åˆ†ææµç¨‹
    e2e_ok = test_end_to_end_analysis()
    test_results.append(e2e_ok)

    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
    if temp_dir and os.path.exists(temp_dir):
        import shutil
        shutil.rmtree(temp_dir)
        print("   - ä¸´æ—¶ç›®å½•å·²æ¸…ç†")

    # è¾“å‡ºæµ‹è¯•ç»“æœ
    print("\n" + "=" * 60)
    print("ğŸ“Š æµ‹è¯•ç»“æœæ±‡æ€»:")

    passed = sum(test_results)
    total = len(test_results)

    print(f"âœ… é€šè¿‡: {passed}/{total}")
    print(f"âŒ å¤±è´¥: {total-passed}/{total}")

    if passed >= total * 0.8:  # 80%é€šè¿‡ç‡
        print("\nğŸ‰ æ–‡ä»¶åˆ†æå’Œç»“æœæ ¼å¼åŒ–åŠŸèƒ½æµ‹è¯•åŸºæœ¬é€šè¿‡ï¼")
        print("æ–‡ä»¶åˆ†æå’Œå¤šæ ¼å¼è¾“å‡ºåŠŸèƒ½å·²å°±ç»ªã€‚")
        if passed < total:
            print(f"âš ï¸ æœ‰ {total-passed} é¡¹æµ‹è¯•å¤±è´¥ï¼Œä½†ä¸å½±å“æ ¸å¿ƒåŠŸèƒ½ã€‚")
        return 0
    else:
        print(f"\nâš ï¸ ä»…æœ‰ {passed}/{total} é¡¹æµ‹è¯•é€šè¿‡ï¼Œéœ€è¦æ£€æŸ¥æ–‡ä»¶åˆ†æåŠŸèƒ½ã€‚")
        return 1

if __name__ == "__main__":
    try:
        exit_code = main()
        sys.exit(exit_code)
    except KeyboardInterrupt:
        print("\nç”¨æˆ·ä¸­æ–­æµ‹è¯•")
        sys.exit(0)
    except Exception as e:
        print(f"\næµ‹è¯•å¼‚å¸¸: {e}")
        sys.exit(1)