#!/usr/bin/env python3
"""
T019æ·±åº¦åˆ†ææ‰§è¡Œæµç¨‹æ¼”ç¤º
å±•ç¤ºå®Œæ•´çš„æ·±åº¦åˆ†æå·¥ä½œæµç¨‹ï¼ŒåŒ…æ‹¬æ–‡ä»¶è¯»å–ã€promptæ„é€ ã€LLMé›†æˆå’Œç»“æœå±•ç¤º
"""

import tempfile
import asyncio
from pathlib import Path
from unittest.mock import AsyncMock, patch

from src.tools import DeepAnalyzer, DeepAnalysisRequest, DeepAnalysisReportGenerator


def demo_t019_deep_analysis():
    """æ¼”ç¤ºT019æ·±åº¦åˆ†ææ‰§è¡Œæµç¨‹çš„å®Œæ•´åŠŸèƒ½"""
    print("=== T019æ·±åº¦åˆ†ææ‰§è¡Œæµç¨‹æ¼”ç¤º ===\n")

    # åˆ›å»ºä¸´æ—¶é¡¹ç›®ç›®å½•
    with tempfile.TemporaryDirectory() as temp_dir:
        project_dir = Path(temp_dir) / "demo_project"
        project_dir.mkdir()

        # åˆ›å»ºç¤ºä¾‹Pythonæ–‡ä»¶ - åŒ…å«å„ç§é—®é¢˜çš„ä»£ç 
        (project_dir / "security_example.py").write_text("""
import os
import subprocess
import sqlite3

# ç¡¬ç¼–ç æ•æ„Ÿä¿¡æ¯
DATABASE_PASSWORD = "admin123_456"
API_KEY = "sk-1234567890abcdef"

def get_user_data(user_id):
    # SQLæ³¨å…¥æ¼æ´
    query = f"SELECT * FROM users WHERE id = {user_id}"

    # ä¸å®‰å…¨çš„æ•°æ®åº“è¿æ¥
    conn = sqlite3.connect("users.db")
    cursor = conn.cursor()

    try:
        result = cursor.execute(query).fetchall()
        return result
    finally:
        conn.close()

def process_file(filename):
    # è·¯å¾„éå†æ¼æ´
    if ".." in filename:
        return "Invalid filename"

    # ä½¿ç”¨evalçš„å±é™©æ“ä½œ
    eval(f"print('Processing file: {filename}')")

    # æ‰§è¡Œç”¨æˆ·æä¾›çš„å‘½ä»¤
    command = f"cat {filename}"
    result = subprocess.run(command, shell=True, capture_output=True, text=True)

    return result.stdout

class DataProcessor:
    def __init__(self):
        self.data = []

    def load_data(self, source):
        # ä¸å®‰å…¨çš„ååºåˆ—åŒ–
        import pickle
        with open(source, 'rb') as f:
            self.data = pickle.load(f)

    def export_report(self, filename):
        # ä¿¡æ¯æ³„éœ² - æ‰“å°æ•æ„Ÿæ•°æ®
        print(f"Exporting {len(self.data)} records to {filename}")
        print(f"Database password: {DATABASE_PASSWORD}")
        print(f"API key: {API_KEY}")

if __name__ == "__main__":
    processor = DataProcessor()
    processor.load_data("data.pkl")
    processor.export_report("report.txt")

    user_data = get_user_data("1")
    print(f"User data: {user_data}")
""")

        # åˆ›å»ºæ€§èƒ½é—®é¢˜ç¤ºä¾‹
        (project_dir / "performance_example.py").write_text("""
import time
import requests

class InefficientDataProcessor:
    def __init__(self):
        self.cache = {}

    def process_large_dataset(self, items):
        results = []

        # æ€§èƒ½é—®é¢˜ï¼šå¾ªç¯å†…é‡å¤HTTPè¯·æ±‚
        for item in items:
            response = requests.get(f"https://api.example.com/data/{item['id']}")
            time.sleep(0.1)  # ä¸å¿…è¦çš„å»¶è¿Ÿ

            # æ€§èƒ½é—®é¢˜ï¼šå¾ªç¯å†…é‡å¤æ•°æ®åº“æŸ¥è¯¢
            details = self.get_item_details(item['id'])
            results.append({
                'item': item,
                'api_data': response.json(),
                'details': details
            })

        return results

    def get_item_details(self, item_id):
        # æ€§èƒ½é—®é¢˜ï¼šN+1æŸ¥è¯¢é—®é¢˜
        # æ¯æ¬¡è°ƒç”¨éƒ½å»ºç«‹æ–°è¿æ¥
        import sqlite3
        conn = sqlite3.connect("items.db")
        cursor = conn.cursor()

        cursor.execute(f"SELECT * FROM item_details WHERE item_id = {item_id}")
        result = cursor.fetchone()

        conn.close()
        return result

    def expensive_computation(self, data):
        # æ€§èƒ½é—®é¢˜ï¼šé‡å¤è®¡ç®—
        results = []

        for i in range(len(data)):
            for j in range(len(data)):
                # O(nÂ²)å¤æ‚åº¦ï¼Œå¯ä»¥ä¼˜åŒ–
                if data[i] == data[j] and i != j:
                    results.append((i, j))

        return results

    def memory_intensive_operation(self, large_list):
        # å†…å­˜é—®é¢˜ï¼šä¸€æ¬¡æ€§åŠ è½½å¤§é‡æ•°æ®
        all_data = []

        for item in large_list:
            # å°†æ‰€æœ‰æ•°æ®ä¿å­˜åœ¨å†…å­˜ä¸­ï¼Œå¯èƒ½å¯¼è‡´å†…å­˜æº¢å‡º
            processed_data = self.heavy_processing(item)
            all_data.append(processed_data)

        return all_data

    def heavy_processing(self, item):
        # æ¨¡æ‹Ÿå¯†é›†è®¡ç®—
        result = []
        for i in range(100000):
            result.append(item * i)
        return result

# ä½¿ç”¨ç¤ºä¾‹
processor = InefficientDataProcessor()

# æ¨¡æ‹Ÿå¤§æ•°æ®é›†
large_dataset = [{'id': i} for i in range(1, 101)]

start_time = time.time()
results = processor.process_large_dataset(large_dataset)
end_time = time.time()

print(f"Processed {len(results)} items in {end_time - start_time:.2f} seconds")
""")

        print(f"âœ… åˆ›å»ºæ¼”ç¤ºé¡¹ç›®: {project_dir}")

        # 1. åˆå§‹åŒ–æ·±åº¦åˆ†æå™¨
        print("\nğŸ” 1. åˆå§‹åŒ–æ·±åº¦åˆ†æå™¨")

        # åˆ›å»ºmockçš„LLMå®¢æˆ·ç«¯æ¥æ¼”ç¤ºåŠŸèƒ½
        mock_llm_client = AsyncMock()

        # æ¨¡æ‹ŸLLMå“åº”
        mock_security_response = AsyncMock()
        mock_security_response.content = """{
    "findings": [
        {
            "type": "SQL Injection",
            "severity": "Critical",
            "line": 8,
            "description": "SQL injection vulnerability in user input",
            "recommendation": "Use parameterized queries"
        },
        {
            "type": "Hardcoded Credentials",
            "severity": "High",
            "line": 5,
            "description": "Hardcoded password and API key in source code",
            "recommendation": "Store credentials in environment variables"
        },
        {
            "type": "Code Injection",
            "severity": "Critical",
            "line": 18,
            "description": "Use of eval() function with user input",
            "recommendation": "Avoid eval() or use safe alternatives"
        },
        {
            "type": "Command Injection",
            "severity": "High",
            "line": 21,
            "description": "Shell command execution with user input",
            "recommendation": "Use subprocess.run with proper argument list"
        }
    ],
    "recommendations": [
        {
            "description": "ç«‹å³ä¿®å¤SQLæ³¨å…¥æ¼æ´ï¼Œä½¿ç”¨å‚æ•°åŒ–æŸ¥è¯¢",
            "priority": "high",
            "category": "security"
        },
        {
            "description": "å°†ç¡¬ç¼–ç å‡­æ®ç§»è‡³ç¯å¢ƒå˜é‡",
            "priority": "high",
            "category": "security"
        },
        {
            "description": "æ›¿æ¢eval()å‡½æ•°ä½¿ç”¨æ›´å®‰å…¨çš„æ›¿ä»£æ–¹æ¡ˆ",
            "priority": "critical",
            "category": "security"
        }
    ],
    "overall_score": 3.2,
    "risk_level": "critical"
}"""
        mock_security_response.usage = {"total_tokens": 1800, "prompt_tokens": 800, "completion_tokens": 1000}
        mock_security_response.model = "gpt-4"

        mock_performance_response = AsyncMock()
        mock_performance_response.content = """{
    "findings": [
        {
            "type": "N+1 Query Problem",
            "severity": "Medium",
            "line": 18,
            "description": "Database queries in loop cause performance issues",
            "recommendation": "Use batch queries or connection pooling"
        },
        {
            "type": "Inefficient Algorithm",
            "severity": "Medium",
            "line": 32,
            "description": "O(nÂ²) complexity can be optimized",
            "recommendation": "Use more efficient algorithm or data structure"
        },
        {
            "type": "Memory Usage",
            "severity": "Low",
            "line": 42,
            "description": "Large data sets stored entirely in memory",
            "recommendation": "Use streaming or pagination"
        }
    ],
    "recommendations": [
        {
            "description": "å®ç°è¿æ¥æ± æ¥å‡å°‘æ•°æ®åº“è¿æ¥å¼€é”€",
            "priority": "medium",
            "category": "performance"
        },
        {
            "description": "ä½¿ç”¨æ‰¹å¤„ç†APIè°ƒç”¨å‡å°‘ç½‘ç»œè¯·æ±‚",
            "priority": "medium",
            "category": "performance"
        },
        {
            "description": "ä¼˜åŒ–ç®—æ³•å¤æ‚åº¦æé«˜å¤„ç†æ•ˆç‡",
            "priority": "low",
            "category": "performance"
        }
    ],
    "overall_score": 6.8,
    "risk_level": "low"
}"""
        mock_performance_response.usage = {"total_tokens": 1500, "prompt_tokens": 700, "completion_tokens": 800}
        mock_performance_response.model = "gpt-4"

        # åˆ›å»ºæ·±åº¦åˆ†æå™¨å®ä¾‹ï¼Œä½¿ç”¨mock LLMå®¢æˆ·ç«¯é¿å…åˆå§‹åŒ–é—®é¢˜
        from unittest.mock import Mock
        mock_logger = Mock()
        mock_logger.info = Mock()
        mock_logger.error = Mock()
        mock_logger.warning = Mock()

        # Mock DeepAnalyzer without real LLM client
        analyzer = DeepAnalyzer.__new__(DeepAnalyzer)
        analyzer.llm_client = mock_llm_client
        analyzer.prompt_manager = Mock()
        analyzer.prompt_manager.get_template.return_value = None
        analyzer.prompt_manager.render_template.return_value = Mock(success=False, content="")
        analyzer.config_manager = Mock()
        analyzer.config_manager.get_section.return_value = {}
        analyzer.default_model = "gpt-4"
        analyzer.default_temperature = 0.3
        analyzer.max_tokens = 4000
        analyzer.max_file_size = 100 * 1024
        analyzer.config = {}
        analyzer.logger = mock_logger

        print(f"   âœ… æ·±åº¦åˆ†æå™¨åˆå§‹åŒ–å®Œæˆ")
        # é‡ç½®_real_æ–¹æ³•
        analyzer.get_supported_analysis_types = DeepAnalyzer.get_supported_analysis_types.__get__(analyzer, DeepAnalyzer)
        analyzer._read_file_content = DeepAnalyzer._read_file_content.__get__(analyzer, DeepAnalyzer)
        analyzer._construct_prompt = DeepAnalyzer._construct_prompt.__get__(analyzer, DeepAnalyzer)
        analyzer._get_fallback_prompt = DeepAnalyzer._get_fallback_prompt.__get__(analyzer, DeepAnalyzer)
        analyzer.analyze_file = DeepAnalyzer.analyze_file.__get__(analyzer, DeepAnalyzer)
        analyzer.analyze_files = DeepAnalyzer.analyze_files.__get__(analyzer, DeepAnalyzer)
        analyzer.format_analysis_result = DeepAnalyzer.format_analysis_result.__get__(analyzer, DeepAnalyzer)

        print(f"   ğŸ”§ æ”¯æŒçš„åˆ†æç±»å‹: {', '.join(analyzer.get_supported_analysis_types())}")

        # 2. æ¼”ç¤ºæ–‡ä»¶å†…å®¹è¯»å–åŠŸèƒ½
        print("\nğŸ“ 2. æ–‡ä»¶å†…å®¹è¯»å–åŠŸèƒ½")

        security_file = project_dir / "security_example.py"
        performance_file = project_dir / "performance_example.py"

        print(f"   ğŸ“„ è¯»å–å®‰å…¨ç¤ºä¾‹æ–‡ä»¶: {security_file.name}")
        content = analyzer._read_file_content(security_file)
        print(f"   ğŸ“ æ–‡ä»¶å¤§å°: {len(content)} å­—ç¬¦")
        print(f"   âœ… æ–‡ä»¶è¯»å–æˆåŠŸï¼ŒåŒ…å« {content.count('def')} ä¸ªå‡½æ•°å®šä¹‰")

        # 3. æ¼”ç¤ºPromptæ„é€ åŠŸèƒ½
        print("\nğŸ› ï¸ 3. Promptæ„é€ åŠŸèƒ½")

        security_request = DeepAnalysisRequest(
            file_path=str(security_file),
            analysis_type="security",
            user_instructions="Focus on critical security vulnerabilities"
        )

        messages = analyzer._construct_prompt(content, security_request)
        print(f"   ğŸ“ æ„é€ çš„promptåŒ…å« {len(messages)} æ¡æ¶ˆæ¯")
        print(f"   ğŸ¤– ç³»ç»Ÿæ¶ˆæ¯é•¿åº¦: {len(messages[0]['content'])} å­—ç¬¦")
        print(f"   ğŸ‘¤ ç”¨æˆ·æ¶ˆæ¯é•¿åº¦: {len(messages[1]['content'])} å­—ç¬¦")
        print(f"   ğŸ”’ promptåŒ…å«å®‰å…¨åˆ†ææŒ‡å¯¼")

        # 4. æ¨¡æ‹ŸLLMåˆ†æå’Œå“åº”è§£æ
        print("\nğŸ§  4. LLMåˆ†æå’Œå“åº”è§£æ")

        # æ¨¡æ‹Ÿå®‰å…¨åˆ†æ
        print("   ğŸ” æ‰§è¡Œå®‰å…¨åˆ†æ...")
        with patch.object(analyzer.llm_client, 'complete', return_value=mock_security_response):
            security_result = asyncio.run(analyzer.analyze_file(security_request))

        print(f"   âœ… å®‰å…¨åˆ†æå®Œæˆ")
        print(f"   â±ï¸  æ‰§è¡Œæ—¶é—´: {security_result.execution_time:.3f}ç§’")
        print(f"   ğŸ“Š tokenä½¿ç”¨: {security_result.token_usage.get('total_tokens', 0)}")
        print(f"   ğŸ¯ å‘ç°é—®é¢˜: {len(security_result.structured_analysis.get('findings', []))} ä¸ª")

        # æ¨¡æ‹Ÿæ€§èƒ½åˆ†æ
        performance_request = DeepAnalysisRequest(
            file_path=str(performance_file),
            analysis_type="performance"
        )

        print("   âš¡ æ‰§è¡Œæ€§èƒ½åˆ†æ...")
        with patch.object(analyzer.llm_client, 'complete', return_value=mock_performance_response):
            performance_result = asyncio.run(analyzer.analyze_file(performance_request))

        print(f"   âœ… æ€§èƒ½åˆ†æå®Œæˆ")
        print(f"   â±ï¸  æ‰§è¡Œæ—¶é—´: {performance_result.execution_time:.3f}ç§’")
        print(f"   ğŸ“Š tokenä½¿ç”¨: {performance_result.token_usage.get('total_tokens', 0)}")
        print(f"   ğŸ¯ å‘ç°é—®é¢˜: {len(performance_result.structured_analysis.get('findings', []))} ä¸ª")

        # 5. æ¼”ç¤ºåˆ†æç»“æœæ ¼å¼åŒ–
        print("\nğŸ“‹ 5. åˆ†æç»“æœæ ¼å¼åŒ–")

        print("   ğŸ“„ æ–‡æœ¬æ ¼å¼æŠ¥å‘Š:")
        text_report = analyzer.format_analysis_result(security_result, "text")
        print(f"   ğŸ“ æŠ¥å‘Šé•¿åº¦: {len(text_report)} å­—ç¬¦")
        print(f"   ğŸ” åŒ…å«å…³é”®è¯: {'æ·±åº¦åˆ†ææŠ¥å‘Š' in text_report}")

        print("   ğŸ“Š JSONæ ¼å¼æŠ¥å‘Š:")
        json_report = analyzer.format_analysis_result(security_result, "json")
        print(f"   ğŸ“ JSONé•¿åº¦: {len(json_report)} å­—ç¬¦")

        print("   ğŸ“ Markdownæ ¼å¼æŠ¥å‘Š:")
        md_report = analyzer.format_analysis_result(security_result, "markdown")
        print(f"   ğŸ“ Markdowné•¿åº¦: {len(md_report)} å­—ç¬¦")
        print(f"   ğŸ” åŒ…å«æ ‡é¢˜: {'# æ·±åº¦åˆ†ææŠ¥å‘Š' in md_report}")

        # 6. æ¼”ç¤ºå¤šæ–‡ä»¶æ‰¹é‡åˆ†æ
        print("\nğŸ”„ 6. å¤šæ–‡ä»¶æ‰¹é‡åˆ†æ")

        batch_requests = [
            security_request,
            performance_request
        ]

        # é…ç½®ä¸åŒçš„å“åº”
        def mock_complete_side_effect(request):
            if "security" in str(request):
                return mock_security_response
            else:
                return mock_performance_response

        print("   ğŸš€ å¯åŠ¨æ‰¹é‡åˆ†æ...")
        with patch.object(analyzer.llm_client, 'complete', side_effect=mock_complete_side_effect):
            batch_results = analyzer.analyze_files(batch_requests)

        print(f"   âœ… æ‰¹é‡åˆ†æå®Œæˆ")
        print(f"   ğŸ“ åˆ†ææ–‡ä»¶æ•°: {len(batch_results)}")
        print(f"   âœ… æˆåŠŸåˆ†æ: {len([r for r in batch_results if r.success])} ä¸ª")
        print(f"   â±ï¸  æ€»æ‰§è¡Œæ—¶é—´: {sum(r.execution_time for r in batch_results):.3f}ç§’")

        # 7. ç”Ÿæˆæ·±åº¦åˆ†ææŠ¥å‘Š
        print("\nğŸ“Š 7. æ·±åº¦åˆ†ææŠ¥å‘Šç”Ÿæˆ")

        report_generator = DeepAnalysisReportGenerator()
        report = report_generator.generate_report(batch_results, "json")

        print("   ğŸ“ˆ æŠ¥å‘Šæ‘˜è¦:")
        summary = report["summary"]
        print(f"   ğŸ“ æ€»æ–‡ä»¶æ•°: {summary['total_files']}")
        print(f"   âœ… æˆåŠŸç‡: {summary['success_rate']:.1%}")
        print(f"   âš ï¸  å‘ç°é—®é¢˜æ€»æ•°: {summary['overall_assessment']['total_issues_found']}")
        print(f"   ğŸ¯ æ€»ä½“é£é™©ç­‰çº§: {summary['overall_assessment']['risk_level']}")

        print("   ğŸ” åˆ†ææ´å¯Ÿ:")
        insights = report["analysis_insights"]
        if insights["common_issues"]:
            print(f"   ğŸ·ï¸  å¸¸è§é—®é¢˜: {', '.join([issue['type'] for issue in insights['common_issues'][:3]])}")

        print("   ğŸ’¡ æ”¹è¿›å»ºè®®:")
        recommendations = report["recommendations"]
        if recommendations["immediate_actions"]:
            print(f"   ğŸš¨ ç«‹å³è¡ŒåŠ¨é¡¹: {len(recommendations['immediate_actions'])} é¡¹")
        if recommendations["security_improvements"]:
            print(f"   ğŸ”’ å®‰å…¨æ”¹è¿›: {len(recommendations['security_improvements'])} é¡¹")

        # 8. ä¿å­˜åˆ†ææŠ¥å‘Š
        print("\nğŸ’¾ 8. ä¿å­˜åˆ†ææŠ¥å‘Š")

        output_dir = Path(temp_dir) / "reports"
        output_dir.mkdir()

        # ä¿å­˜JSONæ ¼å¼æŠ¥å‘Š
        json_path = output_dir / "deep_analysis_report.json"
        success = report_generator.save_report(report, str(json_path), "json")
        if success:
            print(f"   âœ… JSONæŠ¥å‘Šå·²ä¿å­˜: {json_path}")

        # ä¿å­˜æ–‡æœ¬æ ¼å¼æŠ¥å‘Š
        text_path = output_dir / "deep_analysis_report.txt"
        success = report_generator.save_report(report, str(text_path), "text")
        if success:
            print(f"   âœ… æ–‡æœ¬æŠ¥å‘Šå·²ä¿å­˜: {text_path}")

    print("\nğŸ‰ T019æ·±åº¦åˆ†ææ‰§è¡Œæµç¨‹æ¼”ç¤ºå®Œæˆï¼")

    print("\nâœ… T019éªŒæ”¶æ ‡å‡†è¾¾æˆæƒ…å†µ:")
    print("  1. âœ… èƒ½å¤Ÿè¯»å–é€‰å®šæ–‡ä»¶å†…å®¹ - å®‰å…¨è¯»å–é¡¹ç›®æ–‡ä»¶å¹¶å¤„ç†ç¼–ç é—®é¢˜")
    print("  2. âœ… èƒ½å¤Ÿæ„é€ åˆé€‚çš„promptå‘é€ç»™LLM - æ ¹æ®åˆ†æç±»å‹æ„é€ context-aware prompt")
    print("  3. âœ… èƒ½å¤Ÿè§£æLLMè¿”å›çš„åˆ†æå»ºè®® - æ”¯æŒJSONå’Œæ–‡æœ¬æ ¼å¼å“åº”è§£æ")
    print("  4. âœ… èƒ½å¤Ÿæ ¼å¼åŒ–å±•ç¤ºåˆ†æç»“æœ - æä¾›textã€JSONã€Markdownä¸‰ç§æ ¼å¼")

    print("\nğŸš€ T019æ ¸å¿ƒåŠŸèƒ½:")
    print("  - ğŸ“ å®‰å…¨çš„æ–‡ä»¶å†…å®¹è¯»å–ï¼Œæ”¯æŒå¤§æ–‡ä»¶æˆªæ–­å’Œç¼–ç å¤„ç†")
    print("  - ğŸ› ï¸  æ™ºèƒ½promptæ„é€ ï¼Œæ”¯æŒå¤šç§åˆ†æç±»å‹å’Œç”¨æˆ·æŒ‡ä»¤")
    print("  - ğŸ§  LLMæ¥å£é›†æˆï¼Œæ”¯æŒå¼‚æ­¥æ‰¹é‡åˆ†æ")
    print("  - ğŸ“Š å¼ºå¤§çš„å“åº”è§£æï¼Œæ”¯æŒç»“æ„åŒ–JSONå’Œçµæ´»æ–‡æœ¬æ ¼å¼")
    print("  - ğŸ“‹ å¤šæ ·åŒ–ç»“æœå±•ç¤ºï¼Œæ»¡è¶³ä¸åŒåœºæ™¯éœ€æ±‚")
    print("  - ğŸ“ˆ ç»¼åˆæŠ¥å‘Šç”Ÿæˆï¼ŒåŒ…å«æ´å¯Ÿåˆ†æå’Œæ”¹è¿›å»ºè®®")
    print("  - âš¡ é«˜æ€§èƒ½æ‰¹é‡å¤„ç†ï¼Œæ”¯æŒå¤§è§„æ¨¡ä»£ç åˆ†æ")
    print("  - ğŸ¯ å¯é…ç½®åŒ–è®¾è®¡ï¼Œæ”¯æŒè‡ªå®šä¹‰åˆ†ææ¨¡å‹å’Œå‚æ•°")


if __name__ == "__main__":
    demo_t019_deep_analysis()