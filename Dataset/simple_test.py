#!/usr/bin/env python3
"""
ç®€å•çš„Datasetæ¨¡å—åŠŸèƒ½æµ‹è¯•
"""

import sys
import os
from pathlib import Path

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
current_dir = Path(__file__).parent
if str(current_dir) not in sys.path:
    sys.path.insert(0, str(current_dir))

def test_basic_imports():
    """æµ‹è¯•åŸºæœ¬å¯¼å…¥åŠŸèƒ½"""
    print("=" * 50)
    print("Datasetæ¨¡å—ç®€å•æµ‹è¯•")
    print("=" * 50)

    try:
        # æµ‹è¯•æ•°æ®ç±»å‹
        print("1. æµ‹è¯•æ•°æ®ç±»å‹å¯¼å…¥...")
        from data_types import EvaluationTask, EvaluationResult
        print("   âœ“ æ•°æ®ç±»å‹å¯¼å…¥æˆåŠŸ")

        # åˆ›å»ºä¸€ä¸ªç®€å•çš„è¯„ä¼°ä»»åŠ¡
        task = EvaluationTask(
            task_id="test_001",
            dataset_name="test_dataset",
            repo_name="test_repo",
            problem_description="æµ‹è¯•é—®é¢˜æè¿°",
            failing_tests=["test_example"],
            test_command="echo 'test'",
            setup_commands=[],
            timeout=60
        )
        print(f"   âœ“ è¯„ä¼°ä»»åŠ¡åˆ›å»ºæˆåŠŸ: {task.task_id}")

        # æµ‹è¯•å·¥å…·æ¨¡å—
        print("\n2. æµ‹è¯•å·¥å…·æ¨¡å—...")

        # æµ‹è¯•ç®€åŒ–ç‰ˆå·¥å…·
        from utils.metrics_simple import MetricsCalculator
        print("   âœ“ ç®€åŒ–ç‰ˆæŒ‡æ ‡è®¡ç®—å™¨å¯¼å…¥æˆåŠŸ")

        from utils.visualization_simple import EvaluationVisualizer
        print("   âœ“ ç®€åŒ–ç‰ˆå¯è§†åŒ–å·¥å…·å¯¼å…¥æˆåŠŸ")

        from utils.config import EvaluationConfig, ConfigManager
        print("   âœ“ é…ç½®ç®¡ç†å·¥å…·å¯¼å…¥æˆåŠŸ")

        # æµ‹è¯•åŸºç¡€åŠŸèƒ½
        print("\n3. æµ‹è¯•åŸºç¡€åŠŸèƒ½...")

        # æµ‹è¯•æŒ‡æ ‡è®¡ç®—
        calc = MetricsCalculator()
        test_results = [
            {"success": True, "execution_time": 30.5},
            {"success": False, "execution_time": 45.2},
            {"success": True, "execution_time": 28.7}
        ]
        metrics = calc.calculate_basic_metrics(test_results)
        print(f"   âœ“ æŒ‡æ ‡è®¡ç®—æˆåŠŸ: æˆåŠŸç‡={metrics['success_rate']:.2%}")

        # æµ‹è¯•é…ç½®ç®¡ç†
        config_manager = ConfigManager()
        agent_config = config_manager.get_agent_config()
        eval_config = config_manager.get_evaluation_config()
        print(f"   âœ“ é…ç½®åŠ è½½æˆåŠŸ: ä»£ç†è¶…æ—¶={agent_config.get('timeout', 'default')}ç§’")
        print(f"   âœ“ è¯„ä¼°é…ç½®: æœ€å¤§å·¥ä½œçº¿ç¨‹={eval_config.get('max_workers', 'default')}")

        print("\n" + "=" * 50)
        print("âœ… æ‰€æœ‰åŸºç¡€åŠŸèƒ½æµ‹è¯•é€šè¿‡!")
        print("Datasetæ¡†æ¶åŸºæœ¬åŠŸèƒ½æ­£å¸¸")
        print("=" * 50)

        return True

    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    print("å¼€å§‹Datasetæ¨¡å—åŸºç¡€åŠŸèƒ½æµ‹è¯•...")
    
    if test_basic_imports():
        print("\nğŸ‰ DatasetåŸºç¡€æ¡†æ¶å®Œå…¨æ­£å¸¸!")
        exit(0)
    else:
        print("\nâŒ DatasetåŸºç¡€æ¡†æ¶å­˜åœ¨é—®é¢˜")
        exit(1)
