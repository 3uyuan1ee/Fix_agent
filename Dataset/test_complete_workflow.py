#!/usr/bin/env python3
"""
å®Œæ•´å·¥ä½œæµç¨‹æµ‹è¯•

æµ‹è¯•Datasetè¯„ä¼°æ¡†æ¶çš„å®Œæ•´å·¥ä½œæµç¨‹ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®ã€‚
"""

import json
import sys
import tempfile
from pathlib import Path

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
current_dir = Path(__file__).parent
if str(current_dir) not in sys.path:
    sys.path.insert(0, str(current_dir))


def create_mock_swe_bench_data():
    """åˆ›å»ºæ¨¡æ‹Ÿçš„SWE-benchæ•°æ®"""
    mock_data = [
        {
            "instance_id": "test_001",
            "repo": "django/django",
            "base_commit": "abc123",
            "problem_statement": "ä¿®å¤Djangoä¸­çš„è®¤è¯é—®é¢˜",
            "patch": "@@ -1,3 +1,3 @@\n def authenticate(username, password):\n-    return username == password\n+    return check_password(username, password)",
            "test_patch": "",
            "FAIL_TO_PASS": ["test_auth.py::test_basic_auth"],
            "PASS_TO_PASS": ["test_auth.py::test_existing_feature"],
        },
        {
            "instance_id": "test_002",
            "repo": "psf/requests",
            "base_commit": "def456",
            "problem_statement": "ä¿®å¤requestsåº“ä¸­çš„è¶…æ—¶å¤„ç†",
            "patch": "@@ -10,7 +10,7 @@\n def send_request(url, timeout=None):\n-    if timeout is None:\n-        timeout = 30\n+    timeout = timeout or 30\n     return requests.get(url, timeout=timeout)",
            "test_patch": "",
            "FAIL_TO_PASS": ["test_timeout.py::test_custom_timeout"],
            "PASS_TO_PASS": ["test_basic.py::test_get_request"],
        },
    ]
    return mock_data


def create_mock_bugs_in_py_structure():
    """åˆ›å»ºæ¨¡æ‹Ÿçš„BugsInPyæ•°æ®ç»“æ„"""
    import shutil
    import tempfile

    # åˆ›å»ºä¸´æ—¶ç›®å½•ç»“æ„
    temp_dir = Path(tempfile.mkdtemp())
    bugs_dir = temp_dir / "bugs" / "django"
    bugs_dir.mkdir(parents=True, exist_ok=True)

    # åˆ›å»ºbugæ•°æ®
    bug_dir = bugs_dir / "bug_001"
    bug_dir.mkdir(exist_ok=True)

    # bug.json
    bug_data = {
        "type": "authentication",
        "severity": "medium",
        "description": "Djangoè®¤è¯ç³»ç»Ÿä¸­çš„å¯†ç éªŒè¯å­˜åœ¨é—®é¢˜",
    }
    with open(bug_dir / "bug.json", "w") as f:
        json.dump(bug_data, f)

    # failing_test.txt
    with open(bug_dir / "failing_test.txt", "w") as f:
        f.write("tests/test_auth.py::test_password_validation\n")

    # patch.txt
    patch_content = """@@ -15,2 +15,2 @@
 def validate_password(password, user):
-    return len(password) >= 6
+    return len(password) >= 8 and any(c.isdigit() for c in password)
"""
    with open(bug_dir / "patch.txt", "w") as f:
        f.write(patch_content)

    return str(temp_dir)


def test_swe_bench_loader():
    """æµ‹è¯•SWE-benchåŠ è½½å™¨"""
    print("1. æµ‹è¯•SWE-benchåŠ è½½å™¨...")

    try:
        from loaders.swe_bench import SWEBenchLoader

        # åˆ›å»ºä¸´æ—¶æ•°æ®æ–‡ä»¶
        with tempfile.NamedTemporaryFile(mode="w", suffix=".json", delete=False) as f:
            json.dump(create_mock_swe_bench_data(), f)
            temp_file = f.name

        # ä¸´æ—¶ä¿®æ”¹åŠ è½½å™¨çš„æ•°æ®æ–‡ä»¶è·¯å¾„
        loader = SWEBenchLoader("./temp_test")

        # ç›´æ¥æµ‹è¯•æ•°æ®è½¬æ¢
        mock_item = create_mock_swe_bench_data()[0]
        task = loader._convert_to_evaluation_task(mock_item)

        if task:
            print(f"   âœ“ SWE-benchä»»åŠ¡è½¬æ¢æˆåŠŸ: {task.task_id}")
            print(f"     - ä»“åº“: {task.repo_name}")
            print(f"     - æµ‹è¯•æ•°é‡: {len(task.failing_tests)}")
            return True
        else:
            print("   âŒ SWE-benchä»»åŠ¡è½¬æ¢å¤±è´¥")
            return False

    except Exception as e:
        print(f"   âŒ SWE-benchåŠ è½½å™¨æµ‹è¯•å¤±è´¥: {e}")
        return False


def test_bugs_in_py_loader():
    """æµ‹è¯•BugsInPyåŠ è½½å™¨"""
    print("2. æµ‹è¯•BugsInPyåŠ è½½å™¨...")

    try:
        from loaders.bugs_in_py import BugsInPyLoader

        # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®ç»“æ„
        mock_path = create_mock_bugs_in_py_structure()

        loader = BugsInPyLoader(mock_path)

        # æµ‹è¯•å•ä¸ªbugåŠ è½½
        bugs_dir = Path(mock_path) / "bugs" / "django" / "bug_001"
        task = loader._load_single_bug("django", bugs_dir)

        if task:
            print(f"   âœ“ BugsInPyä»»åŠ¡åŠ è½½æˆåŠŸ: {task.task_id}")
            print(f"     - Bugç±»å‹: {task.repo_info.get('bug_type', 'unknown')}")
            print(f"     - æµ‹è¯•æ•°é‡: {len(task.failing_tests)}")
            return True
        else:
            print("   âŒ BugsInPyä»»åŠ¡åŠ è½½å¤±è´¥")
            return False

    except Exception as e:
        print(f"   âŒ BugsInPyåŠ è½½å™¨æµ‹è¯•å¤±è´¥: {e}")
        return False


def test_evaluation_framework():
    """æµ‹è¯•è¯„ä¼°æ¡†æ¶"""
    print("3. æµ‹è¯•è¯„ä¼°æ¡†æ¶...")

    try:
        from core.evaluation import EvaluationFramework
        from data_types import EvaluationTask

        # åˆ›å»ºæ¨¡æ‹Ÿä»»åŠ¡
        task = EvaluationTask(
            task_id="mock_test_001",
            dataset_name="mock_dataset",
            repo_name="test_repo",
            problem_description="æ¨¡æ‹Ÿé—®é¢˜æè¿°",
            failing_tests=["test_example.py::test_func"],
            test_command="echo 'æ¨¡æ‹Ÿæµ‹è¯•é€šè¿‡'",
            setup_commands=["echo 'æ¨¡æ‹Ÿè®¾ç½®'"],
            timeout=30,
            repo_info={"language": "python", "framework": "django"},
        )

        # åˆ›å»ºè¯„ä¼°æ¡†æ¶
        framework = EvaluationFramework()

        print("   âœ“ è¯„ä¼°æ¡†æ¶åˆ›å»ºæˆåŠŸ")
        print(f"   âœ“ æ¨¡æ‹Ÿä»»åŠ¡åˆ›å»ºæˆåŠŸ: {task.task_id}")

        # æµ‹è¯•é…ç½®åŠ è½½
        config = {"agent": {"model": "test-model"}, "evaluation": {"max_workers": 2}}
        framework.config = config

        print("   âœ“ è¯„ä¼°æ¡†æ¶é…ç½®æˆåŠŸ")
        return True

    except Exception as e:
        print(f"   âŒ è¯„ä¼°æ¡†æ¶æµ‹è¯•å¤±è´¥: {e}")
        return False


def test_metrics_calculation():
    """æµ‹è¯•æŒ‡æ ‡è®¡ç®—"""
    print("4. æµ‹è¯•æŒ‡æ ‡è®¡ç®—...")

    try:
        from utils.metrics_simple import MetricsCalculator

        # åˆ›å»ºæµ‹è¯•ç»“æœ
        test_results = [
            {
                "task_id": "test_001",
                "success": True,
                "execution_time": 25.5,
                "agent_actions": ["analyze", "fix", "validate"],
                "error": None,
            },
            {
                "task_id": "test_002",
                "success": False,
                "execution_time": 45.2,
                "agent_actions": ["analyze", "fix"],
                "error": "TimeoutError",
            },
            {
                "task_id": "test_003",
                "success": True,
                "execution_time": 32.1,
                "agent_actions": ["analyze", "fix", "validate"],
                "error": None,
            },
        ]

        calc = MetricsCalculator()
        metrics = calc.calculate_basic_metrics(test_results)

        print(f"   âœ“ æŒ‡æ ‡è®¡ç®—æˆåŠŸ:")
        print(f"     - æˆåŠŸç‡: {metrics['success_rate']:.2%}")
        print(f"     - å¹³å‡æ‰§è¡Œæ—¶é—´: {metrics['average_execution_time']:.2f}ç§’")
        print(f"     - æ¯å°æ—¶ä»»åŠ¡æ•°: {metrics['tasks_per_hour']:.1f}")

        return True

    except Exception as e:
        print(f"   âŒ æŒ‡æ ‡è®¡ç®—æµ‹è¯•å¤±è´¥: {e}")
        return False


def test_complete_workflow():
    """æµ‹è¯•å®Œæ•´å·¥ä½œæµç¨‹"""
    print("\n" + "=" * 60)
    print("Datasetè¯„ä¼°æ¡†æ¶å®Œæ•´å·¥ä½œæµç¨‹æµ‹è¯•")
    print("=" * 60)

    tests = [
        test_swe_bench_loader,
        test_bugs_in_py_loader,
        test_evaluation_framework,
        test_metrics_calculation,
    ]

    passed = 0
    total = len(tests)

    for test_func in tests:
        try:
            if test_func():
                passed += 1
            print()  # ç©ºè¡Œåˆ†éš”
        except Exception as e:
            print(f"   ğŸ’¥ æµ‹è¯•å¼‚å¸¸: {e}")
            print()

    print("=" * 60)
    print(f"å®Œæ•´å·¥ä½œæµç¨‹æµ‹è¯•ç»“æœ: {passed}/{total} é€šè¿‡")

    if passed == total:
        print("ğŸ‰ Datasetè¯„ä¼°æ¡†æ¶å®Œå…¨å¯ç”¨!")
        print("âœ… æ‰€æœ‰æ ¸å¿ƒåŠŸèƒ½æ­£å¸¸å·¥ä½œ")
        print("âœ… æ•°æ®åŠ è½½å™¨å·¥ä½œæ­£å¸¸")
        print("âœ… è¯„ä¼°æ¡†æ¶å·¥ä½œæ­£å¸¸")
        print("âœ… æŒ‡æ ‡è®¡ç®—å·¥ä½œæ­£å¸¸")
        print("\nå¯ä»¥å¼€å§‹ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤è¿›è¡Œå®é™…è¯„ä¼°:")
        print("  python run_evaluation.py --dataset swe-bench --samples 10")
        print("  python run_evaluation.py --dataset bugsinpy --samples 5 --debug")
    else:
        print("âš ï¸ éƒ¨åˆ†åŠŸèƒ½å­˜åœ¨é—®é¢˜ï¼Œä½†åŸºç¡€æ¡†æ¶å¯ç”¨")

    print("=" * 60)

    return passed == total


if __name__ == "__main__":
    success = test_complete_workflow()
    exit(0 if success else 1)
