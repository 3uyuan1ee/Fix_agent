#!/usr/bin/env python3
"""
Datasetè¯„ä¼°æ¡†æ¶ - ä¸»å…¥å£è„šæœ¬

å®ç°å®Œå…¨éš”ç¦»çš„è‡ªåŠ¨åŒ–è¯„ä¼°ï¼Œæ”¯æŒSWE-benchæ ‡å‡†æµç¨‹ã€‚
"""

import argparse
import asyncio
import json
import logging
import os
import sys
import tempfile
from pathlib import Path
from typing import Any, Dict, List, Optional

# æ·»åŠ Datasetç›®å½•åˆ°Pythonè·¯å¾„
current_dir = Path(__file__).parent
sys.path.insert(0, str(current_dir))

from core.evaluation import DatasetEvaluationFramework
from loaders.swe_bench import SWEBenchLiteLoader
from utils.config import Config
from utils.file_utils import setup_logging, create_secure_temp_filename


def parse_arguments():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description="Datasetè¯„ä¼°æ¡†æ¶ - å®Œå…¨è‡ªåŠ¨åŒ–çš„SWE-benchè¯„ä¼°",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # ç”Ÿæˆé¢„æµ‹æ–‡ä»¶
  python main.py --mode generate --dataset ./datasets/swe-bench-lite.jsonl --samples 10

  # è¿è¡ŒSWE-benchæ ‡å‡†è¯„ä¼°
  python main.py --mode evaluate --predictions ./datasets/predictions/test_predictions.jsonl

  # å®Œæ•´æµç¨‹ï¼ˆç”Ÿæˆ+è¯„ä¼°ï¼‰
  python main.py --mode complete --dataset ./datasets/swe-bench-lite.jsonl --samples 50
        """
    )

    parser.add_argument(
        "--mode",
        choices=["generate", "evaluate", "complete"],
        default="complete",
        help="è¿è¡Œæ¨¡å¼ï¼šgenerate(ç”Ÿæˆé¢„æµ‹)ã€evaluate(è¿è¡Œè¯„ä¼°)ã€complete(å®Œæ•´æµç¨‹)"
    )

    parser.add_argument(
        "--dataset",
        default="./datasets/swe-bench-lite.jsonl",
        help="SWE-benchæ•°æ®é›†è·¯å¾„"
    )

    parser.add_argument(
        "--predictions",
        default="./datasets/predictions/test_predictions.jsonl",
        help="é¢„æµ‹æ–‡ä»¶è·¯å¾„"
    )

    parser.add_argument(
        "--samples",
        type=int,
        default=10,
        help="å¤„ç†çš„æ ·æœ¬æ•°é‡ï¼ˆç”¨äºæµ‹è¯•ï¼‰"
    )

    parser.add_argument(
        "--swe-bench-path",
        default="./datasets/SWE-bench",
        help="SWE-benchä»“åº“è·¯å¾„"
    )

    parser.add_argument(
        "--testbed",
        default="./testbed",
        help="æµ‹è¯•åºŠç›®å½•"
    )

    parser.add_argument(
        "--log-dir",
        default="./logs",
        help="æ—¥å¿—ç›®å½•"
    )

    parser.add_argument(
        "--temp-dir",
        default="./temp",
        help="ä¸´æ—¶æ–‡ä»¶ç›®å½•"
    )

    parser.add_argument(
        "--results-dir",
        default="./results",
        help="ç»“æœè¾“å‡ºç›®å½•"
    )

    parser.add_argument(
        "--config",
        default="./config.json",
        help="é…ç½®æ–‡ä»¶è·¯å¾„"
    )

    parser.add_argument(
        "--debug",
        action="store_true",
        help="å¯ç”¨è°ƒè¯•æ¨¡å¼"
    )

    return parser.parse_args()


async def generate_predictions(args: argparse.Namespace) -> bool:
    """ç”Ÿæˆé¢„æµ‹æ–‡ä»¶"""
    try:
        # è®¾ç½®æ—¥å¿—
        logger = setup_logging(
            log_level=logging.DEBUG if args.debug else logging.INFO,
            log_dir=args.log_dir,
            mode="generate"
        )
        logger.info(f"å¼€å§‹ç”Ÿæˆé¢„æµ‹æ–‡ä»¶ï¼Œæ•°æ®é›†: {args.dataset}")

        # åˆ›å»ºå¿…è¦çš„ç›®å½•
        Path(args.temp_dir).mkdir(parents=True, exist_ok=True)
        Path(args.predictions).parent.mkdir(parents=True, exist_ok=True)

        # åŠ è½½æ•°æ®é›†
        loader = SWEBenchLiteLoader(args.dataset)
        tasks = loader.load_tasks(sample_size=args.samples)

        if not tasks:
            logger.error("æœªåŠ è½½åˆ°ä»»ä½•ä»»åŠ¡")
            return False

        logger.info(f"åŠ è½½äº† {len(tasks)} ä¸ªä»»åŠ¡")

        # åˆ›å»ºè¯„ä¼°æ¡†æ¶
        framework = DatasetEvaluationFramework(
            config_path=args.config,
            temp_dir=args.temp_dir,
            debug=args.debug
        )

        # åˆå§‹åŒ–æ¡†æ¶
        if not await framework.initialize():
            logger.error("è¯„ä¼°æ¡†æ¶åˆå§‹åŒ–å¤±è´¥")
            return False

        # ç”Ÿæˆé¢„æµ‹
        predictions = await framework.generate_predictions(tasks)

        # ä¿å­˜é¢„æµ‹æ–‡ä»¶
        loader.save_predictions(predictions, args.predictions)

        logger.info(f"é¢„æµ‹æ–‡ä»¶å·²ä¿å­˜åˆ°: {args.predictions}")
        logger.info(f"æˆåŠŸç”Ÿæˆ {len(predictions)} ä¸ªé¢„æµ‹")

        return True

    except Exception as e:
        logger.error(f"ç”Ÿæˆé¢„æµ‹æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        if args.debug:
            import traceback
            traceback.print_exc()
        return False


async def run_evaluation(args: argparse.Namespace) -> bool:
    """è¿è¡ŒSWE-benchæ ‡å‡†è¯„ä¼°"""
    try:
        # è®¾ç½®æ—¥å¿—
        logger = setup_logging(
            log_level=logging.DEBUG if args.debug else logging.INFO,
            log_dir=args.log_dir,
            mode="evaluate"
        )
        logger.info(f"å¼€å§‹è¿è¡ŒSWE-benchè¯„ä¼°ï¼Œé¢„æµ‹æ–‡ä»¶: {args.predictions}")

        # éªŒè¯é¢„æµ‹æ–‡ä»¶å­˜åœ¨
        if not Path(args.predictions).exists():
            logger.error(f"é¢„æµ‹æ–‡ä»¶ä¸å­˜åœ¨: {args.predictions}")
            return False

        # åˆ›å»ºè¯„ä¼°æ¡†æ¶
        framework = DatasetEvaluationFramework(
            config_path=args.config,
            swe_bench_path=args.swe_bench_path,
            testbed_path=args.testbed,
            temp_dir=args.temp_dir,
            debug=args.debug
        )

        # è¿è¡ŒSWE-benchè¯„ä¼°
        results = await framework.run_swe_bench_evaluation(
            predictions_path=args.predictions,
            log_dir=args.log_dir
        )

        # ä¿å­˜ç»“æœ
        results_file = Path(args.results_dir) / "evaluation_results.json"
        results_file.parent.mkdir(parents=True, exist_ok=True)

        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)

        logger.info(f"è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: {results_file}")

        # ç”ŸæˆæŠ¥å‘Š
        await framework.generate_evaluation_report(results, args.results_dir)

        return True

    except Exception as e:
        logger.error(f"è¿è¡Œè¯„ä¼°æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        if args.debug:
            import traceback
            traceback.print_exc()
        return False


async def run_complete_workflow(args: argparse.Namespace) -> bool:
    """è¿è¡Œå®Œæ•´å·¥ä½œæµç¨‹"""
    logger = setup_logging(
        log_level=logging.DEBUG if args.debug else logging.INFO,
        log_dir=args.log_dir,
        mode="complete"
    )

    logger.info("å¼€å§‹è¿è¡Œå®Œæ•´è¯„ä¼°æµç¨‹")

    # ç¬¬ä¸€æ­¥ï¼šç”Ÿæˆé¢„æµ‹
    logger.info("=== ç¬¬ä¸€æ­¥ï¼šç”Ÿæˆé¢„æµ‹æ–‡ä»¶ ===")
    success = await generate_predictions(args)

    if not success:
        logger.error("é¢„æµ‹æ–‡ä»¶ç”Ÿæˆå¤±è´¥ï¼Œç»ˆæ­¢æµç¨‹")
        return False

    # ç¬¬äºŒæ­¥ï¼šè¿è¡Œè¯„ä¼°
    logger.info("=== ç¬¬äºŒæ­¥ï¼šè¿è¡ŒSWE-benchè¯„ä¼° ===")
    success = await run_evaluation(args)

    if success:
        logger.info("=== å®Œæ•´æµç¨‹æ‰§è¡ŒæˆåŠŸ ===")
    else:
        logger.error("=== è¯„ä¼°é˜¶æ®µå¤±è´¥ ===")

    return success


async def main():
    """ä¸»å‡½æ•°"""
    args = parse_arguments()

    # æ˜¾ç¤ºé…ç½®ä¿¡æ¯
    print("=" * 60)
    print("ğŸš€ Datasetè¯„ä¼°æ¡†æ¶ - å®Œå…¨è‡ªåŠ¨åŒ–çš„SWE-benchè¯„ä¼°")
    print("=" * 60)
    print(f"è¿è¡Œæ¨¡å¼: {args.mode}")
    print(f"æ•°æ®é›†: {args.dataset}")
    print(f"æ ·æœ¬æ•°é‡: {args.samples}")
    print(f"è°ƒè¯•æ¨¡å¼: {args.debug}")
    print("=" * 60)

    # æ ¹æ®æ¨¡å¼æ‰§è¡Œç›¸åº”æ“ä½œ
    if args.mode == "generate":
        success = await generate_predictions(args)
    elif args.mode == "evaluate":
        success = await run_evaluation(args)
    elif args.mode == "complete":
        success = await run_complete_workflow(args)
    else:
        print(f"âŒ ä¸æ”¯æŒçš„æ¨¡å¼: {args.mode}")
        return 1

    if success:
        print("\nâœ… ä»»åŠ¡æ‰§è¡ŒæˆåŠŸ")
        return 0
    else:
        print("\nâŒ ä»»åŠ¡æ‰§è¡Œå¤±è´¥")
        return 1


if __name__ == "__main__":
    sys.exit(asyncio.run(main()))