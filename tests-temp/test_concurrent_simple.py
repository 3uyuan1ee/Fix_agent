#!/usr/bin/env python3
"""
å¤šæ–‡ä»¶å¹¶å‘åˆ†æåŠŸèƒ½æµ‹è¯•è„šæœ¬ï¼ˆç®€åŒ–ç‰ˆï¼‰
éªŒè¯ç³»ç»Ÿå¤„ç†å¤šä¸ªæ–‡ä»¶å¹¶å‘åˆ†æçš„èƒ½åŠ›å’Œæ€§èƒ½
"""

import sys
import os
import time
import concurrent.futures
import tempfile
from pathlib import Path
from unittest.mock import Mock
from datetime import datetime, timedelta
from typing import List, Dict, Any

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent / "src"))

def test_concurrent_analysis_performance():
    """æµ‹è¯•å¹¶å‘åˆ†ææ€§èƒ½"""
    print("ğŸ” æµ‹è¯•å¹¶å‘åˆ†ææ€§èƒ½...")

    try:
        # åˆ›å»ºæµ‹è¯•æ–‡ä»¶
        temp_dir = tempfile.mkdtemp()
        test_files = []

        print("   åˆ›å»ºæµ‹è¯•æ–‡ä»¶:")
        for i in range(30):
            file_path = Path(temp_dir) / f"concurrent_test_{i:02d}.py"
            content = f'''#!/usr/bin/env python3
"""
å¹¶å‘æµ‹è¯•æ–‡ä»¶ {i}
"""

def function_{i}():
    """å‡½æ•° {i}"""
    result = 0
    for j in range({20 + i * 3}):
        result += j * (i + 1)
    return result

class TestClass{i}:
    """æµ‹è¯•ç±» {i}"""

    def __init__(self, name):
        self.name = name
        self.data = list(range({10 + i}))

    def process_data(self):
        """å¤„ç†æ•°æ®"""
        return [x * 2 for x in self.data]

def helper_function_{i}(x, y):
    """è¾…åŠ©å‡½æ•°"""
    return x + y

if __name__ == "__main__":
    print("Running test file {i}")
    obj = TestClass{i}("test")
    print(function_{i}())
    print(obj.process_data())
    print(helper_function_{i}(10, 20))
'''
            file_path.write_text(content)
            test_files.append(str(file_path))

        print(f"     - åˆ›å»ºæ–‡ä»¶æ•°: {len(test_files)}")

        # åˆ†æå‡½æ•°
        def analyze_file(file_path: str) -> Dict[str, Any]:
            """æ–‡ä»¶åˆ†æå‡½æ•°"""
            start_time = time.time()

            try:
                # è¯»å–æ–‡ä»¶
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()

                # æ¨¡æ‹Ÿåˆ†æè€—æ—¶
                file_size = len(content)
                analysis_time = 0.02 + (file_size / 10000)
                time.sleep(analysis_time)

                # åˆ†æå†…å®¹
                lines = content.splitlines()
                functions = len([line for line in lines if line.strip().startswith('def ')])
                classes = len([line for line in lines if line.strip().startswith('class ')])
                imports = len([line for line in lines if line.strip().startswith('import ') or 'from ' in line])

                # è®¡ç®—å¤æ‚åº¦æŒ‡æ ‡
                complexity_score = functions * 2.5 + classes * 3.0 + imports * 1.5

                end_time = time.time()

                return {
                    'file_path': Path(file_path).name,
                    'file_size': file_size,
                    'line_count': len(lines),
                    'function_count': functions,
                    'class_count': classes,
                    'import_count': imports,
                    'complexity_score': round(complexity_score, 2),
                    'analysis_time': end_time - start_time,
                    'success': True,
                    'timestamp': datetime.now().isoformat()
                }

            except Exception as e:
                return {
                    'file_path': Path(file_path).name,
                    'error': str(e),
                    'analysis_time': time.time() - start_time,
                    'success': False
                }

        # æµ‹è¯•ä¸²è¡Œåˆ†æ
        print("\n   æ‰§è¡Œä¸²è¡Œåˆ†æ:")
        serial_start = time.time()
        serial_results = []

        for file_path in test_files:
            result = analyze_file(file_path)
            serial_results.append(result)

        serial_time = time.time() - serial_start
        serial_success = sum(1 for r in serial_results if r.get('success'))

        print(f"     - ä¸²è¡Œåˆ†ææ—¶é—´: {serial_time:.3f}s")
        print(f"     - æˆåŠŸåˆ†ææ•°: {serial_success}/{len(test_files)}")
        print(f"     - å¹³å‡æ¯æ–‡ä»¶æ—¶é—´: {serial_time/len(test_files):.3f}s")

        # æµ‹è¯•ä¸åŒå¹¶å‘çº§åˆ«
        print("\n   æµ‹è¯•ä¸åŒå¹¶å‘çº§åˆ«:")
        concurrency_tests = [2, 4, 8, 16]
        concurrent_results = {}

        for max_workers in concurrency_tests:
            print(f"\n     å¹¶å‘çº§åˆ« {max_workers}:")
            concurrent_start = time.time()
            worker_results = []

            with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
                # æäº¤æ‰€æœ‰ä»»åŠ¡
                future_to_file = {
                    executor.submit(analyze_file, file_path): file_path
                    for file_path in test_files
                }

                # æ”¶é›†ç»“æœ
                for future in concurrent.futures.as_completed(future_to_file):
                    try:
                        result = future.result()
                        worker_results.append(result)
                    except Exception as e:
                        print(f"       - æ‰§è¡Œå¼‚å¸¸: {e}")

            concurrent_time = time.time() - concurrent_start
            concurrent_success = sum(1 for r in worker_results if r.get('success'))
            speedup = serial_time / concurrent_time if concurrent_time > 0 else 0

            concurrent_results[max_workers] = {
                'time': concurrent_time,
                'success_count': concurrent_success,
                'speedup': speedup,
                'throughput': concurrent_success / concurrent_time
            }

            print(f"       - åˆ†ææ—¶é—´: {concurrent_time:.3f}s")
            print(f"       - æˆåŠŸåˆ†æ: {concurrent_success}/{len(test_files)}")
            print(f"       - æ€§èƒ½æå‡: {speedup:.2f}x")
            print(f"       - ååé‡: {concurrent_success/concurrent_time:.1f} æ–‡ä»¶/ç§’")

        # åˆ†ææœ€ä½³å¹¶å‘çº§åˆ«
        print("\n   å¹¶å‘æ€§èƒ½åˆ†æ:")
        best_workers = max(concurrent_results.keys(),
                         key=lambda k: concurrent_results[k]['throughput'])
        best_result = concurrent_results[best_workers]

        print(f"     - æœ€ä½³å¹¶å‘æ•°: {best_workers}")
        print(f"     - æœ€ä½³ååé‡: {best_result['throughput']:.1f} æ–‡ä»¶/ç§’")
        print(f"     - æœ€å¤§æ€§èƒ½æå‡: {best_result['speedup']:.2f}x")

        # éªŒè¯ç»“æœä¸€è‡´æ€§
        serial_files = set(r['file_path'] for r in serial_results if r.get('success'))
        best_files = set()

        # é‡æ–°è¿è¡Œæœ€ä½³å¹¶å‘æ•°æ¥éªŒè¯ä¸€è‡´æ€§
        with concurrent.futures.ThreadPoolExecutor(max_workers=best_workers) as executor:
            future_to_file = {
                executor.submit(analyze_file, file_path): file_path
                for file_path in test_files
            }

            for future in concurrent.futures.as_completed(future_to_file):
                try:
                    result = future.result()
                    if result.get('success'):
                        best_files.add(result['file_path'])
                except Exception:
                    pass

        consistency = serial_files == best_files
        print(f"     - ç»“æœä¸€è‡´æ€§: {'âœ…' if consistency else 'âŒ'}")

        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        import shutil
        shutil.rmtree(temp_dir)

        # æ€§èƒ½éªŒè¯
        performance_good = (
            best_result['speedup'] > 3.0 and
            best_result['success_count'] >= len(test_files) * 0.9 and
            consistency
        )

        print(f"     - å¹¶å‘åˆ†ææ€§èƒ½éªŒè¯: {'âœ…' if performance_good else 'âŒ'}")
        return performance_good

    except Exception as e:
        print(f"âŒ å¹¶å‘åˆ†ææ€§èƒ½æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_concurrent_error_handling():
    """æµ‹è¯•å¹¶å‘åˆ†æçš„é”™è¯¯å¤„ç†"""
    print("\nğŸ” æµ‹è¯•å¹¶å‘åˆ†æçš„é”™è¯¯å¤„ç†...")

    try:
        # åˆ›å»ºåŒ…å«å„ç§é—®é¢˜çš„æµ‹è¯•æ–‡ä»¶
        temp_dir = tempfile.mkdtemp()
        test_files = []

        print("   åˆ›å»ºåŒ…å«é”™è¯¯çš„æµ‹è¯•æ–‡ä»¶:")
        for i in range(20):
            file_path = Path(temp_dir) / f"error_test_{i:02d}.py"

            if i % 4 == 0:
                # ç©ºæ–‡ä»¶
                content = ''
            elif i % 4 == 1:
                # è¯­æ³•é”™è¯¯
                content = f'''# è¯­æ³•é”™è¯¯æ–‡ä»¶ {i}
def broken_function_{i}(
    # ç¼ºå°‘å³æ‹¬å·
    return "broken"
'''
            elif i % 4 == 2:
                # æ­£å¸¸æ–‡ä»¶
                content = f'''# æ­£å¸¸æ–‡ä»¶ {i}
def normal_function_{i}():
    return {i}

class NormalClass{i}:
    pass
'''
            else:
                # å¤§æ–‡ä»¶
                content = 'x' * 5000 + f'\n# å¤§æ–‡ä»¶ {i}\n'

            file_path.write_text(content, encoding='utf-8')
            test_files.append(str(file_path))

        print(f"     - åˆ›å»ºæ–‡ä»¶æ•°: {len(test_files)}")

        # å¸¦é”™è¯¯å¤„ç†çš„åˆ†æå‡½æ•°
        def safe_analyze_file(file_path: str) -> Dict[str, Any]:
            """å®‰å…¨çš„æ–‡ä»¶åˆ†æå‡½æ•°"""
            start_time = time.time()

            try:
                # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                if not os.path.exists(file_path):
                    return {
                        'file_path': Path(file_path).name,
                        'error': 'File not found',
                        'error_type': 'file_error',
                        'success': False
                    }

                # æ£€æŸ¥æ–‡ä»¶å¤§å°
                file_size = os.path.getsize(file_path)

                # è¯»å–æ–‡ä»¶
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                except UnicodeDecodeError:
                    return {
                        'file_path': Path(file_path).name,
                        'error': 'Encoding error',
                        'error_type': 'encoding_error',
                        'file_size': file_size,
                        'success': False
                    }

                # æ£€æŸ¥ç©ºæ–‡ä»¶
                if not content.strip():
                    return {
                        'file_path': Path(file_path).name,
                        'file_size': file_size,
                        'line_count': 0,
                        'function_count': 0,
                        'class_count': 0,
                        'is_empty': True,
                        'success': True,
                        'analysis_time': time.time() - start_time
                    }

                # æ¨¡æ‹Ÿåˆ†æ
                time.sleep(0.01)  # æ¨¡æ‹Ÿåˆ†ææ—¶é—´

                lines = content.splitlines()
                functions = len([line for line in lines if line.strip().startswith('def ')])
                classes = len([line for line in lines if line.strip().startswith('class ')])

                # ç®€å•è¯­æ³•æ£€æŸ¥
                syntax_issues = 0
                for line_num, line in enumerate(lines, 1):
                    if line.count('(') != line.count(')'):
                        syntax_issues += 1

                return {
                    'file_path': Path(file_path).name,
                    'file_size': file_size,
                    'line_count': len(lines),
                    'function_count': functions,
                    'class_count': classes,
                    'syntax_issues': syntax_issues,
                    'has_errors': syntax_issues > 0,
                    'success': True,
                    'analysis_time': time.time() - start_time
                }

            except Exception as e:
                return {
                    'file_path': Path(file_path).name,
                    'error': str(e),
                    'error_type': 'unexpected_error',
                    'success': False,
                    'analysis_time': time.time() - start_time
                }

        # æ‰§è¡Œå¹¶å‘åˆ†æï¼ˆå¸¦é”™è¯¯å¤„ç†ï¼‰
        print("\n   æ‰§è¡Œå¸¦é”™è¯¯å¤„ç†çš„å¹¶å‘åˆ†æ:")
        max_workers = 6

        start_time = time.time()
        all_results = []

        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_file = {
                executor.submit(safe_analyze_file, file_path): file_path
                for file_path in test_files
            }

            # æ”¶é›†ç»“æœ
            for future in concurrent.futures.as_completed(future_to_file):
                try:
                    result = future.result()
                    all_results.append(result)
                except Exception as e:
                    # è¿™ç§æƒ…å†µç†è®ºä¸Šä¸åº”è¯¥å‘ç”Ÿï¼Œå› ä¸ºæˆ‘ä»¬å·²ç»åœ¨safe_analyze_fileä¸­å¤„ç†äº†æ‰€æœ‰å¼‚å¸¸
                    file_path = future_to_file[future]
                    all_results.append({
                        'file_path': Path(file_path).name,
                        'error': f'Executor error: {str(e)}',
                        'error_type': 'executor_error',
                        'success': False
                    })

        total_time = time.time() - start_time

        # ç»Ÿè®¡ç»“æœ
        successful = sum(1 for r in all_results if r.get('success'))
        with_errors = len(all_results) - successful
        empty_files = sum(1 for r in all_results if r.get('is_empty'))
        files_with_syntax_issues = sum(1 for r in all_results if r.get('has_errors'))

        # é”™è¯¯ç±»å‹ç»Ÿè®¡
        error_types = {}
        for result in all_results:
            if not result.get('success'):
                error_type = result.get('error_type', 'unknown')
                error_types[error_type] = error_types.get(error_type, 0) + 1

        print(f"     - æ€»åˆ†ææ—¶é—´: {total_time:.3f}s")
        print(f"     - æˆåŠŸåˆ†æ: {successful}")
        print(f"     - åŒ…å«é”™è¯¯: {with_errors}")
        print(f"     - ç©ºæ–‡ä»¶: {empty_files}")
        print(f"     - è¯­æ³•é—®é¢˜æ–‡ä»¶: {files_with_syntax_issues}")
        print(f"     - é”™è¯¯ç±»å‹: {error_types}")
        print(f"     - å¹³å‡å¤„ç†æ—¶é—´: {total_time/len(test_files):.3f}s/æ–‡ä»¶")

        # éªŒè¯é”™è¯¯å¤„ç†
        error_handling_good = (
            len(all_results) == len(test_files) and  # æ‰€æœ‰æ–‡ä»¶éƒ½è¢«å¤„ç†
            successful > 0 and  # æœ‰æˆåŠŸçš„åˆ†æ
            with_errors > 0 and  # ä¹Ÿæœ‰é”™è¯¯è¢«æ­£ç¡®å¤„ç†
            len(error_types) > 0  # é”™è¯¯è¢«æ­£ç¡®åˆ†ç±»
        )

        print(f"     - é”™è¯¯å¤„ç†åŠŸèƒ½éªŒè¯: {'âœ…' if error_handling_good else 'âŒ'}")

        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        import shutil
        shutil.rmtree(temp_dir)

        return error_handling_good

    except Exception as e:
        print(f"âŒ å¹¶å‘åˆ†æé”™è¯¯å¤„ç†æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_concurrent_resource_management():
    """æµ‹è¯•å¹¶å‘åˆ†æçš„èµ„æºç®¡ç†"""
    print("\nğŸ” æµ‹è¯•å¹¶å‘åˆ†æçš„èµ„æºç®¡ç†...")

    try:
        # åˆ›å»ºå¤§é‡æ–‡ä»¶æµ‹è¯•èµ„æºç®¡ç†
        temp_dir = tempfile.mkdtemp()
        test_files = []

        print("   åˆ›å»ºèµ„æºç®¡ç†æµ‹è¯•æ–‡ä»¶:")
        for i in range(50):
            file_path = Path(temp_dir) / f"resource_test_{i:02d}.py"
            content = f'''# èµ„æºç®¡ç†æµ‹è¯•æ–‡ä»¶ {i}
import os
import sys

def resource_function_{i}():
    """èµ„æºä½¿ç”¨å‡½æ•°"""
    data = list(range({100 + i * 2}))
    return sum(data)

class ResourceClass{i}:
    """èµ„æºç®¡ç†ç±»"""

    def __init__(self):
        self.items = []
        for j in range({20 + i}):
            self.items.append(j * 2)

    def get_total(self):
        return sum(self.items)

def helper_{i}(x):
    return x * x

# æµ‹è¯•æ•°æ®ç»“æ„
test_dict_{i} = {{
    'numbers': list(range({10})),
    'strings': [f'str_{{j}}' for j in range(5)],
    'nested': {{'inner': [j for j in range(3)]}}
}}
'''
            file_path.write_text(content)
            test_files.append(str(file_path))

        print(f"     - åˆ›å»ºæ–‡ä»¶æ•°: {len(test_files)}")

        # èµ„æºæ„ŸçŸ¥çš„åˆ†æå‡½æ•°
        def resource_aware_analysis(file_path: str) -> Dict[str, Any]:
            """èµ„æºæ„ŸçŸ¥çš„åˆ†æå‡½æ•°"""
            import gc
            import threading

            start_time = time.time()
            start_objects = len(gc.get_objects())
            thread_id = threading.current_thread().ident

            try:
                # è¯»å–æ–‡ä»¶
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()

                # æ¨¡æ‹Ÿå†…å­˜ä½¿ç”¨
                temp_data = []
                for line_num, line in enumerate(content.splitlines()[:20]):  # åªå¤„ç†å‰20è¡Œ
                    temp_data.append({
                        'line_num': line_num,
                        'content': line.strip(),
                        'length': len(line),
                        'thread_id': thread_id
                    })

                # åˆ†æå†…å®¹
                lines = content.splitlines()
                functions = len([line for line in lines if 'def ' in line])
                classes = len([line for line in lines if 'class ' in line])

                # è®¡ç®—ä¸€äº›æŒ‡æ ‡
                total_line_length = sum(len(line) for line in lines)
                avg_line_length = total_line_length / len(lines) if lines else 0

                # æ¸…ç†ä¸´æ—¶æ•°æ®
                del temp_data

                end_time = time.time()
                end_objects = len(gc.get_objects())

                return {
                    'file_path': Path(file_path).name,
                    'line_count': len(lines),
                    'function_count': functions,
                    'class_count': classes,
                    'avg_line_length': round(avg_line_length, 1),
                    'total_line_length': total_line_length,
                    'analysis_time': end_time - start_time,
                    'thread_id': thread_id,
                    'objects_created': max(0, end_objects - start_objects),
                    'success': True
                }

            except Exception as e:
                return {
                    'file_path': Path(file_path).name,
                    'error': str(e),
                    'thread_id': thread_id,
                    'analysis_time': time.time() - start_time,
                    'success': False
                }

        # æµ‹è¯•ä¸åŒå¹¶å‘çº§åˆ«çš„èµ„æºä½¿ç”¨
        worker_counts = [4, 8, 16]
        resource_results = {}

        for max_workers in worker_counts:
            print(f"\n     æµ‹è¯•å¹¶å‘æ•° {max_workers}:")

            # æ‰‹åŠ¨åƒåœ¾å›æ”¶ä»¥è·å¾—å‡†ç¡®åŸºçº¿
            import gc
            gc.collect()
            initial_objects = len(gc.get_objects())

            start_time = time.time()
            analysis_results = []
            thread_usage = {}

            with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
                # æäº¤ä»»åŠ¡ï¼ˆé™åˆ¶æ•°é‡ä»¥æ§åˆ¶æµ‹è¯•æ—¶é—´ï¼‰
                future_to_file = {
                    executor.submit(resource_aware_analysis, file_path): file_path
                    for file_path in test_files[:30]  # åªå¤„ç†å‰30ä¸ªæ–‡ä»¶
                }

                for future in concurrent.futures.as_completed(future_to_file):
                    try:
                        result = future.result()
                        analysis_results.append(result)

                        # ç»Ÿè®¡çº¿ç¨‹ä½¿ç”¨
                        thread_id = result.get('thread_id')
                        if thread_id:
                            thread_usage[thread_id] = thread_usage.get(thread_id, 0) + 1

                    except Exception as e:
                        print(f"       - æ‰§è¡Œå¼‚å¸¸: {e}")

            total_time = time.time() - start_time

            # æœ€ç»ˆå¯¹è±¡è®¡æ•°
            gc.collect()
            final_objects = len(gc.get_objects())

            # ç»Ÿè®¡ç»“æœ
            successful = sum(1 for r in analysis_results if r.get('success'))
            total_objects_created = sum(r.get('objects_created', 0) for r in analysis_results)
            avg_objects_per_file = total_objects_created / len(analysis_results) if analysis_results else 0

            resource_results[max_workers] = {
                'total_time': total_time,
                'successful': successful,
                'threads_used': len(thread_usage),
                'total_objects_created': total_objects_created,
                'avg_objects_per_file': avg_objects_per_file,
                'objects_leaked': final_objects - initial_objects,
                'throughput': successful / total_time
            }

            print(f"       - åˆ†ææ—¶é—´: {total_time:.3f}s")
            print(f"       - æˆåŠŸåˆ†æ: {successful}")
            print(f"       - ä½¿ç”¨çº¿ç¨‹æ•°: {len(thread_usage)}")
            print(f"       - åˆ›å»ºå¯¹è±¡: {total_objects_created}")
            print(f"       - å¹³å‡æ¯æ–‡ä»¶å¯¹è±¡: {avg_objects_per_file:.0f}")
            print(f"       - å¯¹è±¡æ³„æ¼: {resource_results[max_workers]['objects_leaked']}")
            print(f"       - ååé‡: {successful/total_time:.1f} æ–‡ä»¶/ç§’")

        # åˆ†æèµ„æºç®¡ç†æ•ˆç‡
        print("\n   èµ„æºç®¡ç†æ•ˆç‡åˆ†æ:")
        best_workers = max(resource_results.keys(),
                         key=lambda k: resource_results[k]['throughput'])
        best_result = resource_results[best_workers]

        print(f"     - æœ€ä½³å¹¶å‘æ•°: {best_workers}")
        print(f"     - æœ€ä½³ååé‡: {best_result['throughput']:.1f} æ–‡ä»¶/ç§’")
        print(f"     - å¯¹è±¡æ³„æ¼æ§åˆ¶: {best_result['objects_leaked']}")

        # éªŒè¯èµ„æºç®¡ç†
        resource_management_good = (
            all(result['successful'] > 0 for result in resource_results.values()) and
            all(result['objects_leaked'] < 500 for result in resource_results.values()) and
            best_result['throughput'] > 20
        )

        print(f"     - èµ„æºç®¡ç†éªŒè¯: {'âœ…' if resource_management_good else 'âŒ'}")

        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        import shutil
        shutil.rmtree(temp_dir)

        return resource_management_good

    except Exception as e:
        print(f"âŒ å¹¶å‘åˆ†æèµ„æºç®¡ç†æµ‹è¯•å¤±è´¥: {e}")
        return False

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ å¼€å§‹å¤šæ–‡ä»¶å¹¶å‘åˆ†æåŠŸèƒ½æµ‹è¯•ï¼ˆç®€åŒ–ç‰ˆï¼‰")
    print("=" * 60)

    test_results = []

    # 1. æµ‹è¯•å¹¶å‘åˆ†ææ€§èƒ½
    performance_ok = test_concurrent_analysis_performance()
    test_results.append(performance_ok)

    # 2. æµ‹è¯•å¹¶å‘åˆ†æé”™è¯¯å¤„ç†
    error_ok = test_concurrent_error_handling()
    test_results.append(error_ok)

    # 3. æµ‹è¯•å¹¶å‘åˆ†æèµ„æºç®¡ç†
    resource_ok = test_concurrent_resource_management()
    test_results.append(resource_ok)

    # è¾“å‡ºæµ‹è¯•ç»“æœ
    print("\n" + "=" * 60)
    print("ğŸ“Š æµ‹è¯•ç»“æœæ±‡æ€»:")

    passed = sum(test_results)
    total = len(test_results)

    print(f"âœ… é€šè¿‡: {passed}/{total}")
    print(f"âŒ å¤±è´¥: {total-passed}/{total}")

    if passed >= total * 0.8:  # 80%é€šè¿‡ç‡
        print("\nğŸ‰ å¤šæ–‡ä»¶å¹¶å‘åˆ†æåŠŸèƒ½æµ‹è¯•åŸºæœ¬é€šè¿‡ï¼")
        print("å¤šæ–‡ä»¶å¹¶å‘åˆ†æåŠŸèƒ½å·²å°±ç»ªã€‚")
        if passed < total:
            print(f"âš ï¸ æœ‰ {total-passed} é¡¹æµ‹è¯•å¤±è´¥ï¼Œä½†ä¸å½±å“æ ¸å¿ƒåŠŸèƒ½ã€‚")
        return 0
    else:
        print(f"\nâš ï¸ ä»…æœ‰ {passed}/{total} é¡¹æµ‹è¯•é€šè¿‡ï¼Œéœ€è¦æ£€æŸ¥å¹¶å‘åˆ†æåŠŸèƒ½ã€‚")
        return 1

if __name__ == "__main__":
    try:
        exit_code = main()
        sys.exit(exit_code)
    except KeyboardInterrupt:
        print("\nç”¨æˆ·ä¸­æ–­æµ‹è¯•")
        sys.exit(0)
    except Exception as e:
        print(f"\næµ‹è¯•å¼‚å¸¸: {e}")
        sys.exit(1)