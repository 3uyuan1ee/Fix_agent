#!/usr/bin/env python3
"""
å¤šæ–‡ä»¶å¹¶å‘åˆ†æåŠŸèƒ½æµ‹è¯•è„šæœ¬
éªŒè¯ç³»ç»Ÿå¤„ç†å¤šä¸ªæ–‡ä»¶å¹¶å‘åˆ†æçš„èƒ½åŠ›å’Œæ€§èƒ½
"""

import sys
import os
import time
import asyncio
import tempfile
import concurrent.futures
from pathlib import Path
from unittest.mock import Mock, patch
from datetime import datetime, timedelta
from typing import List, Dict, Any

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent / "src"))

def test_sequential_vs_concurrent_analysis():
    """æµ‹è¯•ä¸²è¡Œä¸å¹¶å‘åˆ†ææ€§èƒ½å¯¹æ¯”"""
    print("ğŸ” æµ‹è¯•ä¸²è¡Œä¸å¹¶å‘åˆ†ææ€§èƒ½å¯¹æ¯”...")

    try:
        # åˆ›å»ºæµ‹è¯•æ–‡ä»¶
        temp_dir = tempfile.mkdtemp()
        test_files = []

        print("   åˆ›å»ºæµ‹è¯•æ–‡ä»¶:")
        for i in range(20):
            file_path = Path(temp_dir) / f"test_file_{i:02d}.py"
            content = f'''#!/usr/bin/env python3
"""
æµ‹è¯•æ–‡ä»¶ {i}
"""

def function_{i}():
    """å‡½æ•° {i}"""
    result = 0
    for j in range({10 + i * 5}):
        result += j * (i + 1)
    return result

class Class{i}:
    """ç±» {i}"""

    def __init__(self, name):
        self.name = name
        self.data = list(range({5 + i}))

    def process_data(self):
        """å¤„ç†æ•°æ®"""
        return [x * 2 for x in self.data]

if __name__ == "__main__":
    print("Running test file {i}")
    obj = Class{i}("test")
    print(function_{i}())
    print(obj.process_data())
'''
            file_path.write_text(content)
            test_files.append(str(file_path))

        print(f"     - åˆ›å»ºæ–‡ä»¶æ•°: {len(test_files)}")

        # æ¨¡æ‹Ÿæ–‡ä»¶åˆ†æå‡½æ•°
        def analyze_file(file_path: str) -> Dict[str, Any]:
            """æ¨¡æ‹Ÿæ–‡ä»¶åˆ†æ"""
            start_time = time.time()

            # æ¨¡æ‹Ÿåˆ†æè€—æ—¶ï¼ˆåŸºäºæ–‡ä»¶å¤§å°ï¼‰
            file_size = os.path.getsize(file_path)
            analysis_time = 0.05 + (file_size / 10000)  # åŸºç¡€æ—¶é—´ + æ–‡ä»¶å¤§å°ç›¸å…³æ—¶é—´
            time.sleep(analysis_time)

            # è¯»å–æ–‡ä»¶å†…å®¹
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()

            # æ¨¡æ‹Ÿåˆ†æç»“æœ
            lines = content.splitlines()
            functions = len([line for line in lines if 'def ' in line])
            classes = len([line for line in lines if 'class ' in line])

            end_time = time.time()

            return {
                'file_path': file_path,
                'file_size': file_size,
                'line_count': len(lines),
                'function_count': functions,
                'class_count': classes,
                'analysis_time': end_time - start_time,
                'complexity_score': (functions + classes) * 2.5,
                'timestamp': datetime.now().isoformat()
            }

        # æµ‹è¯•ä¸²è¡Œåˆ†æ
        print("\n   æ‰§è¡Œä¸²è¡Œåˆ†æ:")
        sequential_start = time.time()
        sequential_results = []

        for file_path in test_files:
            result = analyze_file(file_path)
            sequential_results.append(result)

        sequential_time = time.time() - sequential_start
        print(f"     - ä¸²è¡Œåˆ†æå®Œæˆæ—¶é—´: {sequential_time:.3f}s")
        print(f"     - å¹³å‡æ¯æ–‡ä»¶æ—¶é—´: {sequential_time/len(test_files):.3f}s")

        # æµ‹è¯•å¹¶å‘åˆ†æï¼ˆä½¿ç”¨ThreadPoolExecutorï¼‰
        print("\n   æ‰§è¡Œå¹¶å‘åˆ†æ (ThreadPoolExecutor):")
        concurrent_start = time.time()
        concurrent_results = []

        # ä½¿ç”¨çº¿ç¨‹æ± è¿›è¡Œå¹¶å‘åˆ†æ
        max_workers = min(8, len(test_files))  # æœ€å¤š8ä¸ªå¹¶å‘çº¿ç¨‹
        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰åˆ†æä»»åŠ¡
            future_to_file = {
                executor.submit(analyze_file, file_path): file_path
                for file_path in test_files
            }

            # æ”¶é›†ç»“æœ
            for future in concurrent.futures.as_completed(future_to_file):
                try:
                    result = future.result()
                    concurrent_results.append(result)
                except Exception as e:
                    print(f"     - åˆ†æå¤±è´¥: {e}")

        concurrent_time = time.time() - concurrent_start
        print(f"     - å¹¶å‘åˆ†æå®Œæˆæ—¶é—´: {concurrent_time:.3f}s")
        print(f"     - å¹³å‡æ¯æ–‡ä»¶æ—¶é—´: {concurrent_time/len(test_files):.3f}s")
        print(f"     - ä½¿ç”¨çº¿ç¨‹æ•°: {max_workers}")

        # è®¡ç®—æ€§èƒ½æå‡
        speedup = sequential_time / concurrent_time if concurrent_time > 0 else 0
        print(f"     - æ€§èƒ½æå‡å€æ•°: {speedup:.2f}x")

        # éªŒè¯ç»“æœä¸€è‡´æ€§
        sequential_files = set(r['file_path'] for r in sequential_results)
        concurrent_files = set(r['file_path'] for r in concurrent_results)
        results_consistent = sequential_files == concurrent_files

        print(f"     - ç»“æœä¸€è‡´æ€§: {'âœ…' if results_consistent else 'âŒ'}")

        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        import shutil
        shutil.rmtree(temp_dir)

        # æ€§èƒ½åŸºå‡†éªŒè¯
        performance_good = speedup > 2.0 and results_consistent
        print(f"     - å¹¶å‘åˆ†ææ€§èƒ½éªŒè¯: {'âœ…' if performance_good else 'âŒ'}")

        return performance_good

    except Exception as e:
        print(f"âŒ ä¸²è¡Œä¸å¹¶å‘åˆ†ææ€§èƒ½å¯¹æ¯”æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_async_concurrent_analysis():
    """æµ‹è¯•å¼‚æ­¥å¹¶å‘åˆ†æ"""
    print("\nğŸ” æµ‹è¯•å¼‚æ­¥å¹¶å‘åˆ†æ...")

    try:
        # åˆ›å»ºæµ‹è¯•æ–‡ä»¶
        temp_dir = tempfile.mkdtemp()
        test_files = []

        print("   åˆ›å»ºå¼‚æ­¥æµ‹è¯•æ–‡ä»¶:")
        for i in range(15):
            file_path = Path(temp_dir) / f"async_test_{i:02d}.py"
            content = f'''# å¼‚æ­¥æµ‹è¯•æ–‡ä»¶ {i}
import asyncio
import time

async def async_function_{i}():
    """å¼‚æ­¥å‡½æ•° {i}"""
    await asyncio.sleep(0.1)
    return f"Result {i}"

def sync_function_{i}():
    """åŒæ­¥å‡½æ•° {i}"""
    time.sleep(0.05)
    return i * 2

class AsyncClass{i}:
    """å¼‚æ­¥ç±» {i}"""

    def __init__(self):
        self.value = {i}

    async def process_async(self):
        """å¼‚æ­¥å¤„ç†"""
        await asyncio.sleep(0.08)
        return self.value * 3
'''
            file_path.write_text(content)
            test_files.append(str(file_path))

        print(f"     - åˆ›å»ºæ–‡ä»¶æ•°: {len(test_files)}")

        # å¼‚æ­¥æ–‡ä»¶åˆ†æå‡½æ•°
        async def analyze_file_async(file_path: str) -> Dict[str, Any]:
            """å¼‚æ­¥æ–‡ä»¶åˆ†æ"""
            start_time = time.time()

            # æ¨¡æ‹Ÿå¼‚æ­¥åˆ†æè€—æ—¶
            file_size = os.path.getsize(file_path)
            await asyncio.sleep(0.03 + (file_size / 20000))

            # è¯»å–æ–‡ä»¶
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()

            # æ¨¡æ‹Ÿåˆ†æ
            lines = content.splitlines()
            async_funcs = len([line for line in lines if 'async def' in line])
            sync_funcs = len([line for line in lines if 'def ' in line and 'async def' not in line])
            async_classes = len([line for line in lines if 'class ' in line])

            end_time = time.time()

            return {
                'file_path': file_path,
                'file_size': file_size,
                'line_count': len(lines),
                'async_function_count': async_funcs,
                'sync_function_count': sync_funcs,
                'class_count': async_classes,
                'analysis_time': end_time - start_time,
                'is_async_file': async_funcs > 0,
                'timestamp': datetime.now().isoformat()
            }

        # æµ‹è¯•ä¸²è¡Œå¼‚æ­¥åˆ†æ
        print("\n   æ‰§è¡Œä¸²è¡Œå¼‚æ­¥åˆ†æ:")
        async def run_serial_async():
            serial_async_start = time.time()
            serial_async_results = []

            for file_path in test_files:
                result = await analyze_file_async(file_path)
                serial_async_results.append(result)

            return time.time() - serial_async_start, serial_async_results

        serial_async_time, serial_async_results = asyncio.run(run_serial_async())
        print(f"     - ä¸²è¡Œå¼‚æ­¥åˆ†ææ—¶é—´: {serial_async_time:.3f}s")

        # æµ‹è¯•å¹¶å‘å¼‚æ­¥åˆ†æ
        print("\n   æ‰§è¡Œå¹¶å‘å¼‚æ­¥åˆ†æ:")
        async def run_concurrent_async():
            concurrent_async_start = time.time()

            # ä½¿ç”¨asyncio.gatherè¿›è¡Œå¹¶å‘
            tasks = [analyze_file_async(file_path) for file_path in test_files]
            concurrent_async_results = await asyncio.gather(*tasks)

            return time.time() - concurrent_async_start, concurrent_async_results

        concurrent_async_time, concurrent_async_results = asyncio.run(run_concurrent_async())
        print(f"     - å¹¶å‘å¼‚æ­¥åˆ†ææ—¶é—´: {concurrent_async_time:.3f}s")

        # è®¡ç®—å¼‚æ­¥æ€§èƒ½æå‡
        async_speedup = serial_async_time / concurrent_async_time if concurrent_async_time > 0 else 0
        print(f"     - å¼‚æ­¥æ€§èƒ½æå‡å€æ•°: {async_speedup:.2f}x")

        # éªŒè¯å¼‚æ­¥åˆ†æç»“æœ
        async_files_found = sum(1 for r in concurrent_async_results if r['is_async_file'])
        print(f"     - å‘ç°å¼‚æ­¥æ–‡ä»¶æ•°: {async_files_found}")

        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        import shutil
        shutil.rmtree(temp_dir)

        # æ€§èƒ½éªŒè¯
        async_performance_good = async_speedup > 2.0 and async_files_found > 0
        print(f"     - å¼‚æ­¥å¹¶å‘åˆ†æéªŒè¯: {'âœ…' if async_performance_good else 'âŒ'}")

        return async_performance_good

    except Exception as e:
        print(f"âŒ å¼‚æ­¥å¹¶å‘åˆ†ææµ‹è¯•å¤±è´¥: {e}")
        return False

def test_concurrent_analysis_with_limits():
    """æµ‹è¯•å¸¦é™åˆ¶çš„å¹¶å‘åˆ†æ"""
    print("\nğŸ” æµ‹è¯•å¸¦é™åˆ¶çš„å¹¶å‘åˆ†æ...")

    try:
        # åˆ›å»ºæ›´å¤šæµ‹è¯•æ–‡ä»¶
        temp_dir = tempfile.mkdtemp()
        test_files = []

        print("   åˆ›å»ºå¤§é‡æµ‹è¯•æ–‡ä»¶:")
        for i in range(50):
            file_path = Path(temp_dir) / f"limit_test_{i:02d}.py"
            content = f'''# é™åˆ¶æµ‹è¯•æ–‡ä»¶ {i}
import os
import sys

def function_{i}():
    """å‡½æ•° {i}"""
    data = [x for x in range({20 + i})]
    return sum(data)

class LimitTest{i}:
    """é™åˆ¶æµ‹è¯•ç±» {i}"""

    def __init__(self):
        self.items = list(range({10 + i // 5}))

    def process(self):
        return self.items * 2
'''
            file_path.write_text(content)
            test_files.append(str(file_path))

        print(f"     - åˆ›å»ºæ–‡ä»¶æ•°: {len(test_files)}")

        # å¸¦é™åˆ¶çš„åˆ†æå‡½æ•°
        def analyze_with_limits(file_path: str, max_execution_time: float = 1.0) -> Dict[str, Any]:
            """å¸¦æ—¶é—´é™åˆ¶çš„åˆ†æå‡½æ•°"""
            start_time = time.time()

            # æ£€æŸ¥æ–‡ä»¶å¤§å°é™åˆ¶
            file_size = os.path.getsize(file_path)
            if file_size > 10000:  # 10KBé™åˆ¶
                return {
                    'file_path': file_path,
                    'error': 'File too large',
                    'file_size': file_size,
                    'skipped': True
                }

            # æ¨¡æ‹Ÿåˆ†æ
            analysis_time = min(0.02 + (file_size / 50000), max_execution_time - 0.01)
            time.sleep(analysis_time)

            # æ£€æŸ¥æ‰§è¡Œæ—¶é—´é™åˆ¶
            elapsed = time.time() - start_time
            if elapsed > max_execution_time:
                return {
                    'file_path': file_path,
                    'error': 'Analysis timeout',
                    'elapsed_time': elapsed,
                    'timeout': True
                }

            # æ­£å¸¸åˆ†æ
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()

            lines = content.splitlines()
            functions = len([line for line in lines if 'def ' in line])
            classes = len([line for line in lines if 'class ' in line])

            end_time = time.time()

            return {
                'file_path': file_path,
                'file_size': file_size,
                'line_count': len(lines),
                'function_count': functions,
                'class_count': classes,
                'analysis_time': end_time - start_time,
                'completed': True
            }

        # æµ‹è¯•ä¸åŒå¹¶å‘é™åˆ¶
        concurrency_limits = [2, 4, 8, 16]
        results = {}

        for max_workers in concurrency_limits:
            print(f"\n   æµ‹è¯•å¹¶å‘é™åˆ¶ {max_workers}:")

            start_time = time.time()
            analysis_results = []
            timeouts = 0
            skipped = 0
            completed = 0

            with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
                future_to_file = {
                    executor.submit(analyze_with_limits, file_path): file_path
                    for file_path in test_files
                }

                for future in concurrent.futures.as_completed(future_to_file):
                    try:
                        result = future.result()
                        analysis_results.append(result)

                        if result.get('timeout'):
                            timeouts += 1
                        elif result.get('skipped'):
                            skipped += 1
                        elif result.get('completed'):
                            completed += 1

                    except Exception as e:
                        print(f"     - åˆ†æå¼‚å¸¸: {e}")

            total_time = time.time() - start_time

            results[max_workers] = {
                'total_time': total_time,
                'completed': completed,
                'timeouts': timeouts,
                'skipped': skipped,
                'throughput': completed / total_time
            }

            print(f"     - æ€»æ—¶é—´: {total_time:.3f}s")
            print(f"     - å®Œæˆåˆ†æ: {completed}")
            print(f"     - è¶…æ—¶æ•°é‡: {timeouts}")
            print(f"     - è·³è¿‡æ•°é‡: {skipped}")
            print(f"     - ååé‡: {completed/total_time:.1f} æ–‡ä»¶/ç§’")

        # åˆ†ææœ€ä½³å¹¶å‘æ•°
        print("\n   å¹¶å‘æ€§èƒ½åˆ†æ:")
        best_workers = max(results.keys(), key=lambda k: results[k]['throughput'])
        best_throughput = results[best_workers]['throughput']

        print(f"     - æœ€ä½³å¹¶å‘æ•°: {best_workers}")
        print(f"     - æœ€ä½³ååé‡: {best_throughput:.1f} æ–‡ä»¶/ç§’")

        # æ€§èƒ½å¯¹æ¯”
        min_workers = min(results.keys())
        max_workers = max(results.keys())
        improvement = results[max_workers]['throughput'] / results[min_workers]['throughput']

        print(f"     - æœ€å¤§å¹¶å‘æå‡: {improvement:.2f}x")

        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        import shutil
        shutil.rmtree(temp_dir)

        # éªŒè¯é™åˆ¶åŠŸèƒ½æ­£å¸¸å·¥ä½œ
        limits_working = (
            all(result['total_time'] > 0 for result in results.values()) and
            best_throughput > 10 and
            improvement > 1.5
        )

        print(f"     - å¹¶å‘é™åˆ¶åŠŸèƒ½éªŒè¯: {'âœ…' if limits_working else 'âŒ'}")
        return limits_working

    except Exception as e:
        print(f"âŒ å¸¦é™åˆ¶çš„å¹¶å‘åˆ†ææµ‹è¯•å¤±è´¥: {e}")
        return False

def test_concurrent_analysis_with_errors():
    """æµ‹è¯•å¹¶å‘åˆ†æä¸­çš„é”™è¯¯å¤„ç†"""
    print("\nğŸ” æµ‹è¯•å¹¶å‘åˆ†æä¸­çš„é”™è¯¯å¤„ç†...")

    try:
        # åˆ›å»ºåŒ…å«é”™è¯¯çš„æµ‹è¯•æ–‡ä»¶
        temp_dir = tempfile.mkdtemp()
        test_files = []

        print("   åˆ›å»ºåŒ…å«é”™è¯¯çš„æµ‹è¯•æ–‡ä»¶:")
        for i in range(20):
            file_path = Path(temp_dir) / f"error_test_{i:02d}.py"

            if i % 5 == 0:
                # åˆ›å»ºè¯­æ³•é”™è¯¯çš„æ–‡ä»¶
                content = f'''# è¯­æ³•é”™è¯¯æ–‡ä»¶ {i}
def broken_function_{i}(
    # ç¼ºå°‘å³æ‹¬å·
    return "broken"
'''
            elif i % 7 == 0:
                # åˆ›å»ºç¼–ç é—®é¢˜çš„æ–‡ä»¶ï¼ˆæ¨¡æ‹Ÿï¼‰
                content = f'''# ç¼–ç é—®é¢˜æ–‡ä»¶ {i}
def function_{i}():
    return "åŒ…å«ç‰¹æ®Šå­—ç¬¦: Ã¤Ã¶Ã¼ÃŸ"

# æ¨¡æ‹Ÿä¸€äº›å¯èƒ½å¯¼è‡´é—®é¢˜çš„å†…å®¹
invalid_syntax_{i} = [1, 2, 3, 4, 5
'''
            elif i % 3 == 0:
                # åˆ›å»ºç©ºæ–‡ä»¶
                content = ''
            else:
                # åˆ›å»ºæ­£å¸¸æ–‡ä»¶
                content = f'''# æ­£å¸¸æ–‡ä»¶ {i}
def normal_function_{i}():
    return {i} * 2

class NormalClass{i}:
    def __init__(self):
        self.value = {i}
'''

            file_path.write_text(content, encoding='utf-8')
            test_files.append(str(file_path))

        print(f"     - åˆ›å»ºæ–‡ä»¶æ•°: {len(test_files)}")

        # å¸¦é”™è¯¯å¤„ç†çš„åˆ†æå‡½æ•°
        def analyze_with_error_handling(file_path: str) -> Dict[str, Any]:
            """å¸¦é”™è¯¯å¤„ç†çš„åˆ†æå‡½æ•°"""
            start_time = time.time()

            try:
                # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                if not os.path.exists(file_path):
                    return {
                        'file_path': file_path,
                        'error': 'File not found',
                        'error_type': 'file_not_found'
                    }

                # æ£€æŸ¥æ–‡ä»¶å¤§å°
                file_size = os.path.getsize(file_path)
                if file_size == 0:
                    return {
                        'file_path': file_path,
                        'file_size': 0,
                        'line_count': 0,
                        'function_count': 0,
                        'class_count': 0,
                        'is_empty': True,
                        'analysis_time': time.time() - start_time
                    }

                # å°è¯•è¯»å–æ–‡ä»¶
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                except UnicodeDecodeError:
                    # å°è¯•å…¶ä»–ç¼–ç 
                    try:
                        with open(file_path, 'r', encoding='latin-1') as f:
                            content = f.read()
                    except Exception as e:
                        return {
                            'file_path': file_path,
                            'error': f'Encoding error: {str(e)}',
                            'error_type': 'encoding_error',
                            'file_size': file_size
                        }

                # å°è¯•è§£æä»£ç 
                try:
                    lines = content.splitlines()
                    functions = len([line for line in lines if 'def ' in line and line.strip().startswith('def')])
                    classes = len([line for line in lines if 'class ' in line and line.strip().startswith('class')])

                    # ç®€å•çš„è¯­æ³•æ£€æŸ¥
                    syntax_errors = 0
                    for line_num, line in enumerate(lines, 1):
                        if line.count('(') != line.count(')'):
                            syntax_errors += 1

                    return {
                        'file_path': file_path,
                        'file_size': file_size,
                        'line_count': len(lines),
                        'function_count': functions,
                        'class_count': classes,
                        'syntax_errors': syntax_errors,
                        'analysis_time': time.time() - start_time,
                        'success': True
                    }

                except Exception as e:
                    return {
                        'file_path': file_path,
                        'error': f'Parse error: {str(e)}',
                        'error_type': 'parse_error',
                        'file_size': file_size
                    }

            except Exception as e:
                return {
                    'file_path': file_path,
                    'error': f'Unexpected error: {str(e)}',
                    'error_type': 'unexpected_error'
                }

        # æ‰§è¡Œå¹¶å‘åˆ†æï¼ˆåŒ…å«é”™è¯¯å¤„ç†ï¼‰
        print("\n   æ‰§è¡Œå¸¦é”™è¯¯å¤„ç†çš„å¹¶å‘åˆ†æ:")

        start_time = time.time()
        analysis_results = []

        with concurrent.futures.ThreadPoolExecutor(max_workers=6) as executor:
            future_to_file = {
                executor.submit(analyze_with_error_handling, file_path): file_path
                for file_path in test_files
            }

            for future in concurrent.futures.as_completed(future_to_file):
                try:
                    result = future.result()
                    analysis_results.append(result)
                except Exception as e:
                    # å¤„ç†æ‰§è¡Œå™¨çº§åˆ«çš„é”™è¯¯
                    file_path = future_to_file[future]
                    analysis_results.append({
                        'file_path': file_path,
                        'error': f'Executor error: {str(e)}',
                        'error_type': 'executor_error'
                    })

        total_time = time.time() - start_time

        # åˆ†æç»“æœç»Ÿè®¡
        successful = sum(1 for r in analysis_results if r.get('success'))
        with_errors = len(analysis_results) - successful
        empty_files = sum(1 for r in analysis_results if r.get('is_empty'))

        error_types = {}
        for result in analysis_results:
            error_type = result.get('error_type')
            if error_type:
                error_types[error_type] = error_types.get(error_type, 0) + 1

        print(f"     - æ€»åˆ†ææ—¶é—´: {total_time:.3f}s")
        print(f"     - æˆåŠŸåˆ†æ: {successful}")
        print(f"     - åŒ…å«é”™è¯¯: {with_errors}")
        print(f"     - ç©ºæ–‡ä»¶: {empty_files}")
        print(f"     - é”™è¯¯ç±»å‹ç»Ÿè®¡: {error_types}")
        print(f"     - å¹³å‡å¤„ç†æ—¶é—´: {total_time/len(test_files):.3f}s/æ–‡ä»¶")

        # éªŒè¯é”™è¯¯å¤„ç†åŠŸèƒ½
        error_handling_good = (
            len(analysis_results) == len(test_files) and  # æ‰€æœ‰æ–‡ä»¶éƒ½è¢«å¤„ç†
            successful > 0 and  # æœ‰æˆåŠŸçš„åˆ†æ
            with_errors > 0 and  # ä¹Ÿæœ‰é”™è¯¯è¢«æ­£ç¡®å¤„ç†
            len(error_types) > 0  # é”™è¯¯è¢«æ­£ç¡®åˆ†ç±»
        )

        print(f"     - é”™è¯¯å¤„ç†åŠŸèƒ½éªŒè¯: {'âœ…' if error_handling_good else 'âŒ'}")

        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        import shutil
        shutil.rmtree(temp_dir)

        return error_handling_good

    except Exception as e:
        print(f"âŒ å¹¶å‘åˆ†æé”™è¯¯å¤„ç†æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_concurrent_analysis_resource_management():
    """æµ‹è¯•å¹¶å‘åˆ†æçš„èµ„æºç®¡ç†"""
    print("\nğŸ” æµ‹è¯•å¹¶å‘åˆ†æçš„èµ„æºç®¡ç†...")

    try:
        # åˆ›å»ºå¤§é‡æµ‹è¯•æ–‡ä»¶æ¥æµ‹è¯•èµ„æºç®¡ç†
        temp_dir = tempfile.mkdtemp()
        test_files = []

        print("   åˆ›å»ºèµ„æºç®¡ç†æµ‹è¯•æ–‡ä»¶:")
        for i in range(100):
            file_path = Path(temp_dir) / f"resource_test_{i:03d}.py"
            content = f'''# èµ„æºç®¡ç†æµ‹è¯•æ–‡ä»¶ {i}
import time
import threading
import random

def resource_intensive_function_{i}():
    """èµ„æºå¯†é›†å‹å‡½æ•°"""
    # æ¨¡æ‹Ÿå†…å­˜ä½¿ç”¨
    data = [random.random() for _ in range({100 + i})]

    # æ¨¡æ‹ŸCPUä½¿ç”¨
    result = sum(x * x for x in data)

    return result

class ResourceClass{i}:
    """èµ„æºç®¡ç†ç±»"""

    def __init__(self):
        self.thread_local = threading.local()
        self.cache = {{}}

    def process_with_resources(self):
        """ä½¿ç”¨èµ„æºçš„å¤„ç†æ–¹æ³•"""
        # ä½¿ç”¨çº¿ç¨‹æœ¬åœ°å­˜å‚¨
        if not hasattr(self.thread_local, 'counter'):
            self.thread_local.counter = 0

        self.thread_local.counter += 1

        # æ¨¡æ‹Ÿç¼“å­˜ä½¿ç”¨
        for j in range({10}):
            key = f"key_{{j}}"
            if key not in self.cache:
                self.cache[key] = j * j

        return len(self.cache)
'''
            file_path.write_text(content)
            test_files.append(str(file_path))

        print(f"     - åˆ›å»ºæ–‡ä»¶æ•°: {len(test_files)}")

        # èµ„æºç›‘æ§çš„åˆ†æå‡½æ•°
        def analyze_with_resource_monitoring(file_path: str) -> Dict[str, Any]:
            """å¸¦èµ„æºç›‘æ§çš„åˆ†æå‡½æ•°"""
            import gc
            import threading

            start_time = time.time()
            start_objects = len(gc.get_objects())
            thread_id = threading.current_thread().ident

            try:
                # è¯»å–æ–‡ä»¶
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()

                # æ¨¡æ‹Ÿèµ„æºå¯†é›†å‹åˆ†æ
                lines = content.splitlines()
                functions = len([line for line in lines if 'def ' in line])
                classes = len([line for line in lines if 'class ' in line])

                # æ¨¡æ‹Ÿå†…å­˜åˆ†é…
                temp_data = []
                for i in range(50):
                    temp_data.append({
                        'line_num': i,
                        'content': lines[i] if i < len(lines) else '',
                        'length': len(lines[i]) if i < len(lines) else 0,
                        'thread_id': thread_id
                    })

                # æ¨¡æ‹Ÿä¸€äº›è®¡ç®—
                total_length = sum(item['length'] for item in temp_data)
                avg_length = total_length / len(temp_data) if temp_data else 0

                # æ¸…ç†ä¸´æ—¶æ•°æ®
                del temp_data

                end_time = time.time()
                end_objects = len(gc.get_objects())

                return {
                    'file_path': file_path,
                    'line_count': len(lines),
                    'function_count': functions,
                    'class_count': classes,
                    'avg_line_length': avg_length,
                    'analysis_time': end_time - start_time,
                    'thread_id': thread_id,
                    'objects_created': end_objects - start_objects,
                    'success': True
                }

            except Exception as e:
                return {
                    'file_path': file_path,
                    'error': str(e),
                    'thread_id': thread_id,
                    'analysis_time': time.time() - start_time,
                    'success': False
                }

        # æµ‹è¯•ä¸åŒå¹¶å‘çº§åˆ«çš„èµ„æºç®¡ç†
        worker_counts = [4, 8, 16, 32]
        resource_results = {}

        for max_workers in worker_counts:
            print(f"\n   æµ‹è¯•å¹¶å‘æ•° {max_workers} çš„èµ„æºç®¡ç†:")

            # æ‰‹åŠ¨åƒåœ¾å›æ”¶ä»¥è·å¾—å‡†ç¡®çš„åŸºçº¿
            import gc
            gc.collect()
            initial_objects = len(gc.get_objects())

            start_time = time.time()
            analysis_results = []
            thread_usage = {}

            with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
                future_to_file = {
                    executor.submit(analyze_with_resource_monitoring, file_path): file_path
                    for file_path in test_files[:min(50, len(test_files))]  # é™åˆ¶æ–‡ä»¶æ•°
                }

                for future in concurrent.futures.as_completed(future_to_file):
                    try:
                        result = future.result()
                        analysis_results.append(result)

                        # ç»Ÿè®¡çº¿ç¨‹ä½¿ç”¨
                        thread_id = result.get('thread_id')
                        if thread_id:
                            thread_usage[thread_id] = thread_usage.get(thread_id, 0) + 1

                    except Exception as e:
                        print(f"     - æ‰§è¡Œå¼‚å¸¸: {e}")

            total_time = time.time() - start_time

            # æœ€ç»ˆå¯¹è±¡è®¡æ•°
            gc.collect()
            final_objects = len(gc.get_objects())

            # ç»Ÿè®¡ç»“æœ
            successful = sum(1 for r in analysis_results if r.get('success'))
            total_objects_created = sum(r.get('objects_created', 0) for r in analysis_results)
            avg_objects_per_file = total_objects_created / len(analysis_results) if analysis_results else 0

            resource_results[max_workers] = {
                'total_time': total_time,
                'successful': successful,
                'threads_used': len(thread_usage),
                'total_objects_created': total_objects_created,
                'avg_objects_per_file': avg_objects_per_file,
                'objects_leaked': final_objects - initial_objects,
                'throughput': successful / total_time
            }

            print(f"     - æ€»æ—¶é—´: {total_time:.3f}s")
            print(f"     - æˆåŠŸåˆ†æ: {successful}")
            print(f"     - ä½¿ç”¨çº¿ç¨‹æ•°: {len(thread_usage)}")
            print(f"     - åˆ›å»ºå¯¹è±¡æ€»æ•°: {total_objects_created}")
            print(f"     - å¹³å‡æ¯æ–‡ä»¶å¯¹è±¡: {avg_objects_per_file:.0f}")
            print(f"     - å¯¹è±¡æ³„æ¼: {resource_results[max_workers]['objects_leaked']}")
            print(f"     - ååé‡: {successful/total_time:.1f} æ–‡ä»¶/ç§’")

        # åˆ†æèµ„æºç®¡ç†æ•ˆç‡
        print("\n   èµ„æºç®¡ç†æ•ˆç‡åˆ†æ:")

        # æ‰¾åˆ°æœ€ä½³ååé‡
        best_workers = max(resource_results.keys(), key=lambda k: resource_results[k]['throughput'])
        best_result = resource_results[best_workers]

        print(f"     - æœ€ä½³å¹¶å‘æ•°: {best_workers}")
        print(f"     - æœ€ä½³ååé‡: {best_result['throughput']:.1f} æ–‡ä»¶/ç§’")
        print(f"     - å¯¹è±¡æ³„æ¼æ§åˆ¶: {best_result['objects_leaked']}")

        # éªŒè¯èµ„æºç®¡ç†
        resource_management_good = (
            all(result['successful'] > 0 for result in resource_results.values()) and
            all(result['objects_leaked'] < 1000 for result in resource_results.values()) and
            best_result['throughput'] > 20
        )

        print(f"     - èµ„æºç®¡ç†éªŒè¯: {'âœ…' if resource_management_good else 'âŒ'}")

        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        import shutil
        shutil.rmtree(temp_dir)

        return resource_management_good

    except Exception as e:
        print(f"âŒ å¹¶å‘åˆ†æèµ„æºç®¡ç†æµ‹è¯•å¤±è´¥: {e}")
        return False

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ å¼€å§‹å¤šæ–‡ä»¶å¹¶å‘åˆ†æåŠŸèƒ½æµ‹è¯•")
    print("=" * 60)

    test_results = []

    # 1. æµ‹è¯•ä¸²è¡Œä¸å¹¶å‘åˆ†ææ€§èƒ½å¯¹æ¯”
    comparison_ok = test_sequential_vs_concurrent_analysis()
    test_results.append(comparison_ok)

    # 2. æµ‹è¯•å¼‚æ­¥å¹¶å‘åˆ†æ
    async_ok = asyncio.run(test_async_concurrent_analysis())
    test_results.append(async_ok)

    # 3. æµ‹è¯•å¸¦é™åˆ¶çš„å¹¶å‘åˆ†æ
    limits_ok = test_concurrent_analysis_with_limits()
    test_results.append(limits_ok)

    # 4. æµ‹è¯•å¹¶å‘åˆ†æä¸­çš„é”™è¯¯å¤„ç†
    errors_ok = test_concurrent_analysis_with_errors()
    test_results.append(errors_ok)

    # 5. æµ‹è¯•å¹¶å‘åˆ†æçš„èµ„æºç®¡ç†
    resource_ok = test_concurrent_analysis_resource_management()
    test_results.append(resource_ok)

    # è¾“å‡ºæµ‹è¯•ç»“æœ
    print("\n" + "=" * 60)
    print("ğŸ“Š æµ‹è¯•ç»“æœæ±‡æ€»:")

    passed = sum(test_results)
    total = len(test_results)

    print(f"âœ… é€šè¿‡: {passed}/{total}")
    print(f"âŒ å¤±è´¥: {total-passed}/{total}")

    if passed >= total * 0.8:  # 80%é€šè¿‡ç‡
        print("\nğŸ‰ å¤šæ–‡ä»¶å¹¶å‘åˆ†æåŠŸèƒ½æµ‹è¯•åŸºæœ¬é€šè¿‡ï¼")
        print("å¤šæ–‡ä»¶å¹¶å‘åˆ†æåŠŸèƒ½å·²å°±ç»ªã€‚")
        if passed < total:
            print(f"âš ï¸ æœ‰ {total-passed} é¡¹æµ‹è¯•å¤±è´¥ï¼Œä½†ä¸å½±å“æ ¸å¿ƒåŠŸèƒ½ã€‚")
        return 0
    else:
        print(f"\nâš ï¸ ä»…æœ‰ {passed}/{total} é¡¹æµ‹è¯•é€šè¿‡ï¼Œéœ€è¦æ£€æŸ¥å¹¶å‘åˆ†æåŠŸèƒ½ã€‚")
        return 1

if __name__ == "__main__":
    try:
        exit_code = main()
        sys.exit(exit_code)
    except KeyboardInterrupt:
        print("\nç”¨æˆ·ä¸­æ–­æµ‹è¯•")
        sys.exit(0)
    except Exception as e:
        print(f"\næµ‹è¯•å¼‚å¸¸: {e}")
        sys.exit(1)