"""
AIæ–‡ä»¶é€‰æ‹©æ‰§è¡Œå™¨ - T005.2
è°ƒç”¨AIæ¨¡å‹æ™ºèƒ½é€‰æ‹©éœ€è¦åˆ†æçš„æ ¸å¿ƒæ–‡ä»¶
"""

import json
import time
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, field
from datetime import datetime
import threading
from pathlib import Path

from ..utils.logger import get_logger

# ä½¿ç”¨ç°æœ‰çš„LLMå®¢æˆ·ç«¯
try:
    from ..llm.client import LLMClient
except ImportError:
    # å¦‚æœLLMå®¢æˆ·ç«¯ä¸å¯ç”¨ï¼Œåˆ›å»ºæ¨¡æ‹Ÿå®¢æˆ·ç«¯
    class LLMClient:
        def __init__(self, *args, **kwargs):
            pass

        def chat_completion(self, *args, **kwargs):
            return {"content": '{"selected_files": []}', "success": True}

# AIæ–‡ä»¶é€‰æ‹©æç¤ºè¯æ„å»ºå™¨
class AIFileSelectionPromptBuilder:
    def __init__(self, *args, **kwargs):
        pass

    def build_prompt(self, project_path: str, analysis_results: List[Any] = None,
                     user_requirements: str = "", analysis_focus: List[str] = None,
                     runtime_errors: List[Dict[str, Any]] = None,
                     project_structure: Dict[str, Any] = None):
        """æ„å»ºAIæ–‡ä»¶é€‰æ‹©æç¤ºè¯"""

        # ç³»ç»Ÿæç¤ºè¯
        system_prompt = """
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä»£ç åˆ†æä¸“å®¶ï¼Œæ“…é•¿ä»é¡¹ç›®ä¸­è¯†åˆ«å‡ºæœ€éœ€è¦åˆ†æå’Œæ”¹è¿›çš„å…³é”®æ–‡ä»¶ã€‚

ä½ çš„ä»»åŠ¡æ˜¯åˆ†ææä¾›çš„é¡¹ç›®ä¿¡æ¯ï¼Œæ™ºèƒ½é€‰æ‹©éœ€è¦é‡ç‚¹åˆ†æçš„æ–‡ä»¶ã€‚é€‰æ‹©æ—¶è¯·è€ƒè™‘ï¼š

1. **é—®é¢˜ä¸¥é‡æ€§**: ä¼˜å…ˆé€‰æ‹©æœ‰å®‰å…¨æ¼æ´ã€ä¸¥é‡é”™è¯¯æˆ–é«˜é£é™©é—®é¢˜çš„æ–‡ä»¶
2. **ä»£ç è´¨é‡**: é€‰æ‹©å­˜åœ¨ä»£ç è´¨é‡é—®é¢˜çš„æ–‡ä»¶ï¼Œå¦‚å¤æ‚åº¦è¿‡é«˜ã€é‡å¤ä»£ç ç­‰
3. **æ ¸å¿ƒé‡è¦æ€§**: é€‰æ‹©é¡¹ç›®çš„æ ¸å¿ƒç»„ä»¶ã€ä¸»è¦ä¸šåŠ¡é€»è¾‘æ–‡ä»¶
4. **ç”¨æˆ·éœ€æ±‚**: æ ¹æ®ç”¨æˆ·çš„ç‰¹å®šéœ€æ±‚ä¼˜å…ˆé€‰æ‹©ç›¸å…³æ–‡ä»¶

è¯·ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¾“å‡ºï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
```json
{
  "selected_files": [
    {
      "file_path": "æ–‡ä»¶è·¯å¾„ï¼ˆç›¸å¯¹è·¯å¾„ï¼‰",
      "priority": "high|medium|low",
      "reason": "é€‰æ‹©æ­¤æ–‡ä»¶çš„è¯¦ç»†ç†ç”±",
      "confidence": 0.8,
      "key_issues": ["é—®é¢˜1", "é—®é¢˜2"]
    }
  ],
  "selection_summary": {
    "total_selected": æ•°é‡,
    "selection_criteria_met": true,
    "additional_notes": "é¢å¤–è¯´æ˜"
  }
}
```

æ³¨æ„äº‹é¡¹ï¼š
- é€‰æ‹©5-15ä¸ªæœ€é‡è¦çš„æ–‡ä»¶
- file_pathä½¿ç”¨ç›¸å¯¹è·¯å¾„
- confidenceèŒƒå›´ä¸º0.0-1.0
- reasonè¦å…·ä½“è¯´æ˜é€‰æ‹©ç†ç”±
- key_issuesåˆ—å‡ºå‘ç°çš„ä¸»è¦é—®é¢˜
"""

        # ç”¨æˆ·æç¤ºè¯
        user_prompt_parts = [
            "# æ–‡ä»¶é€‰æ‹©ä»»åŠ¡",
            f"",
            f"## é¡¹ç›®ä¿¡æ¯",
            f"- é¡¹ç›®è·¯å¾„: {project_path}",
            f"- ç”¨æˆ·éœ€æ±‚: {user_requirements}",
            f"- åˆ†æé‡ç‚¹: {', '.join(analysis_focus or [])}",
            f""
        ]

        # æ·»åŠ é™æ€åˆ†æç»“æœ
        if analysis_results:
            user_prompt_parts.extend([
                "## é™æ€åˆ†æç»“æœ",
                f"å‘ç°ä¸¥é‡é—®é¢˜ï¼Œéœ€è¦é‡ç‚¹å…³æ³¨ï¼š"
            ])

            problem_files = {}

            # æ”¶é›†æ‰€æœ‰é—®é¢˜ï¼ŒæŒ‰æ–‡ä»¶åˆ†ç»„
            for result in analysis_results:
                if hasattr(result, 'issues') and result.issues:
                    for issue in result.issues:
                        file_path = getattr(issue, 'file_path', 'unknown')
                        if file_path != 'unknown':
                            if file_path not in problem_files:
                                problem_files[file_path] = []

                            message = getattr(issue, 'message', 'æœªçŸ¥é—®é¢˜')
                            severity = getattr(issue, 'severity', 'unknown')
                            line_num = getattr(issue, 'line_number', '?')

                            problem_files[file_path].append({
                                'severity': severity,
                                'message': message,
                                'line': line_num
                            })

            # æ˜¾ç¤ºæ¯ä¸ªæ–‡ä»¶çš„é—®é¢˜
            for file_path, issues in problem_files.items():
                user_prompt_parts.append(f"\nğŸ“ {file_path}")
                for issue in issues[:5]:  # æ¯ä¸ªæ–‡ä»¶æœ€å¤šæ˜¾ç¤º5ä¸ªé—®é¢˜
                    user_prompt_parts.append(
                        f"   â€¢ è¡Œ{issue['line']}: {issue['severity']} - {issue['message']}"
                    )

                # è®¡ç®—ä¸¥é‡ç¨‹åº¦
                high_count = sum(1 for issue in issues if issue['severity'].upper() in ['HIGH', 'CRITICAL'])
                medium_count = sum(1 for issue in issues if issue['severity'].upper() == 'MEDIUM')

                if high_count > 0:
                    user_prompt_parts.append(f"   âš ï¸  åŒ…å« {high_count} ä¸ªé«˜ä¸¥é‡ç¨‹åº¦é—®é¢˜")
                if medium_count > 0:
                    user_prompt_parts.append(f"   âš ï¸  åŒ…å« {medium_count} ä¸ªä¸­ç­‰ä¸¥é‡ç¨‹åº¦é—®é¢˜")

            if not problem_files:
                user_prompt_parts.append("   æœªå‘ç°å…·ä½“çš„é™æ€åˆ†æé—®é¢˜")

            user_prompt_parts.append("")
        else:
            user_prompt_parts.extend([
                "## é™æ€åˆ†æç»“æœ",
                "âš ï¸ æœªæ”¶åˆ°é™æ€åˆ†æç»“æœï¼Œè¯·åŸºäºé¡¹ç›®ç»“æ„è¿›è¡Œæ–‡ä»¶é€‰æ‹©",
                ""
            ])

        # æ·»åŠ è¿è¡Œæ—¶é”™è¯¯
        if runtime_errors:
            user_prompt_parts.extend([
                "## è¿è¡Œæ—¶é”™è¯¯",
                "å‘ç°ä»¥ä¸‹è¿è¡Œæ—¶é—®é¢˜ï¼š"
            ])
            for error in runtime_errors[:5]:
                file_path = error.get('file', 'unknown')
                message = error.get('message', 'æœªçŸ¥é”™è¯¯')
                user_prompt_parts.append(f"- {file_path}: {message[:100]}")
            user_prompt_parts.append("")

        # æ·»åŠ é¡¹ç›®ç»“æ„ä¿¡æ¯
        if project_structure:
            user_prompt_parts.extend([
                "## é¡¹ç›®ç»“æ„",
                f"ä¸»è¦æ–‡ä»¶å’Œç›®å½•ï¼š"
            ])
            # ç®€åŒ–æ˜¾ç¤ºé¡¹ç›®ç»“æ„
            if isinstance(project_structure, dict):
                for key, value in list(project_structure.items())[:10]:
                    user_prompt_parts.append(f"- {key}: {str(value)[:50]}")
            user_prompt_parts.append("")

        user_prompt_parts.extend([
            "## ä»»åŠ¡è¦æ±‚",
            f"åŸºäºä»¥ä¸Šä¿¡æ¯ï¼Œè¯·é€‰æ‹©éœ€è¦é‡ç‚¹åˆ†æçš„æ–‡ä»¶ã€‚",
            f"ä¼˜å…ˆé€‰æ‹©æœ‰å®‰å…¨é£é™©ã€ä¸¥é‡é”™è¯¯æˆ–æ ¸å¿ƒä¸šåŠ¡é€»è¾‘çš„æ–‡ä»¶ã€‚",
            f"ç¡®ä¿é€‰æ‹©çš„æ–‡ä»¶è¦†ç›–æœ€é‡è¦çš„é—®é¢˜ã€‚",
            f""
        ])

        user_prompt = "\n".join(user_prompt_parts)

        # è¿”å›æç¤ºè¯å¯¹è±¡
        return type('Prompt', (), {
            'system_prompt': system_prompt.strip(),
            'user_prompt': user_prompt.strip()
        })()

class FileSelectionCriteria:
    def __init__(self, *args, **kwargs):
        pass


@dataclass
class FileSelectionResult:
    """æ–‡ä»¶é€‰æ‹©ç»“æœ"""
    file_path: str
    priority: str  # high, medium, low
    reason: str
    confidence: float  # 0.0-1.0
    key_issues: List[str] = field(default_factory=list)
    selection_score: float = 0.0

    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        return {
            "file_path": self.file_path,
            "priority": self.priority,
            "reason": self.reason,
            "confidence": self.confidence,
            "key_issues": self.key_issues,
            "selection_score": self.selection_score
        }


@dataclass
class AIFileSelectionResult:
    """AIæ–‡ä»¶é€‰æ‹©æ‰§è¡Œç»“æœ"""
    selected_files: List[FileSelectionResult] = field(default_factory=list)
    selection_summary: Dict[str, Any] = field(default_factory=dict)
    execution_success: bool = True
    execution_time: float = 0.0
    error_message: str = ""
    ai_response_raw: str = ""
    token_usage: Dict[str, int] = field(default_factory=dict)
    execution_timestamp: str = ""

    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        return {
            "selected_files": [file.to_dict() for file in self.selected_files],
            "selection_summary": self.selection_summary,
            "execution_success": self.execution_success,
            "execution_time": self.execution_time,
            "error_message": self.error_message,
            "ai_response_raw": self.ai_response_raw,
            "token_usage": self.token_usage,
            "execution_timestamp": self.execution_timestamp,
            "total_selected": len(self.selected_files)
        }


@dataclass
class SelectionStatistics:
    """é€‰æ‹©ç»Ÿè®¡ä¿¡æ¯"""
    total_selected: int = 0
    high_priority_count: int = 0
    medium_priority_count: int = 0
    low_priority_count: int = 0
    average_confidence: float = 0.0
    language_distribution: Dict[str, int] = field(default_factory=dict)
    reason_categories: Dict[str, int] = field(default_factory=dict)


class AIFileSelector:
    """AIæ–‡ä»¶é€‰æ‹©æ‰§è¡Œå™¨"""

    def __init__(self,
                 llm_client: Optional[Any] = None,
                 max_retries: int = 3,
                 retry_delay: float = 1.0):
        self.llm_client = llm_client
        self.max_retries = max_retries
        self.retry_delay = retry_delay
        self.logger = get_logger()

        # åˆå§‹åŒ–ç»„ä»¶
        self.prompt_builder = AIFileSelectionPromptBuilder()

        # ä¼˜å…ˆçº§æƒé‡
        self.priority_weights = {
            "high": 3.0,
            "medium": 2.0,
            "low": 1.0
        }

    def select_files(self,
                    project_path: str,
                    analysis_results: List[Any] = None,
                    user_requirements: str = "",
                    analysis_focus: List[str] = None,
                    runtime_errors: List[Dict[str, Any]] = None,
                    project_structure: Dict[str, Any] = None) -> AIFileSelectionResult:
        """
        æ‰§è¡ŒAIæ–‡ä»¶é€‰æ‹©

        Args:
            project_path: é¡¹ç›®è·¯å¾„
            analysis_results: é™æ€åˆ†æç»“æœ
            user_requirements: ç”¨æˆ·éœ€æ±‚
            analysis_focus: åˆ†æé‡ç‚¹
            runtime_errors: è¿è¡Œæ—¶é”™è¯¯
            project_structure: é¡¹ç›®ç»“æ„

        Returns:
            AIFileSelectionResult: AIé€‰æ‹©ç»“æœ
        """
        start_time = time.time()
        self.logger.info("å¼€å§‹æ‰§è¡ŒAIæ–‡ä»¶é€‰æ‹©")

        result = AIFileSelectionResult(
            execution_timestamp=datetime.now().isoformat()
        )

        try:
            # ä½¿ç”¨AIè¿›è¡Œæ–‡ä»¶é€‰æ‹©
            project_root = Path(project_path)

            # æ„å»ºAIæç¤ºè¯
            prompt = self.prompt_builder.build_prompt(
                project_path=project_path,
                analysis_results=analysis_results,
                user_requirements=user_requirements,
                analysis_focus=analysis_focus or [],
                runtime_errors=runtime_errors or [],
                project_structure=project_structure or {}
            )

            # è°ƒç”¨AIè¿›è¡Œæ–‡ä»¶é€‰æ‹©
            ai_response = self._call_ai_with_retry(prompt)

            selected_files = []

            if ai_response.get("success", False):
                # è§£æAIå“åº”
                try:
                    ai_content = ai_response.get("content", "")
                    self.logger.debug(f"AIå“åº”å†…å®¹: {ai_content[:200]}...")

                    # ä½¿ç”¨æ–°çš„è§£ææ–¹æ³•
                    ai_data = self._parse_ai_response(ai_content)

                    if ai_data:
                        # ä»AIå“åº”ä¸­æå–é€‰æ‹©çš„æ–‡ä»¶
                        ai_selected = ai_data.get("selected_files", [])
                        for file_data in ai_selected:
                            selected_files.append({
                                "file_path": file_data.get("file_path", ""),
                                "priority": file_data.get("priority", "medium"),
                                "reason": file_data.get("reason", "AIå»ºè®®åˆ†ææ­¤æ–‡ä»¶"),
                                "confidence": float(file_data.get("confidence", 0.7)),
                                "key_issues": file_data.get("key_issues", [])
                            })

                        self.logger.info(f"AIæˆåŠŸé€‰æ‹©äº† {len(selected_files)} ä¸ªæ–‡ä»¶")
                    else:
                        self.logger.warning("AIå“åº”è§£æä¸ºç©ºï¼Œä½¿ç”¨å¤‡ç”¨é€»è¾‘")
                        selected_files = self._fallback_file_selection(
                            project_root, analysis_results, runtime_errors
                        )

                except Exception as e:
                    self.logger.warning(f"è§£æAIå“åº”å¤±è´¥: {e}")
                    # å¦‚æœè§£æå¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨é€»è¾‘
                    selected_files = self._fallback_file_selection(
                        project_root, analysis_results, runtime_errors
                    )
            else:
                self.logger.warning(f"AIè°ƒç”¨å¤±è´¥: {ai_response.get('error_message', 'æœªçŸ¥é”™è¯¯')}")
                # å¦‚æœAIè°ƒç”¨å¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨é€»è¾‘
                selected_files = self._fallback_file_selection(
                    project_root, analysis_results, runtime_errors
                )

            # å¤„ç†é€‰æ‹©çš„æ–‡ä»¶
            result.selected_files = self._process_selected_files(selected_files)

            # ç”Ÿæˆé€‰æ‹©æ‘˜è¦
            result.selection_summary = self._generate_selection_summary(
                result.selected_files, {"confidence": 0.75, "criteria_met": True}
            )

            # è®¡ç®—æ‰§è¡Œæ—¶é—´
            result.execution_time = time.time() - start_time

            self.logger.info(
                f"AIæ–‡ä»¶é€‰æ‹©å®Œæˆ: é€‰æ‹©äº† {len(result.selected_files)} ä¸ªæ–‡ä»¶, "
                f"è€—æ—¶ {result.execution_time:.2f}ç§’"
            )

        except Exception as e:
            result.execution_success = False
            result.error_message = f"AIæ–‡ä»¶é€‰æ‹©æ‰§è¡Œå¤±è´¥: {e}"
            result.execution_time = time.time() - start_time
            self.logger.error(result.error_message)

        return result

    def _call_ai_with_retry(self, prompt) -> Dict[str, Any]:
        """å¸¦é‡è¯•çš„AIè°ƒç”¨"""
        last_error = ""

        for attempt in range(self.max_retries):
            try:
                self.logger.info(f"è°ƒç”¨AIæ¨¡å‹ï¼Œå°è¯• {attempt + 1}/{self.max_retries}")

                if self.llm_client is None:
                    # å¦‚æœæ²¡æœ‰æä¾›LLMå®¢æˆ·ç«¯ï¼Œå°è¯•åˆ›å»ºé»˜è®¤å®¢æˆ·ç«¯
                    self.llm_client = self._create_default_llm_client()

                # å‡†å¤‡è°ƒç”¨å‚æ•°
                call_params = {
                    "messages": [
                        {"role": "system", "content": prompt.system_prompt},
                        {"role": "user", "content": prompt.user_prompt}
                    ],
                    "temperature": 0.3,  # è¾ƒä½æ¸©åº¦ä»¥è·å¾—æ›´ä¸€è‡´çš„ç»“æœ
                    "max_tokens": 2000
                    # æ³¨æ„ï¼šresponse_formatæš‚ä¸æ”¯æŒï¼Œåœ¨ç³»ç»Ÿæç¤ºè¯ä¸­æŒ‡å®šJSONæ ¼å¼
                }

                # è°ƒç”¨AI
                response = self.llm_client.chat_completion(**call_params)

                if response.get("success", False):
                    self.logger.info("AIè°ƒç”¨æˆåŠŸ")
                    return response
                else:
                    last_error = response.get("error_message", "æœªçŸ¥é”™è¯¯")
                    self.logger.warning(f"AIè°ƒç”¨å¤±è´¥: {last_error}")

            except Exception as e:
                last_error = str(e)
                self.logger.error(f"AIè°ƒç”¨å¼‚å¸¸ (å°è¯• {attempt + 1}): {e}")

            # å¦‚æœä¸æ˜¯æœ€åä¸€æ¬¡å°è¯•ï¼Œç­‰å¾…é‡è¯•
            if attempt < self.max_retries - 1:
                time.sleep(self.retry_delay * (2 ** attempt))  # æŒ‡æ•°é€€é¿

        self.logger.error(f"AIè°ƒç”¨æœ€ç»ˆå¤±è´¥: {last_error}")
        return {
            "success": False,
            "error_message": f"AIè°ƒç”¨å¤±è´¥ï¼Œå·²é‡è¯• {self.max_retries} æ¬¡: {last_error}"
        }

    def _create_default_llm_client(self):
        """åˆ›å»ºé»˜è®¤LLMå®¢æˆ·ç«¯"""
        try:
            from ..llm.client import LLMClient
            return LLMClient()
        except Exception as e:
            self.logger.error(f"æ— æ³•åˆ›å»ºé»˜è®¤LLMå®¢æˆ·ç«¯: {e}")
            raise

    def _parse_ai_response(self, ai_content: str) -> Optional[Dict[str, Any]]:
        """
        è§£æAIå“åº”

        Args:
            ai_content: AIè¿”å›çš„å†…å®¹

        Returns:
            Optional[Dict[str, Any]]: è§£æåçš„æ•°æ®
        """
        try:
            # å°è¯•ç›´æ¥è§£æJSON
            parsed_data = json.loads(ai_content)
            return self._validate_and_normalize_response(parsed_data)

        except json.JSONDecodeError:
            # å¦‚æœç›´æ¥è§£æå¤±è´¥ï¼Œå°è¯•æå–JSONéƒ¨åˆ†
            json_content = self._extract_json_from_response(ai_content)
            if json_content:
                try:
                    parsed_data = json.loads(json_content)
                    return self._validate_and_normalize_response(parsed_data)
                except json.JSONDecodeError as e:
                    self.logger.error(f"æå–çš„JSONè§£æå¤±è´¥: {e}")

            self.logger.error("æ— æ³•è§£æAIå“åº”ä¸ºJSONæ ¼å¼")
            return None

    def _extract_json_from_response(self, content: str) -> Optional[str]:
        """ä»å“åº”ä¸­æå–JSONå†…å®¹"""
        import re

        # é¦–å…ˆå°è¯•ç§»é™¤markdownä»£ç å—æ ‡è®°
        # åŒ¹é… ```json...``` æˆ– ```...``` æ ¼å¼
        code_block_pattern = r'```(?:json)?\s*(\{.*?\})\s*```'
        code_matches = re.findall(code_block_pattern, content, re.DOTALL | re.IGNORECASE)

        if code_matches:
            # ä½¿ç”¨ä»£ç å—ä¸­çš„å†…å®¹
            json_content = code_matches[0]
            try:
                json.loads(json_content)
                self.logger.info("æˆåŠŸä»markdownä»£ç å—ä¸­æå–JSON")
                return json_content
            except json.JSONDecodeError:
                self.logger.warning("markdownä»£ç å—ä¸­çš„JSONæ ¼å¼æ— æ•ˆï¼Œå°è¯•å…¶ä»–æ–¹æ³•")

        # å¦‚æœæ²¡æœ‰ä»£ç å—æˆ–æå–å¤±è´¥ï¼Œå°è¯•ç›´æ¥åŒ¹é…JSONå¯¹è±¡
        json_pattern = r'\{.*\}'
        matches = re.findall(json_pattern, content, re.DOTALL)

        if matches:
            # é€‰æ‹©æœ€é•¿çš„åŒ¹é…ï¼ˆé€šå¸¸æ˜¯æœ€å®Œæ•´çš„JSONï¼‰
            longest_match = max(matches, key=len)

            # å°è¯•éªŒè¯æ˜¯å¦ä¸ºæœ‰æ•ˆJSON
            try:
                json.loads(longest_match)
                self.logger.info("æˆåŠŸä»æ–‡æœ¬ä¸­æå–JSONå¯¹è±¡")
                return longest_match
            except json.JSONDecodeError:
                self.logger.warning("æå–çš„JSONå¯¹è±¡æ ¼å¼æ— æ•ˆ")

        self.logger.error("æ— æ³•ä»å“åº”ä¸­æ‰¾åˆ°æœ‰æ•ˆçš„JSONå†…å®¹")
        return None

    def _validate_and_normalize_response(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """éªŒè¯å’Œæ ‡å‡†åŒ–å“åº”æ•°æ®"""
        normalized = {
            "selected_files": [],
            "selection_summary": {}
        }

        # å¤„ç†é€‰æ‹©çš„æ–‡ä»¶
        selected_files = data.get("selected_files", [])
        if not isinstance(selected_files, list):
            self.logger.warning("selected_filesä¸æ˜¯åˆ—è¡¨æ ¼å¼")
            return normalized

        for file_data in selected_files:
            if not isinstance(file_data, dict):
                continue

            normalized_file = {
                "file_path": file_data.get("file_path", ""),
                "priority": file_data.get("priority", "medium"),
                "reason": file_data.get("reason", ""),
                "confidence": float(file_data.get("confidence", 0.5)),
                "key_issues": file_data.get("key_issues", [])
            }

            # éªŒè¯å¿…éœ€å­—æ®µ
            if not normalized_file["file_path"]:
                continue

            # éªŒè¯ä¼˜å…ˆçº§
            if normalized_file["priority"] not in ["high", "medium", "low"]:
                normalized_file["priority"] = "medium"

            # éªŒè¯ç½®ä¿¡åº¦èŒƒå›´
            normalized_file["confidence"] = max(0.0, min(1.0, normalized_file["confidence"]))

            normalized["selected_files"].append(normalized_file)

        # å¤„ç†é€‰æ‹©æ‘˜è¦
        selection_summary = data.get("selection_summary", {})
        if isinstance(selection_summary, dict):
            normalized["selection_summary"] = {
                "total_selected": selection_summary.get("total_selected", len(normalized["selected_files"])),
                "selection_criteria_met": selection_summary.get("selection_criteria_met", True),
                "additional_notes": selection_summary.get("additional_notes", "")
            }
        else:
            normalized["selection_summary"] = {
                "total_selected": len(normalized["selected_files"]),
                "selection_criteria_met": True,
                "additional_notes": ""
            }

        return normalized

    def _process_selected_files(self, selected_files: List[Dict[str, Any]]) -> List[FileSelectionResult]:
        """å¤„ç†é€‰æ‹©çš„æ–‡ä»¶"""
        processed_files = []

        for file_data in selected_files:
            # è®¡ç®—é€‰æ‹©åˆ†æ•°
            selection_score = self._calculate_selection_score(file_data)

            result = FileSelectionResult(
                file_path=file_data["file_path"],
                priority=file_data["priority"],
                reason=file_data["reason"],
                confidence=file_data["confidence"],
                key_issues=file_data.get("key_issues", []),
                selection_score=selection_score
            )

            processed_files.append(result)

        # æŒ‰é€‰æ‹©åˆ†æ•°æ’åº
        processed_files.sort(key=lambda x: x.selection_score, reverse=True)

        return processed_files

    def _calculate_selection_score(self, file_data: Dict[str, Any]) -> float:
        """è®¡ç®—æ–‡ä»¶é€‰æ‹©åˆ†æ•°"""
        score = 0.0

        # åŸºäºä¼˜å…ˆçº§çš„åˆ†æ•°
        priority = file_data.get("priority", "medium")
        score += self.priority_weights.get(priority, 2.0)

        # åŸºäºç½®ä¿¡åº¦çš„åˆ†æ•°
        confidence = file_data.get("confidence", 0.5)
        score += confidence * 2.0

        # åŸºäºå…³é”®é—®é¢˜æ•°é‡çš„åˆ†æ•°
        key_issues = file_data.get("key_issues", [])
        score += len(key_issues) * 0.5

        # åŸºäºç†ç”±è¯¦ç»†ç¨‹åº¦çš„åˆ†æ•°
        reason = file_data.get("reason", "")
        reason_score = min(len(reason) / 50, 2.0)  # æœ€å¤š2åˆ†
        score += reason_score

        return round(score, 2)

    def _generate_selection_summary(self,
                                  selected_files: List[FileSelectionResult],
                                  ai_summary: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆé€‰æ‹©æ‘˜è¦"""
        statistics = self._calculate_selection_statistics(selected_files)

        summary = {
            "total_selected": len(selected_files),
            "priority_distribution": {
                "high": statistics.high_priority_count,
                "medium": statistics.medium_priority_count,
                "low": statistics.low_priority_count
            },
            "average_confidence": round(statistics.average_confidence, 2),
            "average_selection_score": round(
                sum(f.selection_score for f in selected_files) / len(selected_files), 2
            ) if selected_files else 0.0,
            "language_distribution": statistics.language_distribution,
            "reason_categories": statistics.reason_categories,
            "top_selected_files": [
                {
                    "file_path": f.file_path,
                    "selection_score": f.selection_score,
                    "priority": f.priority,
                    "reason": f.reason[:100] + "..." if len(f.reason) > 100 else f.reason
                }
                for f in selected_files[:5]
            ],
            "ai_summary": ai_summary
        }

        return summary

    def _calculate_selection_statistics(self, selected_files: List[FileSelectionResult]) -> SelectionStatistics:
        """è®¡ç®—é€‰æ‹©ç»Ÿè®¡ä¿¡æ¯"""
        stats = SelectionStatistics()

        if not selected_files:
            return stats

        stats.total_selected = len(selected_files)

        # ç»Ÿè®¡ä¼˜å…ˆçº§åˆ†å¸ƒ
        for file_result in selected_files:
            if file_result.priority == "high":
                stats.high_priority_count += 1
            elif file_result.priority == "medium":
                stats.medium_priority_count += 1
            else:
                stats.low_priority_count += 1

        # è®¡ç®—å¹³å‡ç½®ä¿¡åº¦
        total_confidence = sum(f.confidence for f in selected_files)
        stats.average_confidence = total_confidence / len(selected_files)

        # ç»Ÿè®¡è¯­è¨€åˆ†å¸ƒï¼ˆä»æ–‡ä»¶è·¯å¾„æ¨æ–­ï¼‰
        language_extensions = {
            ".py": "Python",
            ".js": "JavaScript",
            ".ts": "TypeScript",
            ".java": "Java",
            ".go": "Go",
            ".cpp": "C++",
            ".c": "C",
            ".cs": "C#",
            ".rs": "Rust",
            ".php": "PHP",
            ".rb": "Ruby"
        }

        for file_result in selected_files:
            import os
            _, ext = os.path.splitext(file_result.file_path)
            language = language_extensions.get(ext.lower(), "Other")
            stats.language_distribution[language] = stats.language_distribution.get(language, 0) + 1

        # ç»Ÿè®¡ç†ç”±ç±»åˆ«
        reason_keywords = {
            "security": ["å®‰å…¨", "æ¼æ´", "é£é™©", "security", "vulnerability"],
            "performance": ["æ€§èƒ½", "æ•ˆç‡", "ç¼“æ…¢", "performance", "efficiency"],
            "quality": ["è´¨é‡", "è§„èŒƒ", "æ ‡å‡†", "quality", "standard"],
            "complexity": ["å¤æ‚", "éš¾åº¦", "ç»´æŠ¤", "complexity", "maintenance"],
            "importance": ["é‡è¦", "æ ¸å¿ƒ", "å…³é”®", "important", "core", "critical"]
        }

        for file_result in selected_files:
            reason_lower = file_result.reason.lower()
            categorized = False

            for category, keywords in reason_keywords.items():
                if any(keyword in reason_lower for keyword in keywords):
                    stats.reason_categories[category] = stats.reason_categories.get(category, 0) + 1
                    categorized = True
                    break

            if not categorized:
                stats.reason_categories["other"] = stats.reason_categories.get("other", 0) + 1

        return stats

    def validate_selection_result(self,
                                 result: AIFileSelectionResult,
                                 criteria: FileSelectionCriteria) -> Dict[str, Any]:
        """
        éªŒè¯é€‰æ‹©ç»“æœ

        Args:
            result: AIé€‰æ‹©ç»“æœ
            criteria: é€‰æ‹©æ ‡å‡†

        Returns:
            Dict[str, Any]: éªŒè¯ç»“æœ
        """
        validation = {
            "is_valid": True,
            "warnings": [],
            "errors": [],
            "metrics": {}
        }

        # æ£€æŸ¥æ‰§è¡ŒæˆåŠŸ
        if not result.execution_success:
            validation["is_valid"] = False
            validation["errors"].append(f"æ‰§è¡Œå¤±è´¥: {result.error_message}")
            return validation

        selected_files = result.selected_files

        # æ£€æŸ¥æ–‡ä»¶æ•°é‡
        if len(selected_files) == 0:
            validation["is_valid"] = False
            validation["errors"].append("æ²¡æœ‰é€‰æ‹©ä»»ä½•æ–‡ä»¶")
        elif len(selected_files) > criteria.max_files:
            validation["warnings"].append(
                f"é€‰æ‹©çš„æ–‡ä»¶æ•°é‡({len(selected_files)})è¶…è¿‡é™åˆ¶({criteria.max_files})"
            )

        # æ£€æŸ¥æ–‡ä»¶è·¯å¾„æœ‰æ•ˆæ€§
        invalid_files = []
        for file_result in selected_files:
            if not file_result.file_path or not isinstance(file_result.file_path, str):
                invalid_files.append(file_result.file_path)

        if invalid_files:
            validation["warnings"].append(f"å‘ç° {len(invalid_files)} ä¸ªæ— æ•ˆæ–‡ä»¶è·¯å¾„")

        # æ£€æŸ¥ç½®ä¿¡åº¦åˆ†å¸ƒ
        low_confidence_files = [f for f in selected_files if f.confidence < 0.5]
        if len(low_confidence_files) > len(selected_files) * 0.5:
            validation["warnings"].append(
                f"è¶…è¿‡50%çš„æ–‡ä»¶é€‰æ‹©ç½®ä¿¡åº¦è¾ƒä½({len(low_confidence_files)}/{len(selected_files)})"
            )

        # è®¡ç®—æŒ‡æ ‡
        validation["metrics"] = {
            "total_files": len(selected_files),
            "avg_confidence": round(sum(f.confidence for f in selected_files) / len(selected_files), 2) if selected_files else 0,
            "high_priority_ratio": round(
                len([f for f in selected_files if f.priority == "high"]) / len(selected_files), 2
            ) if selected_files else 0,
            "avg_selection_score": round(
                sum(f.selection_score for f in selected_files) / len(selected_files), 2
            ) if selected_files else 0
        }

        return validation

    def _fallback_file_selection(self, project_root: Path, analysis_results: List[Any] = None, runtime_errors: List[Dict[str, Any]] = None) -> List[Dict[str, Any]]:
        """å¤‡ç”¨æ–‡ä»¶é€‰æ‹©é€»è¾‘ï¼ˆåŸºäºè§„åˆ™ï¼‰"""
        selected_files = []

        # ä»é™æ€åˆ†æç»“æœä¸­é€‰æ‹©æ–‡ä»¶
        if analysis_results:
            for analysis_result in analysis_results:
                if hasattr(analysis_result, 'issues') and analysis_result.issues:
                    # å¦‚æœæœ‰é—®é¢˜çš„æ–‡ä»¶ï¼Œä¼˜å…ˆé€‰æ‹©
                    for issue in analysis_result.issues[:5]:  # é™åˆ¶æ•°é‡
                        file_path = getattr(issue, 'file_path', 'unknown')
                        if file_path != 'unknown':
                            selected_files.append({
                                "file_path": file_path,
                                "priority": "high",
                                "reason": f"å‘ç°{getattr(issue, 'severity', 'unknown')}çº§åˆ«é—®é¢˜: {getattr(issue, 'message', '')[:50]}",
                                "confidence": 0.8,
                                "key_issues": [getattr(issue, 'message', '')]
                            })
                elif hasattr(analysis_result, 'file_path'):
                    # å¦‚æœæ²¡æœ‰é—®é¢˜è®°å½•ä½†æœ‰æ–‡ä»¶è·¯å¾„ï¼Œä¹Ÿé€‰æ‹©ä¸€äº›é‡è¦æ–‡ä»¶
                    file_path = analysis_result.file_path
                    selected_files.append({
                        "file_path": file_path,
                        "priority": "medium",
                        "reason": "é‡è¦æ–‡ä»¶ï¼Œéœ€è¦è¿›ä¸€æ­¥åˆ†æ",
                        "confidence": 0.6,
                        "key_issues": []
                    })

        # ä»è¿è¡Œæ—¶é”™è¯¯ä¸­é€‰æ‹©æ–‡ä»¶
        if runtime_errors:
            for error in runtime_errors:
                file_path = error.get('file', '')
                if file_path:
                    selected_files.append({
                        "file_path": file_path,
                        "priority": "high",
                        "reason": f"è¿è¡Œæ—¶é”™è¯¯: {error.get('message', '')[:50]}",
                        "confidence": 0.9,
                        "key_issues": [error.get('message', '')]
                    })

        # å¦‚æœæ²¡æœ‰å…¶ä»–æ–‡ä»¶ï¼Œé€‰æ‹©é¡¹ç›®ä¸­çš„ä¸»è¦æ–‡ä»¶
        if not selected_files and project_root.exists():
            main_files = ['main.py', 'app.py', 'index.js', 'app.js', 'main.go', 'main.rs']
            for main_file in main_files:
                main_file_path = project_root / main_file
                if main_file_path.exists():
                    selected_files.append({
                        "file_path": main_file,
                        "priority": "medium",
                        "reason": "é¡¹ç›®ä¸»å…¥å£æ–‡ä»¶",
                        "confidence": 0.7,
                        "key_issues": []
                    })
                    break

        return selected_files


# ä¾¿æ·å‡½æ•°
def select_files_with_ai(project_report: Dict[str, Any],
                        max_files: int = 20,
                        preferred_languages: List[str] = None,
                        focus_categories: List[str] = None,
                        user_requirements: str = None,
                        llm_client: Any = None) -> Dict[str, Any]:
    """
    ä¾¿æ·çš„AIæ–‡ä»¶é€‰æ‹©å‡½æ•°

    Args:
        project_report: é¡¹ç›®åˆ†ææŠ¥å‘Š
        max_files: æœ€å¤§æ–‡ä»¶é€‰æ‹©æ•°é‡
        preferred_languages: ä¼˜å…ˆç¼–ç¨‹è¯­è¨€
        focus_categories: å…³æ³¨é—®é¢˜ç±»å‹
        user_requirements: ç”¨æˆ·ç‰¹æ®Šéœ€æ±‚
        llm_client: LLMå®¢æˆ·ç«¯å®ä¾‹

    Returns:
        Dict[str, Any]: é€‰æ‹©ç»“æœå’ŒéªŒè¯ä¿¡æ¯
    """
    criteria = FileSelectionCriteria(
        max_files=max_files,
        preferred_languages=preferred_languages or [],
        focus_categories=focus_categories or []
    )

    selector = AIFileSelector(llm_client)
    result = selector.select_files(project_report, criteria, user_requirements)
    validation = selector.validate_selection_result(result, criteria)

    return {
        "selection_result": result.to_dict(),
        "validation": validation
    }