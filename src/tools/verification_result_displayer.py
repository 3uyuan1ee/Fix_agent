#!/usr/bin/env python3
"""
éªŒè¯ç»“æœå±•ç¤ºå™¨ - T015.1
ä»¥ç”¨æˆ·å‹å¥½çš„æ–¹å¼å±•ç¤ºä¿®å¤éªŒè¯ç»“æœ
"""

import json
from dataclasses import asdict, dataclass
from datetime import datetime
from enum import Enum
from pathlib import Path
from typing import Any, Dict, List, Optional

from ..utils.config import get_config_manager
from ..utils.logger import get_logger
from .ai_dynamic_analysis_caller import AIDynamicAnalysisResult
from .fix_verification_aggregator import (
    ComprehensiveVerificationReport,
    RecommendedAction,
    VerificationStatus,
)
from .verification_static_analyzer import StaticVerificationReport

logger = get_logger()


@dataclass
class VerificationDisplayData:
    """éªŒè¯å±•ç¤ºæ•°æ®"""

    display_id: str
    session_id: str
    suggestion_id: str
    file_path: str
    summary_overview: Dict[str, Any]
    fix_effectiveness: Dict[str, Any]
    quality_impact: Dict[str, Any]
    new_issues_analysis: Dict[str, Any]
    risk_assessment: Dict[str, Any]
    recommendations: List[str]
    detailed_metrics: Dict[str, Any]

    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        return {
            "display_id": self.display_id,
            "session_id": self.session_id,
            "suggestion_id": self.suggestion_id,
            "file_path": self.file_path,
            "summary_overview": self.summary_overview,
            "fix_effectiveness": self.fix_effectiveness,
            "quality_impact": self.quality_impact,
            "new_issues_analysis": self.new_issues_analysis,
            "risk_assessment": self.risk_assessment,
            "recommendations": self.recommendations,
            "detailed_metrics": self.detailed_metrics,
        }


class DisplayFormat(Enum):
    """å±•ç¤ºæ ¼å¼"""

    SUMMARY = "summary"  # æ‘˜è¦æ ¼å¼
    DETAILED = "detailed"  # è¯¦ç»†æ ¼å¼
    TECHNICAL = "technical"  # æŠ€æœ¯æ ¼å¼
    QUICK_OVERVIEW = "quick_overview"  # å¿«é€Ÿæ¦‚è§ˆ
    COMPARISON = "comparison"  # å¯¹æ¯”æ ¼å¼


class VerificationResultDisplayer:
    """éªŒè¯ç»“æœå±•ç¤ºå™¨ - T015.1

    å·¥ä½œæµä½ç½®: èŠ‚ç‚¹Iæ ¸å¿ƒï¼Œå‘ç”¨æˆ·å±•ç¤ºä¿®å¤éªŒè¯ç»“æœ
    æ ¸å¿ƒåŠŸèƒ½: ä»¥ç”¨æˆ·å‹å¥½çš„æ–¹å¼å±•ç¤ºä¿®å¤éªŒè¯ç»“æœ
    ç”¨æˆ·åä½œ: ä¸ºç”¨æˆ·æä¾›æ¸…æ™°çš„éªŒè¯ç»“æœä¿¡æ¯
    """

    def __init__(self, config_manager=None):
        """åˆå§‹åŒ–éªŒè¯ç»“æœå±•ç¤ºå™¨"""
        self.config_manager = config_manager or get_config_manager()
        self.logger = get_logger()

        # è·å–é…ç½®
        self.config = self.config_manager.get("project_analysis", {})

        # å±•ç¤ºé…ç½®
        self.display_config = self.config.get(
            "verification_display",
            {
                "default_format": "summary",
                "show_code_snippets": True,
                "show_metrics_charts": False,
                "max_recommendations": 10,
                "color_coding": True,
            },
        )

    def display_verification_results(
        self,
        comprehensive_report: ComprehensiveVerificationReport,
        display_format: DisplayFormat = None,
    ) -> VerificationDisplayData:
        """
        å±•ç¤ºéªŒè¯ç»“æœ

        Args:
            comprehensive_report: ç»¼åˆéªŒè¯æŠ¥å‘Š
            display_format: å±•ç¤ºæ ¼å¼

        Returns:
            VerificationDisplayData: æ ¼å¼åŒ–çš„å±•ç¤ºæ•°æ®
        """
        try:
            self.logger.info(
                f"å¼€å§‹å±•ç¤ºéªŒè¯ç»“æœ: å»ºè®®={comprehensive_report.suggestion_id}"
            )

            # ç¡®å®šå±•ç¤ºæ ¼å¼
            format_type = display_format or DisplayFormat(
                self.display_config["default_format"]
            )

            # æ„å»ºå±•ç¤ºæ•°æ®
            display_data = VerificationDisplayData(
                display_id=str(datetime.now().timestamp()),
                session_id=comprehensive_report.session_id,
                suggestion_id=comprehensive_report.suggestion_id,
                file_path=comprehensive_report.file_path,
                summary_overview=self._build_summary_overview(comprehensive_report),
                fix_effectiveness=self._build_fix_effectiveness(comprehensive_report),
                quality_impact=self._build_quality_impact(comprehensive_report),
                new_issues_analysis=self._build_new_issues_analysis(
                    comprehensive_report
                ),
                risk_assessment=self._build_risk_assessment_display(
                    comprehensive_report
                ),
                recommendations=self._build_recommendations_display(
                    comprehensive_report
                ),
                detailed_metrics=self._build_detailed_metrics(comprehensive_report),
            )

            # æ ¹æ®æ ¼å¼è°ƒæ•´å±•ç¤ºå†…å®¹
            if format_type == DisplayFormat.SUMMARY:
                display_data = self._format_for_summary(display_data)
            elif format_type == DisplayFormat.DETAILED:
                display_data = self._format_for_detailed(display_data)
            elif format_type == DisplayFormat.TECHNICAL:
                display_data = self._format_for_technical(display_data)
            elif format_type == DisplayFormat.QUICK_OVERVIEW:
                display_data = self._format_for_quick_overview(display_data)
            elif format_type == DisplayFormat.COMPARISON:
                display_data = self._format_for_comparison(display_data)

            self.logger.info(f"éªŒè¯ç»“æœå±•ç¤ºå®Œæˆ: æ ¼å¼={format_type.value}")
            return display_data

        except Exception as e:
            self.logger.error(f"å±•ç¤ºéªŒè¯ç»“æœå¤±è´¥: {e}")
            raise

    def _build_summary_overview(
        self, report: ComprehensiveVerificationReport
    ) -> Dict[str, Any]:
        """æ„å»ºæ‘˜è¦æ¦‚è§ˆ"""
        summary = report.verification_summary

        return {
            "verification_status": self._format_status_with_icon(
                summary.verification_status
            ),
            "problem_resolved": self._format_boolean_with_icon(
                summary.problem_resolved
            ),
            "quality_improved": self._format_boolean_with_icon(
                summary.quality_improved
            ),
            "new_issues_introduced": self._format_boolean_with_icon(
                summary.introduced_new_issues, invert=True
            ),
            "recommended_action": self._format_recommended_action(
                summary.recommended_action
            ),
            "confidence_level": f"{summary.confidence_level:.1%}",
            "file_path": report.file_path,
            "verification_time": report.verification_timestamp.strftime(
                "%Y-%m-%d %H:%M:%S"
            ),
        }

    def _build_fix_effectiveness(
        self, report: ComprehensiveVerificationReport
    ) -> Dict[str, Any]:
        """æ„å»ºä¿®å¤æ•ˆæœä¿¡æ¯"""
        metrics = report.verification_metrics
        ai_analysis = report.ai_dynamic_analysis

        # å¤„ç†AIåˆ†æä¸ºNoneçš„æƒ…å†µ
        if ai_analysis is None:
            ai_analysis = type(
                "MockAIAnalysis",
                (),
                {
                    "problem_resolution_status": "unknown",
                    "fix_effectiveness_score": 0.5,
                },
            )()

        return {
            "fix_success_rate": {
                "value": f"{metrics.fix_success_rate:.1%}",
                "description": "ä¿®å¤æˆåŠŸç‡",
                "trend": "up" if metrics.fix_success_rate > 0.7 else "down",
            },
            "problem_resolution_status": {
                "value": ai_analysis.problem_resolution_status,
                "description": "é—®é¢˜è§£å†³çŠ¶æ€",
                "translation": self._translate_resolution_status(
                    ai_analysis.problem_resolution_status
                ),
            },
            "ai_effectiveness_score": {
                "value": f"{ai_analysis.fix_effectiveness_score:.2f}",
                "description": "AIè¯„ä¼°æœ‰æ•ˆæ€§åˆ†æ•°",
                "level": self._get_score_level(ai_analysis.fix_effectiveness_score),
            },
            "static_analysis_score": {
                "value": (
                    f"{report.static_verification.overall_quality_score:.2f}"
                    if report.static_verification
                    else "N/A"
                ),
                "description": "é™æ€åˆ†æè´¨é‡åˆ†æ•°",
                "level": (
                    self._get_score_level(
                        report.static_verification.overall_quality_score
                    )
                    if report.static_verification
                    else "unknown"
                ),
            },
        }

    def _build_quality_impact(
        self, report: ComprehensiveVerificationReport
    ) -> Dict[str, Any]:
        """æ„å»ºè´¨é‡å½±å“ä¿¡æ¯"""
        ai_analysis = report.ai_dynamic_analysis
        metrics = report.verification_metrics

        # å¤„ç†AIåˆ†æä¸ºNoneçš„æƒ…å†µ
        if ai_analysis is None:
            quality_impact = {}
        else:
            quality_impact = ai_analysis.code_quality_impact

        impact_display = {}
        for aspect, impact in quality_impact.items():
            impact_display[aspect] = {
                "value": impact,
                "icon": self._get_impact_icon(impact),
                "description": self._get_aspect_description(aspect),
            }

        # æ·»åŠ ç»¼åˆè´¨é‡åˆ†æ•°
        impact_display["overall_score"] = {
            "value": f"{metrics.quality_improvement_score:.2f}",
            "description": "ç»¼åˆè´¨é‡æ”¹è¿›åˆ†æ•°",
            "level": self._get_score_level(metrics.quality_improvement_score),
            "change": self._calculate_quality_change(metrics.quality_improvement_score),
        }

        return impact_display

    def _build_new_issues_analysis(
        self, report: ComprehensiveVerificationReport
    ) -> Dict[str, Any]:
        """æ„å»ºæ–°é—®é¢˜åˆ†æ"""
        static_report = report.static_verification
        ai_analysis = report.ai_dynamic_analysis

        # å¤„ç†é™æ€éªŒè¯ä¸ºNoneçš„æƒ…å†µ
        if static_report is None:
            static_new_issues = []
            static_new_count = 0
        else:
            # é™æ€åˆ†æå‘ç°çš„æ–°é—®é¢˜
            static_new_issues = [
                issue.to_dict()
                for issue in static_report.verification_issues
                if issue.is_new_issue
            ]
            static_new_count = static_report.new_issues_count

        # AIåˆ†æå‘ç°çš„æ–°é—®é¢˜ - å¤„ç†AIåˆ†æä¸ºNoneçš„æƒ…å†µ
        ai_new_issues = ai_analysis.new_issues_detected if ai_analysis else []

        return {
            "total_new_issues": static_new_count + len(ai_new_issues),
            "static_analysis_issues": {
                "count": len(static_new_issues),
                "issues": static_new_issues[:5],  # åªæ˜¾ç¤ºå‰5ä¸ª
                "has_more": len(static_new_issues) > 5,
            },
            "ai_detected_issues": {
                "count": len(ai_new_issues),
                "issues": ai_new_issues[:5],  # åªæ˜¾ç¤ºå‰5ä¸ª
                "has_more": len(ai_new_issues) > 5,
            },
            "severity_distribution": self._analyze_issue_severity(
                static_new_issues + ai_new_issues
            ),
            "recommendation": self._get_new_issues_recommendation(
                static_new_count + len(ai_new_issues)
            ),
        }

    def _build_risk_assessment_display(
        self, report: ComprehensiveVerificationReport
    ) -> Dict[str, Any]:
        """æ„å»ºé£é™©è¯„ä¼°å±•ç¤º"""
        risk_assessment = report.risk_assessment

        return {
            "overall_risk_level": {
                "level": risk_assessment["overall_risk_level"],
                "icon": self._get_risk_icon(risk_assessment["overall_risk_level"]),
                "color": self._get_risk_color(risk_assessment["overall_risk_level"]),
            },
            "risk_factors": risk_assessment["risk_factors"],
            "mitigation_strategies": risk_assessment.get("mitigation_strategies", []),
            "risk_score": self._calculate_risk_score(risk_assessment),
        }

    def _build_recommendations_display(
        self, report: ComprehensiveVerificationReport
    ) -> List[str]:
        """æ„å»ºå»ºè®®å±•ç¤º"""
        recommendations = report.improvement_recommendations

        # é™åˆ¶å»ºè®®æ•°é‡
        max_recommendations = self.display_config["max_recommendations"]
        if len(recommendations) > max_recommendations:
            recommendations = recommendations[:max_recommendations] + [
                f"... è¿˜æœ‰ {len(recommendations) - max_recommendations} æ¡å»ºè®®"
            ]

        return recommendations

    def _build_detailed_metrics(
        self, report: ComprehensiveVerificationReport
    ) -> Dict[str, Any]:
        """æ„å»ºè¯¦ç»†æŒ‡æ ‡"""
        metrics = report.verification_metrics
        ai_analysis = report.ai_dynamic_analysis

        # å¤„ç†AIåˆ†æä¸ºNoneçš„æƒ…å†µ
        if ai_analysis is None:
            ai_analysis = type(
                "MockAIAnalysis",
                (),
                {
                    "confidence_score": 0.5,
                    "new_issues_detected": [],
                    "recommendations": [],
                },
            )()

        # å¤„ç†é™æ€éªŒè¯ä¸ºNoneçš„æƒ…å†µ
        if report.static_verification is None:
            static_metrics = {
                "original_issues_count": 0,
                "fixed_issues_count": 0,
                "remaining_issues_count": 0,
                "new_issues_count": 0,
            }
        else:
            static_metrics = {
                "original_issues_count": len(
                    report.static_verification.fix_comparison.original_issues
                ),
                "fixed_issues_count": len(
                    report.static_verification.fix_comparison.fixed_issues
                ),
                "remaining_issues_count": len(
                    report.static_verification.fix_comparison.remaining_issues
                ),
                "new_issues_count": report.static_verification.new_issues_count,
            }

        return {
            "verification_metrics": {
                "fix_success_rate": metrics.fix_success_rate,
                "quality_improvement_score": metrics.quality_improvement_score,
                "security_impact_score": metrics.security_impact_score,
                "performance_impact_score": metrics.performance_impact_score,
                "overall_verification_score": metrics.overall_verification_score,
            },
            "static_analysis_metrics": static_metrics,
            "ai_analysis_metrics": {
                "confidence_score": ai_analysis.confidence_score,
                "new_issues_detected_count": len(ai_analysis.new_issues_detected),
                "recommendations_count": len(ai_analysis.recommendations),
            },
        }

    def _format_for_summary(
        self, display_data: VerificationDisplayData
    ) -> VerificationDisplayData:
        """æ ¼å¼åŒ–ä¸ºæ‘˜è¦æ ¼å¼"""
        # ä¿ç•™å…³é”®ä¿¡æ¯ï¼Œç®€åŒ–è¯¦ç»†å†…å®¹
        display_data.detailed_metrics = {}
        display_data.new_issues_analysis = {
            "total_new_issues": display_data.new_issues_analysis["total_new_issues"],
            "recommendation": display_data.new_issues_analysis["recommendation"],
        }
        return display_data

    def _format_for_detailed(
        self, display_data: VerificationDisplayData
    ) -> VerificationDisplayData:
        """æ ¼å¼åŒ–ä¸ºè¯¦ç»†æ ¼å¼"""
        # ä¿æŒå®Œæ•´ä¿¡æ¯ï¼Œæ·»åŠ æ›´å¤šç»†èŠ‚
        return display_data

    def _format_for_technical(
        self, display_data: VerificationDisplayData
    ) -> VerificationDisplayData:
        """æ ¼å¼åŒ–ä¸ºæŠ€æœ¯æ ¼å¼"""
        # å¼ºè°ƒæŠ€æœ¯æŒ‡æ ‡å’Œæ•°æ®
        display_data.summary_overview.pop("verification_time", None)
        display_data.risk_assessment.pop("mitigation_strategies", None)
        return display_data

    def _format_for_quick_overview(
        self, display_data: VerificationDisplayData
    ) -> VerificationDisplayData:
        """æ ¼å¼åŒ–ä¸ºå¿«é€Ÿæ¦‚è§ˆ"""
        # åªä¿ç•™æœ€å…³é”®çš„ä¿¡æ¯
        quick_data = VerificationDisplayData(
            display_id=display_data.display_id,
            session_id=display_data.session_id,
            suggestion_id=display_data.suggestion_id,
            file_path=display_data.file_path,
            summary_overview=display_data.summary_overview,
            fix_effectiveness={
                k: v
                for k, v in display_data.fix_effectiveness.items()
                if k in ["fix_success_rate", "problem_resolution_status"]
            },
            quality_impact={},
            new_issues_analysis={
                "total_new_issues": display_data.new_issues_analysis["total_new_issues"]
            },
            risk_assessment={
                "overall_risk_level": display_data.risk_assessment["overall_risk_level"]
            },
            recommendations=display_data.recommendations[:3],
            detailed_metrics={},
        )
        return quick_data

    def _format_for_comparison(
        self, display_data: VerificationDisplayData
    ) -> VerificationDisplayData:
        """æ ¼å¼åŒ–ä¸ºå¯¹æ¯”æ ¼å¼"""
        # å¼ºè°ƒä¿®å¤å‰åçš„å¯¹æ¯”
        display_data.fix_effectiveness["before_after_comparison"] = {
            "before_issues": len(
                display_data.detailed_metrics.get("static_analysis_metrics", {}).get(
                    "original_issues_count", []
                )
            ),
            "after_issues": display_data.detailed_metrics.get(
                "static_analysis_metrics", {}
            ).get("remaining_issues_count", 0),
            "improvement_percentage": "è®¡ç®—æ”¹è¿›ç™¾åˆ†æ¯”",
        }
        return display_data

    # è¾…åŠ©æ–¹æ³•
    def _format_status_with_icon(self, status: str) -> str:
        """æ ¼å¼åŒ–çŠ¶æ€å¹¶æ·»åŠ å›¾æ ‡"""
        icons = {
            "SUCCESS": "âœ… æˆåŠŸ",
            "PARTIAL_SUCCESS": "âš ï¸ éƒ¨åˆ†æˆåŠŸ",
            "FAILED": "âŒ å¤±è´¥",
            "REGRESSED": "ğŸ“‰ å›é€€",
            "UNCERTAIN": "â“ ä¸ç¡®å®š",
        }
        return icons.get(status, f"â“ {status}")

    def _format_boolean_with_icon(self, value: bool, invert: bool = False) -> str:
        """æ ¼å¼åŒ–å¸ƒå°”å€¼å¹¶æ·»åŠ å›¾æ ‡"""
        actual_value = not value if invert else value
        return "âœ… æ˜¯" if actual_value else "âŒ å¦"

    def _format_recommended_action(self, action: str) -> str:
        """æ ¼å¼åŒ–æ¨èè¡ŒåŠ¨"""
        actions = {
            "ACCEPT_FIX": "âœ… æ¥å—ä¿®å¤",
            "REJECT_FIX": "âŒ æ‹’ç»ä¿®å¤",
            "IMPROVE_FIX": "ğŸ”§ æ”¹è¿›ä¿®å¤",
            "MANUAL_REVIEW": "ğŸ‘ï¸ äººå·¥å®¡æŸ¥",
            "RETRY_ANALYSIS": "ğŸ”„ é‡æ–°åˆ†æ",
        }
        return actions.get(action, f"â“ {action}")

    def _translate_resolution_status(self, status: str) -> str:
        """ç¿»è¯‘é—®é¢˜è§£å†³çŠ¶æ€"""
        translations = {
            "fully_resolved": "å®Œå…¨è§£å†³",
            "partially_resolved": "éƒ¨åˆ†è§£å†³",
            "not_resolved": "æœªè§£å†³",
            "regressed": "å‡ºç°å›é€€",
        }
        return translations.get(status, status)

    def _get_score_level(self, score: float) -> str:
        """è·å–åˆ†æ•°ç­‰çº§"""
        if score >= 0.8:
            return "excellent"
        elif score >= 0.6:
            return "good"
        elif score >= 0.4:
            return "fair"
        else:
            return "poor"

    def _get_impact_icon(self, impact: str) -> str:
        """è·å–å½±å“å›¾æ ‡"""
        icons = {
            "improved": "ğŸ“ˆ",
            "unchanged": "â¡ï¸",
            "degraded": "ğŸ“‰",
            "positive": "âœ…",
            "negative": "âŒ",
            "minimal": "ğŸ”",
            "moderate": "âš ï¸",
            "significant": "ğŸ”¥",
        }
        return icons.get(impact, "â“")

    def _get_aspect_description(self, aspect: str) -> str:
        """è·å–æ–¹é¢æè¿°"""
        descriptions = {
            "readability": "å¯è¯»æ€§",
            "maintainability": "å¯ç»´æŠ¤æ€§",
            "complexity": "å¤æ‚åº¦",
            "documentation": "æ–‡æ¡£",
            "security": "å®‰å…¨æ€§",
            "performance": "æ€§èƒ½",
        }
        return descriptions.get(aspect, aspect)

    def _calculate_quality_change(self, score: float) -> str:
        """è®¡ç®—è´¨é‡å˜åŒ–"""
        if score > 0.7:
            return "æ˜¾è‘—æ”¹è¿›"
        elif score > 0.5:
            return "æœ‰æ‰€æ”¹è¿›"
        elif score > 0.3:
            return "è½»å¾®æ”¹è¿›"
        else:
            return "æ— æ˜æ˜¾æ”¹è¿›"

    def _analyze_issue_severity(self, issues: List[Dict[str, Any]]) -> Dict[str, int]:
        """åˆ†æé—®é¢˜ä¸¥é‡ç¨‹åº¦åˆ†å¸ƒ"""
        severity_count = {"error": 0, "warning": 0, "info": 0, "unknown": 0}
        for issue in issues:
            severity = issue.get("severity", "unknown")
            if severity in severity_count:
                severity_count[severity] += 1
            else:
                severity_count["unknown"] += 1
        return severity_count

    def _get_new_issues_recommendation(self, count: int) -> str:
        """è·å–æ–°é—®é¢˜å»ºè®®"""
        if count == 0:
            return "âœ… å¾ˆå¥½ï¼æ²¡æœ‰å¼•å…¥æ–°é—®é¢˜"
        elif count <= 2:
            return "âš ï¸ å¼•å…¥äº†å°‘é‡æ–°é—®é¢˜ï¼Œå»ºè®®å…³æ³¨"
        elif count <= 5:
            return "ğŸ” å¼•å…¥äº†ä¸€äº›æ–°é—®é¢˜ï¼Œå»ºè®®å®¡æŸ¥"
        else:
            return "âŒ å¼•å…¥äº†è¾ƒå¤šæ–°é—®é¢˜ï¼Œå»ºè®®é‡æ–°è¯„ä¼°ä¿®å¤æ–¹æ¡ˆ"

    def _get_risk_icon(self, risk_level: str) -> str:
        """è·å–é£é™©å›¾æ ‡"""
        icons = {"low": "ğŸŸ¢", "medium": "ğŸŸ¡", "high": "ğŸ”´", "unknown": "âšª"}
        return icons.get(risk_level, "âšª")

    def _get_risk_color(self, risk_level: str) -> str:
        """è·å–é£é™©é¢œè‰²"""
        colors = {
            "low": "#28a745",
            "medium": "#ffc107",
            "high": "#dc3545",
            "unknown": "#6c757d",
        }
        return colors.get(risk_level, "#6c757d")

    def _calculate_risk_score(self, risk_assessment: Dict[str, Any]) -> float:
        """è®¡ç®—é£é™©åˆ†æ•°"""
        risk_levels = {"low": 0.2, "medium": 0.5, "high": 0.8, "unknown": 0.5}
        base_score = risk_levels.get(risk_assessment["overall_risk_level"], 0.5)

        # æ ¹æ®é£é™©å› ç´ æ•°é‡è°ƒæ•´åˆ†æ•°
        factor_count = len(risk_assessment.get("risk_factors", []))
        adjusted_score = min(1.0, base_score + (factor_count * 0.1))

        return adjusted_score

    def generate_display_html(self, display_data: VerificationDisplayData) -> str:
        """
        ç”ŸæˆHTMLæ ¼å¼çš„å±•ç¤ºå†…å®¹

        Args:
            display_data: å±•ç¤ºæ•°æ®

        Returns:
            str: HTMLæ ¼å¼çš„å±•ç¤ºå†…å®¹
        """
        try:
            html_template = """
<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title>ä¿®å¤éªŒè¯ç»“æœ</title>
    <style>
        body { font-family: Arial, sans-serif; margin: 20px; }
        .header { background-color: #f8f9fa; padding: 20px; border-radius: 5px; }
        .section { margin: 20px 0; padding: 15px; border: 1px solid #dee2e6; border-radius: 5px; }
        .metric { display: inline-block; margin: 10px; padding: 10px; background-color: #e9ecef; border-radius: 3px; }
        .success { color: #28a745; }
        .warning { color: #ffc107; }
        .danger { color: #dc3545; }
        .recommendation { background-color: #d1ecf1; padding: 10px; margin: 5px 0; border-radius: 3px; }
    </style>
</head>
<body>
    <div class="header">
        <h1>ä¿®å¤éªŒè¯ç»“æœæŠ¥å‘Š</h1>
        <p><strong>æ–‡ä»¶:</strong> {file_path}</p>
        <p><strong>éªŒè¯æ—¶é—´:</strong> {verification_time}</p>
    </div>

    <div class="section">
        <h2>æ¦‚è§ˆ</h2>
        {summary_overview}
    </div>

    <div class="section">
        <h2>ä¿®å¤æ•ˆæœ</h2>
        {fix_effectiveness}
    </div>

    <div class="section">
        <h2>è´¨é‡å½±å“</h2>
        {quality_impact}
    </div>

    <div class="section">
        <h2>æ–°é—®é¢˜åˆ†æ</h2>
        {new_issues_analysis}
    </div>

    <div class="section">
        <h2>é£é™©è¯„ä¼°</h2>
        {risk_assessment}
    </div>

    <div class="section">
        <h2>æ”¹è¿›å»ºè®®</h2>
        {recommendations}
    </div>
</body>
</html>
            """

            # å¡«å……æ¨¡æ¿æ•°æ®
            html_content = html_template.format(
                file_path=display_data.file_path,
                verification_time=display_data.summary_overview.get(
                    "verification_time", "æœªçŸ¥"
                ),
                summary_overview=self._format_dict_to_html(
                    display_data.summary_overview
                ),
                fix_effectiveness=self._format_dict_to_html(
                    display_data.fix_effectiveness
                ),
                quality_impact=self._format_dict_to_html(display_data.quality_impact),
                new_issues_analysis=self._format_dict_to_html(
                    display_data.new_issues_analysis
                ),
                risk_assessment=self._format_dict_to_html(display_data.risk_assessment),
                recommendations=self._format_list_to_html(display_data.recommendations),
            )

            return html_content

        except Exception as e:
            self.logger.error(f"ç”ŸæˆHTMLå±•ç¤ºå¤±è´¥: {e}")
            return f"<html><body><h1>ç”Ÿæˆå±•ç¤ºå†…å®¹å¤±è´¥: {e}</h1></body></html>"

    def _format_dict_to_html(self, data: Dict[str, Any]) -> str:
        """å°†å­—å…¸è½¬æ¢ä¸ºHTMLæ ¼å¼"""
        html_parts = []
        for key, value in data.items():
            if isinstance(value, dict):
                html_parts.append(f"<p><strong>{key}:</strong></p>")
                html_parts.append(f"<ul>{self._format_dict_items(value)}</ul>")
            else:
                html_parts.append(f"<p><strong>{key}:</strong> {value}</p>")
        return "".join(html_parts)

    def _format_dict_items(self, data: Dict[str, Any]) -> str:
        """æ ¼å¼åŒ–å­—å…¸é¡¹ç›®ä¸ºHTMLåˆ—è¡¨"""
        items = []
        for key, value in data.items():
            items.append(f"<li><strong>{key}:</strong> {value}</li>")
        return "".join(items)

    def _format_list_to_html(self, data: List[str]) -> str:
        """å°†åˆ—è¡¨è½¬æ¢ä¸ºHTMLæ ¼å¼"""
        items = [f"<li>{item}</li>" for item in data]
        return f"<ul>{''.join(items)}</ul>" if items else "<p>æ— å»ºè®®</p>"
