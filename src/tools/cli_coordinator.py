#!/usr/bin/env python3
"""
CLIåˆ†æåè°ƒå™¨
ä¸ºCLIå‘½ä»¤è¡Œæ¥å£æä¾›ç®€åŒ–çš„åˆ†æåè°ƒåŠŸèƒ½
"""

import json
import sys
from pathlib import Path
from typing import List, Dict, Any, Optional, Union
from dataclasses import dataclass

from ..utils.logger import get_logger
from ..utils.progress import ProgressTracker
from .static_coordinator import StaticAnalysisCoordinator
from .deep_analyzer import DeepAnalysisRequest


class ConversationContext:
    """å¯¹è¯ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""

    def __init__(self, target: str, max_context_length: int = 10):
        """
        åˆå§‹åŒ–å¯¹è¯ä¸Šä¸‹æ–‡

        Args:
            target: åˆ†æç›®æ ‡
            max_context_length: æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦ï¼ˆè½®å¯¹è¯ï¼‰
        """
        self.target = target
        self.max_context_length = max_context_length
        self.session_start = self._get_current_time()
        self.conversation_history = []
        self.analysis_context = {
            'target': target,
            'analysis_type': 'comprehensive',
            'current_file': None,
            'previous_results': [],
            'preferences': {},
            'user_patterns': {},
            'session_stats': {
                'total_analyses': 0,
                'successful_analyses': 0,
                'failed_analyses': 0,
                'total_time': 0.0
            }
        }

    def add_message(self, user_input: str, ai_response: str, message_type: str = 'general'):
        """æ·»åŠ å¯¹è¯æ¶ˆæ¯"""
        message = {
            'timestamp': self._get_current_time(),
            'type': message_type,
            'user_input': user_input,
            'ai_response': ai_response,
            'session_time': self._get_session_duration()
        }

        self.conversation_history.append(message)
        self._trim_context()

    def add_analysis_result(self, file_path: str, analysis_type: str, result: dict, execution_time: float):
        """æ·»åŠ åˆ†æç»“æœ"""
        analysis_entry = {
            'timestamp': self._get_current_time(),
            'type': 'file_analysis',
            'file_path': file_path,
            'analysis_type': analysis_type,
            'result': result,
            'execution_time': execution_time,
            'success': result.get('success', False),
            'session_time': self._get_session_duration()
        }

        self.conversation_history.append(analysis_entry)
        self.analysis_context['previous_results'].append(analysis_entry)
        self.analysis_context['current_file'] = file_path

        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        self.analysis_context['session_stats']['total_analyses'] += 1
        if result.get('success', False):
            self.analysis_context['session_stats']['successful_analyses'] += 1
        else:
            self.analysis_context['session_stats']['failed_analyses'] += 1
        self.analysis_context['session_stats']['total_time'] += execution_time

        self._trim_context()

    def set_analysis_type(self, analysis_type: str):
        """è®¾ç½®åˆ†æç±»å‹"""
        self.analysis_context['analysis_type'] = analysis_type
        # è®°å½•ç”¨æˆ·åå¥½
        if 'analysis_type' not in self.analysis_context['user_patterns']:
            self.analysis_context['user_patterns']['analysis_type'] = {}
        if analysis_type not in self.analysis_context['user_patterns']['analysis_type']:
            self.analysis_context['user_patterns']['analysis_type'][analysis_type] = 0
        self.analysis_context['user_patterns']['analysis_type'][analysis_type] += 1

    def set_preference(self, key: str, value: any):
        """è®¾ç½®ç”¨æˆ·åå¥½"""
        self.analysis_context['preferences'][key] = value

    def get_context_summary(self) -> str:
        """è·å–ä¸Šä¸‹æ–‡æ‘˜è¦"""
        history = self.conversation_history
        if not history:
            return "è¿™æ˜¯æˆ‘ä»¬çš„ç¬¬ä¸€æ¬¡å¯¹è¯"

        recent_analyses = [entry for entry in history[-5:] if entry.get('type') == 'file_analysis']
        if recent_analyses:
            last_file = recent_analyses[-1].get('file_path', 'unknown')
            last_type = recent_analyses[-1].get('analysis_type', 'unknown')
            return f"æœ€è¿‘åˆ†æäº† {Path(last_file).name} (ç±»å‹: {last_type})"

        recent_messages = history[-3:]
        if recent_messages:
            return f"æˆ‘ä»¬æœ€è¿‘è®¨è®ºäº† {len(recent_messages)} ä¸ªè¯é¢˜"

        return f"æˆ‘ä»¬å·²ç»è¿›è¡Œäº† {len(history)} è½®å¯¹è¯"

    def get_recent_context(self, num_entries: int = 3) -> list:
        """è·å–æœ€è¿‘çš„ä¸Šä¸‹æ–‡"""
        return self.conversation_history[-num_entries:]

    def get_file_analysis_history(self, file_path: str = None) -> list:
        """è·å–æ–‡ä»¶åˆ†æå†å²"""
        analyses = [entry for entry in self.conversation_history if entry.get('type') == 'file_analysis']

        if file_path:
            analyses = [entry for entry in analyses if file_path in entry.get('file_path', '')]

        return analyses

    def get_session_stats(self) -> dict:
        """è·å–ä¼šè¯ç»Ÿè®¡"""
        stats = self.analysis_context['session_stats'].copy()
        stats.update({
            'session_duration': self._get_session_duration(),
            'total_messages': len(self.conversation_history),
            'files_analyzed': len(set(entry.get('file_path', '') for entry in self.conversation_history if entry.get('type') == 'file_analysis')),
            'most_used_analysis_type': self._get_most_used_analysis_type()
        })
        return stats

    def _trim_context(self):
        """ä¿®å‰ªä¸Šä¸‹æ–‡ä»¥ä¿æŒåœ¨æœ€å¤§é•¿åº¦å†…"""
        if len(self.conversation_history) > self.max_context_length:
            # ä¿ç•™é‡è¦çš„åˆ†æç»“æœ
            important_entries = []
            regular_entries = []

            for entry in self.conversation_history:
                if entry.get('type') == 'file_analysis' and entry.get('success', False):
                    important_entries.append(entry)
                else:
                    regular_entries.append(entry)

            # ä¼˜å…ˆä¿ç•™é‡è¦çš„åˆ†æç»“æœ
            self.conversation_history = important_entries + regular_entries[-(self.max_context_length - len(important_entries)):]

    def _get_most_used_analysis_type(self) -> str:
        """è·å–æœ€å¸¸ç”¨çš„åˆ†æç±»å‹"""
        patterns = self.analysis_context['user_patterns'].get('analysis_type', {})
        if patterns:
            return max(patterns.items(), key=lambda x: x[1])[0]
        return 'comprehensive'

    def _get_session_duration(self) -> float:
        """è·å–ä¼šè¯æŒç»­æ—¶é—´ï¼ˆç§’ï¼‰"""
        from datetime import datetime
        start_time = datetime.strptime(self.session_start, "%Y-%m-%d %H:%M:%S")
        current_time = datetime.now()
        return (current_time - start_time).total_seconds()

    def _get_current_time(self) -> str:
        """è·å–å½“å‰æ—¶é—´å­—ç¬¦ä¸²"""
        from datetime import datetime
        return datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    def to_dict(self) -> dict:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        return {
            'target': self.target,
            'session_start': self.session_start,
            'conversation_history': self.conversation_history,
            'analysis_context': self.analysis_context,
            'max_context_length': self.max_context_length
        }


@dataclass
class CLIStaticCoordinator:
    """CLIé™æ€åˆ†æåè°ƒå™¨"""

    def __init__(self, tools: Optional[List[str]] = None, format: str = "simple",
                 output_file: Optional[str] = None, dry_run: bool = False,
                 progress: Optional[ProgressTracker] = None):
        """
        åˆå§‹åŒ–CLIé™æ€åˆ†æåè°ƒå™¨

        Args:
            tools: æŒ‡å®šçš„å·¥å…·åˆ—è¡¨
            format: è¾“å‡ºæ ¼å¼
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            dry_run: æ˜¯å¦ä¸ºè¯•è¿è¡Œ
            progress: è¿›åº¦è·Ÿè¸ªå™¨
        """
        self.tools = tools or ['ast', 'pylint', 'flake8']
        self.format = format
        self.output_file = output_file
        self.dry_run = dry_run
        self.progress = progress or ProgressTracker()
        self.logger = get_logger()

        # åˆ›å»ºé™æ€åˆ†æåè°ƒå™¨
        self.coordinator = StaticAnalysisCoordinator()
        if tools:
            self.coordinator.set_enabled_tools(tools)

    def analyze(self, target: str) -> Dict[str, Any]:
        """
        æ‰§è¡Œé™æ€åˆ†æ

        Args:
            target: ç›®æ ‡è·¯å¾„

        Returns:
            åˆ†æç»“æœå­—å…¸
        """
        try:
            if self.dry_run:
                print(f"ğŸ” [è¯•è¿è¡Œ] å°†åˆ†æ: {target}")
                return {
                    'dry_run': True,
                    'target': target,
                    'tools': self.tools,
                    'format': self.format
                }

            target_path = Path(target)

            if not target_path.exists():
                print(f"âŒ é”™è¯¯: ç›®æ ‡è·¯å¾„ä¸å­˜åœ¨: {target}")
                return {'error': f"Target path does not exist: {target}"}

            # æ”¶é›†æ–‡ä»¶
            files = self._collect_files(target_path)
            if not files:
                print(f"âš ï¸  è­¦å‘Š: åœ¨ {target} ä¸­æœªæ‰¾åˆ°Pythonæ–‡ä»¶")
                return {
                    'target': target,
                    'files_analyzed': 0,
                    'total_issues': 0,
                    'files': []
                }

            # å¼€å§‹åˆ†æ
            self.progress.start(len(files))
            self.progress.update_file_count(0)

            all_results = []
            total_issues = 0

            for i, file_path in enumerate(files):
                self.progress.show_file_progress(file_path, i, len(files))

                try:
                    result = self.coordinator.analyze_file(file_path)
                    all_results.append(result)
                    total_issues += len(result.issues)
                    self.progress.update_issue_count(total_issues)

                    # æ˜¾ç¤ºå®æ—¶è¿›åº¦
                    if self.progress.verbose:
                        for issue in result.issues[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ªé—®é¢˜
                            print(f"  ğŸ“ {result.file_path}:{issue.line} - {issue.message}")

                except Exception as e:
                    self.logger.error(f"Failed to analyze {file_path}: {e}")
                    if self.progress.verbose:
                        print(f"  âŒ åˆ†æå¤±è´¥: {e}")

            self.progress.finish()
            self.progress.update_file_count(len(files))

            # ç”ŸæˆæŠ¥å‘Š
            report = self._generate_report(all_results, target)

            # ä¿å­˜ç»“æœ
            if self.output_file:
                self._save_report(report, self.output_file)

            return report

        except Exception as e:
            self.logger.error(f"Analysis failed: {e}")
            print(f"âŒ åˆ†æå¤±è´¥: {e}")
            return {'error': str(e)}

    def _collect_files(self, target_path: Path) -> List[str]:
        """æ”¶é›†Pythonæ–‡ä»¶"""
        files = []

        if target_path.is_file():
            if target_path.suffix == '.py':
                files.append(str(target_path))
        elif target_path.is_dir():
            # é€’å½’æ”¶é›†Pythonæ–‡ä»¶
            for py_file in target_path.rglob("*.py"):
                files.append(str(py_file))

        # æ’åºæ–‡ä»¶
        files.sort()
        return files

    def _generate_report(self, results: List[Any], target: str) -> Dict[str, Any]:
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        total_issues = sum(len(result.issues) for result in results)

        report = {
            'target': target,
            'files_analyzed': len(results),
            'total_issues': total_issues,
            'format': self.format,
            'tools_used': self.tools,
            'files': [],
            'summary': {
                'total_files': len(results),
                'total_issues': total_issues,
                'severity_distribution': {},
                'tool_distribution': {}
            },
            'execution_time': sum(getattr(result, 'execution_time', 0) for result in results)
        }

        # ç»Ÿè®¡ä¸¥é‡ç¨‹åº¦å’Œå·¥å…·åˆ†å¸ƒ
        for result in results:
            for issue in result.issues:
                severity = issue.severity.value
                tool = issue.tool_name

                if severity not in report['summary']['severity_distribution']:
                    report['summary']['severity_distribution'][severity] = 0
                if tool not in report['summary']['tool_distribution']:
                    report['summary']['tool_distribution'][tool] = 0

                report['summary']['severity_distribution'][severity] += 1
                report['summary']['tool_distribution'][tool] += 1

        # æ·»åŠ æ–‡ä»¶è¯¦æƒ…
        for result in results:
            file_info = {
                'file_path': result.file_path,
                'issues_count': len(result.issues),
                'execution_time': result.execution_time,
                'summary': result.summary
            }

            # æ ¹æ®è¾“å‡ºæ ¼å¼æ·»åŠ è¯¦ç»†ä¿¡æ¯
            if self.format in ['detailed', 'json']:
                file_info['issues'] = [
                    {
                        'tool': issue.tool_name,
                        'line': issue.line,
                        'column': issue.column,
                        'severity': issue.severity.value,
                        'message': issue.message,
                        'type': issue.issue_type,
                        'code': issue.code
                    }
                    for issue in result.issues
                ]

            report['files'].append(file_info)

        return report

    def _save_report(self, report: Dict[str, Any], output_file: str):
        """ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶"""
        try:
            output_path = Path(output_file)
            output_path.parent.mkdir(parents=True, exist_ok=True)

            if output_file.endswith('.json'):
                with open(output_path, 'w', encoding='utf-8') as f:
                    json.dump(report, f, indent=2, ensure_ascii=False)
            elif output_file.endswith('.md'):
                self._save_markdown_report(report, output_path)
            else:
                # ç®€å•æ–‡æœ¬æ ¼å¼
                with open(output_path, 'w', encoding='utf-8') as f:
                    self._save_text_report(report, f)

        except Exception as e:
            self.logger.error(f"Failed to save report: {e}")
            print(f"âŒ ä¿å­˜æŠ¥å‘Šå¤±è´¥: {e}")


    def _save_text_report(self, report: Dict[str, Any], file):
        """ä¿å­˜æ–‡æœ¬æ ¼å¼æŠ¥å‘Š"""
        file.write(f"é™æ€åˆ†ææŠ¥å‘Š\n")
        file.write("=" * 50 + "\n")
        file.write(f"ç›®æ ‡: {report['target']}\n")
        file.write(f"åˆ†ææ–‡ä»¶æ•°: {report['files_analyzed']}\n")
        file.write(f"å‘ç°é—®é¢˜æ•°: {report['total_issues']}\n")
        file.write(f"æ‰§è¡Œæ—¶é—´: {report['execution_time']:.2f}ç§’\n")
        file.write(f"ä½¿ç”¨å·¥å…·: {', '.join(report['tools_used'])}\n\n")

        if report['files']:
            file.write("æ–‡ä»¶è¯¦æƒ…:\n")
            file.write("-" * 30 + "\n")
            for file_info in report['files']:
                file.write(f"æ–‡ä»¶: {file_info['file_path']}\n")
                file.write(f"é—®é¢˜æ•°: {file_info['issues_count']}\n")
                file.write(f"æ‰§è¡Œæ—¶é—´: {file_info['execution_time']:.2f}ç§’\n")
                file.write("-" * 30 + "\n")

    def _save_markdown_report(self, report: Dict[str, Any], file):
        """ä¿å­˜Markdownæ ¼å¼æŠ¥å‘Š"""
        file.write("# é™æ€åˆ†ææŠ¥å‘Š\n\n")
        file.write(f"**ç›®æ ‡è·¯å¾„**: `{report['target']}`\n")
        file.write(f"**åˆ†ææ–‡ä»¶æ•°**: {report['files_analyzed']}\n")
        file.write(f"**å‘ç°é—®é¢˜æ•°**: {report['total_issues']}\n")
        file.write(f"**æ‰§è¡Œæ—¶é—´**: {report['execution_time']:.2f}ç§’\n")
        file.write(f"**ä½¿ç”¨å·¥å…·**: {', '.join(report['tools_used'])}\n\n")

        # ä¸¥é‡ç¨‹åº¦åˆ†å¸ƒ
        if report['summary']['severity_distribution']:
            file.write("## é—®é¢˜ä¸¥é‡ç¨‹åº¦åˆ†å¸ƒ\n\n")
            file.write("| ä¸¥é‡ç¨‹åº¦ | æ•°é‡ |\n")
            file.write("|----------|------|\n")
            for severity, count in report['summary']['severity_distribution'].items():
                file.write(f"| {severity} | {count} |\n")
            file.write("\n")

        # å·¥å…·åˆ†å¸ƒ
        if report['summary']['tool_distribution']:
            file.write("## å·¥å…·å‘ç°é—®é¢˜åˆ†å¸ƒ\n\n")
            file.write("| å·¥å…· | å‘ç°é—®é¢˜æ•° |\n")
            file.write("|------|------------|\n")
            for tool, count in report['summary']['tool_distribution'].items():
                file.write(f"| {tool} | {count} |\n")
            file.write("\n")

        # æ–‡ä»¶è¯¦æƒ…
        if report['files']:
            file.write("## æ–‡ä»¶åˆ†æè¯¦æƒ…\n\n")
            for file_info in report['files']:
                file.write(f"### {file_info['file_path']}\n\n")
                file.write(f"- **é—®é¢˜æ•°**: {file_info['issues_count']}\n")
                file.write(f"- **æ‰§è¡Œæ—¶é—´**: {file_info['execution_time']:.2f}ç§’\n")

                if 'issues' in file_info and file_info['issues']:
                    file.write("- **é—®é¢˜åˆ—è¡¨**:\n")
                    for issue in file_info['issues'][:10]:  # åªæ˜¾ç¤ºå‰10ä¸ªé—®é¢˜
                        file.write(f"  - **{issue['tool']}**: [{issue['severity']}] {issue['message']}\n")
                        file.write(f"    - ä½ç½®: ç¬¬{issue['line']}è¡Œ\n")
                    if len(file_info['issues']) > 10:
                        file.write(f"  - ... è¿˜æœ‰ {len(file_info['issues']) - 10} ä¸ªé—®é¢˜\n")

                file.write("\n")


class CLIInteractiveCoordinator:
    """CLIäº¤äº’å¼åˆ†æåè°ƒå™¨"""

    def __init__(self, mode: str = 'deep', output_file: Optional[str] = None,
                 progress: Optional[ProgressTracker] = None, max_context_length: int = 15,
                 enable_performance_monitoring: bool = True, enable_caching: bool = True):
        """
        åˆå§‹åŒ–CLIäº¤äº’å¼åè°ƒå™¨

        Args:
            mode: åˆ†ææ¨¡å¼ (deep/fix)
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            progress: è¿›åº¦è·Ÿè¸ªå™¨
            max_context_length: æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦
            enable_performance_monitoring: æ˜¯å¦å¯ç”¨æ€§èƒ½ç›‘æ§
            enable_caching: æ˜¯å¦å¯ç”¨ç¼“å­˜åŠŸèƒ½
        """
        self.mode = mode
        self.output_file = output_file
        self.progress = progress or ProgressTracker(verbose=True)
        self.logger = get_logger()
        self.max_context_length = max_context_length
        self.conversation_context = None

        # æ€§èƒ½ç›‘æ§
        self.enable_performance_monitoring = enable_performance_monitoring
        self.enable_caching = enable_caching

        # æ€§èƒ½ç»Ÿè®¡
        self.performance_stats = {
            'total_analysis_time': 0.0,
            'total_cache_hits': 0,
            'total_cache_misses': 0,
            'analysis_count': 0,
            'avg_response_time': 0.0,
            'slow_requests': []
        }

        # ç¼“å­˜ç³»ç»Ÿ
        self.analysis_cache = {}
        self.cache_ttl = 300  # 5åˆ†é’Ÿç¼“å­˜æ—¶é—´

        # æ€§èƒ½ä¼˜åŒ–é…ç½®
        self.optimization_config = {
            'context_length_limit': 8000,  # ä¸Šä¸‹æ–‡é•¿åº¦é™åˆ¶ï¼ˆå­—ç¬¦æ•°ï¼‰
            'enable_batch_processing': True,
            'parallel_file_analysis': False,  # æš‚æ—¶ç¦ç”¨å¹¶è¡Œåˆ†æä»¥é¿å…å¤æ‚æ€§
            'smart_context_trimming': True,
            'response_timeout': 60  # å“åº”è¶…æ—¶ï¼ˆç§’ï¼‰
        }

        # æ·±åº¦åˆ†æé…ç½®é€‰é¡¹
        self.analysis_config = {
            'model_selection': 'auto',  # æ¨¡å‹é€‰æ‹©: auto, glm-4.5, glm-4.6, gpt-4, claude-3
            'analysis_depth': 'standard',  # åˆ†ææ·±åº¦: basic, standard, detailed, comprehensive
            'custom_prompt_template': None,  # è‡ªå®šä¹‰æç¤ºè¯æ¨¡æ¿
            'temperature': 0.3,  # AIå“åº”åˆ›é€ æ€§ (0.0-1.0)
            'max_tokens': 4000,  # æœ€å¤§ç”Ÿæˆtokenæ•°
            'enable_structured_output': True,  # å¯ç”¨ç»“æ„åŒ–è¾“å‡º
            'focus_areas': [],  # é‡ç‚¹å…³æ³¨é¢†åŸŸ: ['security', 'performance', 'architecture', 'code_quality']
            'exclude_patterns': [],  # æ’é™¤æ–‡ä»¶æ¨¡å¼
            'include_patterns': [],  # åŒ…å«æ–‡ä»¶æ¨¡å¼
            'language_style': 'professional',  # è¯­è¨€é£æ ¼: casual, professional, technical
            'output_format': 'comprehensive'  # è¾“å‡ºæ ¼å¼: concise, standard, comprehensive
        }

        # é”™è¯¯å¤„ç†å’Œæ¢å¤é…ç½®
        self.error_handling_config = {
            'max_retry_attempts': 3,  # æœ€å¤§é‡è¯•æ¬¡æ•°
            'retry_delay': 2,  # é‡è¯•å»¶è¿Ÿï¼ˆç§’ï¼‰
            'enable_fallback_mode': True,  # å¯ç”¨é™çº§æ¨¡å¼
            'offline_mode_available': True,  # ç¦»çº¿æ¨¡å¼å¯ç”¨
            'error_recovery_strategies': {
                'network_error': 'retry_with_backoff',  # ç½‘ç»œé”™è¯¯é‡è¯•ç­–ç•¥
                'api_error': 'fallback_model',  # APIé”™è¯¯é™çº§ç­–ç•¥
                'timeout_error': 'increase_timeout',  # è¶…æ—¶é”™è¯¯å¤„ç†ç­–ç•¥
                'rate_limit_error': 'exponential_backoff'  # é™æµé”™è¯¯å¤„ç†ç­–ç•¥
            }
        }

        # é”™è¯¯ç»Ÿè®¡å’Œæ—¥å¿—
        self.error_stats = {
            'total_errors': 0,
            'network_errors': 0,
            'api_errors': 0,
            'timeout_errors': 0,
            'file_errors': 0,
            'successful_recoveries': 0,
            'failed_recoveries': 0,
            'recent_errors': []  # ä¿å­˜æœ€è¿‘10ä¸ªé”™è¯¯
        }

        # é™æ€åˆ†æé›†æˆé…ç½®
        self.static_analysis_integration = {
            'auto_load_reports': True,  # è‡ªåŠ¨åŠ è½½é™æ€åˆ†ææŠ¥å‘Š
            'report_search_paths': ['.', 'static_analysis_report_*.json'],  # æŠ¥å‘Šæœç´¢è·¯å¾„
            'max_report_age_days': 7,  # æœ€å¤§æŠ¥å‘Šå¹´é¾„ï¼ˆå¤©ï¼‰
            'priority_threshold': 5,  # ä¼˜å…ˆçº§é˜ˆå€¼ï¼ˆä½äºæ­¤å€¼çš„é—®é¢˜ä¼˜å…ˆå¤„ç†ï¼‰
            'report_cache': {},  # é™æ€åˆ†ææŠ¥å‘Šç¼“å­˜
            'integrated_reports': []  # å·²é›†æˆçš„æŠ¥å‘Šåˆ—è¡¨
        }

        # é«˜çº§æ·±åº¦åˆ†æç¼“å­˜æœºåˆ¶ (T026-010)
        self.advanced_cache_config = {
            'enable_persistent_cache': True,  # å¯ç”¨æŒä¹…åŒ–ç¼“å­˜
            'cache_file_path': '.aidefect_deep_analysis_cache.json',  # ç¼“å­˜æ–‡ä»¶è·¯å¾„
            'max_cache_size_mb': 50,  # æœ€å¤§ç¼“å­˜å¤§å°ï¼ˆMBï¼‰
            'smart_cache_key_generation': True,  # æ™ºèƒ½ç¼“å­˜é”®ç”Ÿæˆ
            'cache_validation_enabled': True,  # å¯ç”¨ç¼“å­˜éªŒè¯
            'cache_compression': True,  # å¯ç”¨ç¼“å­˜å‹ç¼©
            'semantic_cache_enabled': True,  # å¯ç”¨è¯­ä¹‰ç¼“å­˜
            'cache_hierarchy': {
                'L1_memory': {'size_limit': 20, 'ttl': 300},      # å†…å­˜ç¼“å­˜: 20é¡¹, 5åˆ†é’Ÿ
                'L2_disk': {'size_limit': 100, 'ttl': 86400},     # ç£ç›˜ç¼“å­˜: 100é¡¹, 24å°æ—¶
                'L3_semantic': {'size_limit': 50, 'ttl': 3600}    # è¯­ä¹‰ç¼“å­˜: 50é¡¹, 1å°æ—¶
            }
        }

        # æ™ºèƒ½ç¼“å­˜ç»Ÿè®¡
        self.cache_stats = {
            'L1_memory_hits': 0,
            'L1_memory_misses': 0,
            'L2_disk_hits': 0,
            'L2_disk_misses': 0,
            'L3_semantic_hits': 0,
            'L3_semantic_misses': 0,
            'cache_evictions': 0,
            'cache_compressions': 0,
            'semantic_matches': 0,
            'total_cache_writes': 0,
            'cache_size_bytes': 0,
            'average_cache retrieval_time': 0.0
        }

        # å¸¸è§é—®é¢˜å’Œç­”æ¡ˆç¼“å­˜ï¼ˆæ™ºèƒ½é—®ç­”ç¼“å­˜ï¼‰
        self.common_qa_cache = {
            'common_issues': [
                "é«˜å¤æ‚åº¦å‡½æ•°",
                "ä»£ç é‡å¤",
                "å®‰å…¨æ¼æ´",
                "æ€§èƒ½é—®é¢˜",
                "ä»£ç é£æ ¼",
                "æ¶æ„é—®é¢˜",
                "é”™è¯¯å¤„ç†",
                "å†…å­˜æ³„æ¼"
            ],
            'qa_pairs': {}  # ç¼“å­˜å¸¸è§é—®ç­”å¯¹
        }

        # è¯­ä¹‰ç›¸ä¼¼åº¦ç¼“å­˜
        self.semantic_cache = {
            'enabled': True,
            'similarity_threshold': 0.85,  # ç›¸ä¼¼åº¦é˜ˆå€¼
            'max_text_length': 1000,  # æœ€å¤§æ–‡æœ¬é•¿åº¦
            'cache_entries': {}  # è¯­ä¹‰ç¼“å­˜æ¡ç›®
        }

        # ç¼“å­˜å¤±æ•ˆå’Œæ›´æ–°ç­–ç•¥
        self.cache_invalidation_config = {
            'auto_invalidate_on_file_change': True,  # æ–‡ä»¶æ›´æ”¹æ—¶è‡ªåŠ¨å¤±æ•ˆ
            'dependency_tracking': True,  # ä¾èµ–å…³ç³»è·Ÿè¸ª
            'cascade_invalidation': True,  # çº§è”å¤±æ•ˆ
            'smart_invalidation': True,  # æ™ºèƒ½å¤±æ•ˆç­–ç•¥
            'invalidation_triggers': {
                'file_modified': True,
                'dependency_changed': True,
                'config_updated': True,
                'manual_refresh': True
            }
        }

    def run_interactive(self, target: str) -> Dict[str, Any]:
        """è¿è¡Œäº¤äº’å¼åˆ†æ"""
        if self.mode == 'deep':
            return self._run_deep_analysis(target)
        elif self.mode == 'fix':
            return self._run_fix_analysis(target)
        else:
            raise ValueError(f"Unsupported interactive mode: {self.mode}")

    def _run_deep_analysis(self, target: str) -> Dict[str, Any]:
        """è¿è¡Œæ·±åº¦åˆ†æ"""
        from .deep_analyzer import DeepAnalyzer, DeepAnalysisRequest
        import asyncio
        import time
        from threading import Thread
        import sys

        # å¢å¼ºçš„å¯åŠ¨ç•Œé¢
        self._show_enhanced_startup_banner(target)

        try:
            # æ˜¾ç¤ºåˆå§‹åŒ–è¿›åº¦
            self._show_initialization_progress()

            # åˆå§‹åŒ–å¯¹è¯ä¸Šä¸‹æ–‡ç®¡ç†å™¨
            self.conversation_context = ConversationContext(target, self.max_context_length)

            # åŠ è½½é™æ€åˆ†ææŠ¥å‘Š
            static_reports = self._load_static_analysis_reports(target)
            if static_reports:
                print(f"ğŸ“‚ å‘ç° {len(static_reports)} ä¸ªé™æ€åˆ†ææŠ¥å‘Š")
                print(f"ğŸ“Š æœ€è¿‘æŠ¥å‘Š: {static_reports[0].get('age_days', 0)} å¤©å‰")
                print()

            analyzer = DeepAnalyzer()

            # æ˜¾ç¤ºä¼šè¯ä¿¡æ¯
            self._show_enhanced_session_info(self.conversation_context, analyzer)

            # æ˜¾ç¤ºé™æ€åˆ†æé›†æˆçŠ¶æ€
            if static_reports:
                print(f"ğŸ”— é™æ€åˆ†æé›†æˆ: å·²å¯ç”¨")
                print(f"ğŸ“‹ å¯ç”¨æŠ¥å‘Šæ–‡ä»¶: {len([r for r in static_reports if any(f.get('file_path', '').endswith(Path(f).name) for f in r.get('files', []))])}")
                print(f"ğŸ’¡ AIå°†åŸºäºé™æ€åˆ†æç»“æœæä¾›æ·±åº¦å»ºè®®")
                print()

            print()

            # äº¤äº’å¼å¯¹è¯å¾ªç¯
            while True:
                try:
                    user_input = input("ğŸ¤– AIåˆ†æåŠ©æ‰‹> ").strip()

                    if not user_input:
                        continue

                    # å¤„ç†é€€å‡ºå‘½ä»¤
                    if user_input.lower() in ['quit', 'exit', 'q']:
                        print("ğŸ‘‹ é€€å‡ºæ·±åº¦åˆ†ææ¨¡å¼")
                        break

                    # å¤„ç†å¸®åŠ©å‘½ä»¤
                    elif user_input.lower() == 'help':
                        self._show_deep_analysis_help()
                        continue

                    # å¤„ç†ç±»å‹è®¾ç½®å‘½ä»¤
                    elif user_input.lower().startswith('type '):
                        new_type = user_input[5:].strip()
                        try:
                            supported_types = analyzer.get_supported_analysis_types()
                        except:
                            supported_types = ['comprehensive', 'security', 'performance', 'architecture', 'code_review', 'refactoring']

                        if new_type in supported_types:
                            self.conversation_context.set_analysis_type(new_type)
                            print(f"âœ… åˆ†æç±»å‹å·²è®¾ç½®ä¸º: {new_type}")
                            print(f"ğŸ’¾ å·²è®°å½•æ‚¨çš„åå¥½è®¾ç½®")
                        else:
                            print(f"âŒ ä¸æ”¯æŒçš„åˆ†æç±»å‹: {new_type}")
                            print(f"æ”¯æŒçš„ç±»å‹: {', '.join(supported_types)}")
                        continue

                    # å¤„ç†åˆ†æå‘½ä»¤
                    elif user_input.lower().startswith('analyze '):
                        file_path = user_input[8:].strip()
                        result = self._analyze_file_interactive_with_context(analyzer, file_path)
                        continue

                    # å¤„ç†æ€»ç»“å‘½ä»¤
                    elif user_input.lower() == 'summary':
                        session_stats = self.conversation_context.get_session_stats()
                        self._show_analysis_summary(self.conversation_context.analysis_context)
                        continue

                    # å¤„ç†æ€§èƒ½ç»Ÿè®¡å‘½ä»¤
                    elif user_input.lower() in ['stats', 'performance', 'æ€§èƒ½']:
                        self._show_performance_stats()
                        continue

                    # å¤„ç†é”™è¯¯ç»Ÿè®¡å‘½ä»¤
                    elif user_input.lower() in ['errors', 'error', 'é”™è¯¯', 'error_stats']:
                        self._show_error_statistics()
                        continue

                    # å¤„ç†é…ç½®å‘½ä»¤
                    elif user_input.lower().startswith('config '):
                        config_parts = user_input[7:].strip().split(' ', 1)
                        if len(config_parts) == 2:
                            config_key, config_value = config_parts
                            self._configure_analysis_settings(config_key, config_value)
                        elif len(config_parts) == 1:
                            if config_parts[0].lower() in ['show', 'current', 'list']:
                                self._show_current_config()
                            elif config_parts[0].lower() in ['help', 'options', 'available']:
                                self._show_available_configs()
                            elif config_parts[0].lower() in ['reset', 'default', 'defaults']:
                                self._reset_config_to_defaults()
                            else:
                                print("âŒ é…ç½®å‘½ä»¤æ ¼å¼é”™è¯¯ï¼Œä½¿ç”¨ 'config help' æŸ¥çœ‹å¸®åŠ©")
                        else:
                            print("âŒ é…ç½®å‘½ä»¤æ ¼å¼é”™è¯¯ï¼Œä½¿ç”¨ 'config help' æŸ¥çœ‹å¸®åŠ©")
                        continue

                    # å¤„ç†å¯¼å‡ºå‘½ä»¤
                    elif user_input.lower().startswith('export '):
                        export_file = user_input[7:].strip()
                        self._export_conversation_with_context(export_file)
                        continue

                    # å¤„ç†æ™®é€šå¯¹è¯è¾“å…¥
                    else:
                        print(f"ğŸ’­ æ­£åœ¨å¤„ç†æ‚¨çš„é—®é¢˜: {user_input}")
                        print("ğŸ” AIæ­£åœ¨æ€è€ƒ...")

                        # ä½¿ç”¨ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„å¯¹è¯å¤„ç†
                        response = self._generate_contextual_response(user_input)
                        print(f"ğŸ¤– AI: {response}")

                        # è®°å½•å¯¹è¯å†å²åˆ°ä¸Šä¸‹æ–‡ç®¡ç†å™¨
                        self.conversation_context.add_message(user_input, response, 'general')

                except KeyboardInterrupt:
                    print("\nğŸ‘‹ ä½¿ç”¨Ctrl+Cé€€å‡ºæ·±åº¦åˆ†ææ¨¡å¼")
                    break
                except EOFError:
                    print("\nğŸ‘‹ åˆ°è¾¾è¾“å…¥æœ«å°¾ï¼Œé€€å‡ºæ·±åº¦åˆ†ææ¨¡å¼")
                    break

            # ç”Ÿæˆæœ€ç»ˆç»“æœ
            result = {
                'mode': 'deep',
                'target': target,
                'status': 'completed',
                'conversation_history': self.conversation_context.conversation_history,
                'analysis_context': self.conversation_context.analysis_context,
                'files_analyzed': len(self.conversation_context.analysis_context['previous_results']),
                'total_execution_time': sum(r.get('execution_time', 0) for r in self.conversation_context.analysis_context['previous_results']),
                'performance_stats': self.performance_stats
            }

            # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
            if self.output_file:
                self._save_deep_analysis_result(result, self.output_file)

            print("âœ… æ·±åº¦åˆ†æå®Œæˆ")
            return result

        except Exception as e:
            self.logger.error(f"Deep analysis failed: {e}")
            print(f"âŒ æ·±åº¦åˆ†æå¤±è´¥: {e}")
            return {'error': str(e)}

    def _run_fix_analysis(self, target: str) -> Dict[str, Any]:
        """è¿è¡Œä¿®å¤åˆ†æ"""
        from .fix_coordinator import FixCoordinator, FixAnalysisRequest
        from .static_coordinator import StaticAnalysisCoordinator
        import asyncio

        print(f"ğŸ”§ å¼€å§‹ä¿®å¤åˆ†æ: {target}")
        print("=" * 60)
        print("ğŸ’¡ æç¤º: è¾“å…¥ 'quit' æˆ– 'exit' é€€å‡ºä¿®å¤æ¨¡å¼")
        print("ğŸ’¡ æç¤º: è¾“å…¥ 'help' æŸ¥çœ‹å¯ç”¨å‘½ä»¤")
        print("ğŸ’¡ æç¤º: è¾“å…¥ 'scan' æ‰«ææ–‡ä»¶é—®é¢˜")
        print("ğŸ’¡ æç¤º: è¾“å…¥ 'fix <file_path>' ä¿®å¤æŒ‡å®šæ–‡ä»¶")
        print("ğŸ’¡ æç¤º: è¾“å…¥ 'batch fix' æ‰¹é‡ä¿®å¤")
        print()

        try:
            # åˆå§‹åŒ–åè°ƒå™¨
            static_coordinator = StaticAnalysisCoordinator()
            fix_coordinator = FixCoordinator()

            # ä¿®å¤ä¼šè¯çŠ¶æ€
            fix_session = {
                'target': target,
                'scanned_files': [],
                'identified_issues': {},
                'fix_history': [],
                'current_file': None,
                'auto_confirm': False
            }

            print(f"ğŸ“ ç›®æ ‡è·¯å¾„: {target}")
            print(f"ğŸ”§ ä¿®å¤æ¨¡å¼: æ‰‹åŠ¨ç¡®è®¤ (å¯ä½¿ç”¨ 'auto confirm' åˆ‡æ¢)")
            print()

            # äº¤äº’å¼ä¿®å¤å¾ªç¯
            while True:
                try:
                    user_input = input("ğŸ”§ ä¿®å¤åŠ©æ‰‹> ").strip()

                    if not user_input:
                        continue

                    # å¤„ç†é€€å‡ºå‘½ä»¤
                    if user_input.lower() in ['quit', 'exit', 'q']:
                        print("ğŸ‘‹ é€€å‡ºä¿®å¤æ¨¡å¼")
                        break

                    # å¤„ç†å¸®åŠ©å‘½ä»¤
                    elif user_input.lower() == 'help':
                        self._show_fix_analysis_help()
                        continue

                    # å¤„ç†æ‰«æå‘½ä»¤
                    elif user_input.lower() == 'scan':
                        self._scan_files_for_issues(static_coordinator, target, fix_session)
                        continue

                    # å¤„ç†è‡ªåŠ¨ç¡®è®¤åˆ‡æ¢
                    elif user_input.lower() == 'auto confirm':
                        fix_session['auto_confirm'] = not fix_session['auto_confirm']
                        status = "å¯ç”¨" if fix_session['auto_confirm'] else "ç¦ç”¨"
                        print(f"âœ… è‡ªåŠ¨ç¡®è®¤å·²{status}")
                        continue

                    # å¤„ç†ä¿®å¤å‘½ä»¤
                    elif user_input.lower().startswith('fix '):
                        file_path = user_input[4:].strip()
                        result = self._fix_file_interactive(fix_coordinator, file_path, fix_session)
                        if result:
                            fix_session['fix_history'].append(result)
                        continue

                    # å¤„ç†æ‰¹é‡ä¿®å¤å‘½ä»¤
                    elif user_input.lower() == 'batch fix':
                        result = self._batch_fix_interactive(fix_coordinator, fix_session)
                        if result:
                            fix_session['fix_history'].extend(result.get('process_results', []))
                        continue

                    # å¤„ç†çŠ¶æ€å‘½ä»¤
                    elif user_input.lower() == 'status':
                        self._show_fix_status(fix_session)
                        continue

                    # å¤„ç†å†å²å‘½ä»¤
                    elif user_input.lower() == 'history':
                        self._show_fix_history(fix_session)
                        continue

                    # å¤„ç†å¯¼å‡ºå‘½ä»¤
                    elif user_input.lower().startswith('export '):
                        export_file = user_input[7:].strip()
                        self._export_fix_session(fix_session, export_file)
                        continue

                    # å¤„ç†æ™®é€šå¯¹è¯è¾“å…¥
                    else:
                        print(f"ğŸ’­ æ­£åœ¨å¤„ç†æ‚¨çš„é—®é¢˜: {user_input}")
                        response = self._generate_fix_response(user_input, fix_session)
                        print(f"ğŸ¤– ä¿®å¤åŠ©æ‰‹: {response}")

                except KeyboardInterrupt:
                    print("\nğŸ‘‹ ä½¿ç”¨Ctrl+Cé€€å‡ºä¿®å¤æ¨¡å¼")
                    break
                except EOFError:
                    print("\nğŸ‘‹ åˆ°è¾¾è¾“å…¥æœ«å°¾ï¼Œé€€å‡ºä¿®å¤æ¨¡å¼")
                    break

            # ç”Ÿæˆæœ€ç»ˆç»“æœ
            result = {
                'mode': 'fix',
                'target': target,
                'status': 'completed',
                'fix_session': fix_session,
                'files_scanned': len(fix_session['scanned_files']),
                'total_issues_found': sum(len(issues) for issues in fix_session['identified_issues'].values()),
                'fixes_attempted': len(fix_session['fix_history']),
                'successful_fixes': len([f for f in fix_session['fix_history'] if f.get('success', False)])
            }

            # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
            if self.output_file:
                self._save_fix_analysis_result(result, self.output_file)

            print("âœ… ä¿®å¤åˆ†æå®Œæˆ")
            return result

        except Exception as e:
            self.logger.error(f"Fix analysis failed: {e}")
            print(f"âŒ ä¿®å¤åˆ†æå¤±è´¥: {e}")
            return {'error': str(e)}

    def _show_deep_analysis_help(self):
        """æ˜¾ç¤ºæ·±åº¦åˆ†æå¸®åŠ©ä¿¡æ¯"""
        print("\nğŸ“– æ·±åº¦åˆ†æå¸®åŠ©")
        print("-" * 40)
        print("å¯ç”¨å‘½ä»¤:")
        print("  help                    - æ˜¾ç¤ºæ­¤å¸®åŠ©ä¿¡æ¯")
        print("  analyze <file_path>     - åˆ†ææŒ‡å®šæ–‡ä»¶")
        print("  type <analysis_type>    - è®¾ç½®åˆ†æç±»å‹")
        print("  summary                 - æ˜¾ç¤ºåˆ†ææ€»ç»“")
        print("  stats/performance/æ€§èƒ½ - æ˜¾ç¤ºæ€§èƒ½ç»Ÿè®¡")
        print("  errors/error/é”™è¯¯      - æ˜¾ç¤ºé”™è¯¯ç»Ÿè®¡")
        print("  config <é€‰é¡¹> <å€¼>      - é…ç½®åˆ†æå‚æ•°")
        print("  config show             - æ˜¾ç¤ºå½“å‰é…ç½®")
        print("  config help             - æ˜¾ç¤ºé…ç½®é€‰é¡¹")
        print("  config reset            - é‡ç½®ä¸ºé»˜è®¤é…ç½®")
        print("  export <filename>       - å¯¼å‡ºå¯¹è¯å†å²")
        print("  quit/exit/q             - é€€å‡ºåˆ†æ")
        print("\nåˆ†æç±»å‹:")
        print("  comprehensive           - ç»¼åˆåˆ†æ")
        print("  security                - å®‰å…¨åˆ†æ")
        print("  performance             - æ€§èƒ½åˆ†æ")
        print("  architecture            - æ¶æ„åˆ†æ")
        print("  code_review             - ä»£ç å®¡æŸ¥")
        print("  refactoring             - é‡æ„å»ºè®®")
        print("\né…ç½®é€‰é¡¹:")
        print("  config model <æ¨¡å‹>     - é€‰æ‹©AIæ¨¡å‹ (auto,glm-4.5,glm-4.6,gpt-4,claude-3)")
        print("  config depth <çº§åˆ«>     - è®¾ç½®åˆ†ææ·±åº¦ (basic,standard,detailed,comprehensive)")
        print("  config temperature <å€¼> - è®¾ç½®åˆ›é€ æ€§å‚æ•° (0.0-1.0)")
        print("  config max_tokens <æ•°å­—> - è®¾ç½®æœ€å¤§ç”Ÿæˆtokenæ•°")
        print("  config style <é£æ ¼>     - è®¾ç½®è¯­è¨€é£æ ¼ (casual,professional,technical)")
        print("  config format <æ ¼å¼>    - è®¾ç½®è¾“å‡ºæ ¼å¼ (concise,standard,comprehensive)")
        print("  config focus_areas <é¢†åŸŸ> - è®¾ç½®å…³æ³¨é¢†åŸŸ (security,performance,architecture)")
        print("\né«˜çº§åŠŸèƒ½:")
        print("  ğŸš€ æ™ºèƒ½ç¼“å­˜ç³»ç»Ÿ         - è‡ªåŠ¨ç¼“å­˜åˆ†æç»“æœ")
        print("  ğŸ“Š æ€§èƒ½ç›‘æ§             - å®æ—¶è·Ÿè¸ªå“åº”æ—¶é—´")
        print("  ğŸ” ä¸Šä¸‹æ–‡ä¼˜åŒ–           - æ™ºèƒ½ä¿®å‰ªå¯¹è¯ä¸Šä¸‹æ–‡")
        print("  â±ï¸ è¶…æ—¶ä¿æŠ¤             - é˜²æ­¢é•¿æ—¶é—´ç­‰å¾…")
        print("  âš™ï¸ çµæ´»é…ç½®             - å¤šæ ·åŒ–åˆ†æå‚æ•°é…ç½®")
        print("  ğŸ›¡ï¸ é”™è¯¯å¤„ç†             - è‡ªåŠ¨é‡è¯•å’Œé™çº§æ¨¡å¼")
        print("  ğŸ“ˆ é”™è¯¯ç»Ÿè®¡             - è¯¦ç»†é”™è¯¯åˆ†æå’Œå»ºè®®")
        print("  ğŸ”„ é™çº§æ¨¡å¼             - ç½‘ç»œå¼‚å¸¸æ—¶çš„ç¦»çº¿åˆ†æ")
        print("\né”™è¯¯å¤„ç†:")
        print("  è‡ªåŠ¨é‡è¯•æœºåˆ¶           - ç½‘ç»œå¼‚å¸¸æ—¶è‡ªåŠ¨é‡è¯•")
        print("  æ™ºèƒ½é™çº§æ¨¡å¼           - AIæœåŠ¡ä¸å¯ç”¨æ—¶ä½¿ç”¨é™æ€åˆ†æ")
        print("  è¯¦ç»†é”™è¯¯å»ºè®®           - é’ˆå¯¹ä¸åŒé”™è¯¯ç±»å‹æä¾›è§£å†³æ–¹æ¡ˆ")
        print("  é”™è¯¯ç»Ÿè®¡æŠ¥å‘Š           - è·Ÿè¸ªå’Œåˆ†æé”™è¯¯æ¨¡å¼")
        print("\nç¤ºä¾‹:")
        print("  analyze src/main.py")
        print("  type security")
        print("  config model glm-4.6")
        print("  config depth comprehensive")
        print("  config temperature 0.7")
        print("  stats                   # æŸ¥çœ‹æ€§èƒ½ç»Ÿè®¡")
        print("  errors                  # æŸ¥çœ‹é”™è¯¯ç»Ÿè®¡")
        print("  export conversation.json")
        print()

    def _analyze_file_interactive_with_context(self, analyzer, file_path: str) -> dict:
        """ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨çš„äº¤äº’å¼æ–‡ä»¶åˆ†æ"""
        from pathlib import Path
        import asyncio
        import time
        import hashlib

        try:
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            full_path = Path(self.conversation_context.target) / file_path
            if not full_path.exists():
                full_path = Path(file_path)  # å°è¯•ç»å¯¹è·¯å¾„

            if not full_path.exists():
                print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                print("ğŸ’¡ è¯·æ£€æŸ¥æ–‡ä»¶è·¯å¾„æ˜¯å¦æ­£ç¡®")

                # è®°å½•å¤±è´¥å°è¯•åˆ°ä¸Šä¸‹æ–‡
                self.conversation_context.add_message(
                    f"analyze {file_path}",
                    f"åˆ†æå¤±è´¥: æ–‡ä»¶ä¸å­˜åœ¨",
                    'file_analysis'
                )
                return None

            # æ€§èƒ½ä¼˜åŒ–ï¼šæ£€æŸ¥ç¼“å­˜
            cache_key = self._generate_cache_key(str(full_path), self.conversation_context.analysis_context['analysis_type'])
            cached_result = self._get_cached_result(cache_key)

            if cached_result:
                print(f"\nâš¡ ä½¿ç”¨ç¼“å­˜ç»“æœ")
                print(f"ğŸ“„ æ–‡ä»¶: {full_path.name}")
                print(f"ğŸ”„ ç¼“å­˜æ—¶é—´: {cached_result.get('cached_at', 'æœªçŸ¥')}")

                # æ›´æ–°æ€§èƒ½ç»Ÿè®¡
                self.performance_stats['total_cache_hits'] += 1

                # æ·»åŠ åˆ°ä¸Šä¸‹æ–‡ç®¡ç†å™¨
                self.conversation_context.add_analysis_result(
                    str(full_path),
                    self.conversation_context.analysis_context['analysis_type'],
                    cached_result['result'],
                    0.01  # ç¼“å­˜å“åº”æ—¶é—´
                )

                print(f"âœ… ç¼“å­˜åˆ†æå®Œæˆ")
                return cached_result['result']

            # æ€§èƒ½ç›‘æ§ï¼šå¼€å§‹è®¡æ—¶
            analysis_start_time = time.time()
            self.performance_stats['analysis_count'] += 1

            # æ˜¾ç¤ºæ–‡ä»¶ä¿¡æ¯
            file_name = full_path.name
            file_size = self._format_file_size(full_path.stat().st_size)

            print(f"\nğŸ” å‡†å¤‡åˆ†ææ–‡ä»¶: {file_name}")
            print(f"ğŸ“„ æ–‡ä»¶å¤§å°: {file_size}")
            print(f"ğŸ“Š åˆ†æç±»å‹: {self.conversation_context.analysis_context['analysis_type']}")
            print(f"ğŸ• å¼€å§‹æ—¶é—´: {self._get_current_time()}")

            # æ˜¾ç¤ºä¸Šä¸‹æ–‡ä¿¡æ¯
            context_summary = self.conversation_context.get_context_summary()
            print(f"ğŸ’­ ä¸Šä¸‹æ–‡: {context_summary}")

            # æ˜¾ç¤ºé™æ€åˆ†ææ‘˜è¦ï¼ˆå¦‚æœæœ‰ï¼‰
            self._show_static_analysis_summary(str(full_path))

            print("-" * 50)

            # æ˜¾ç¤ºåˆ†æè¿›åº¦
            self._show_analyzing_animation("AIæ­£åœ¨æ·±åº¦åˆ†æä»£ç ç»“æ„")

            # æ€§èƒ½ä¼˜åŒ–ï¼šåˆ›å»ºä¼˜åŒ–çš„åˆ†æè¯·æ±‚
            optimized_context = self._optimize_context_for_analysis()

            # é›†æˆé™æ€åˆ†æç»“æœ
            integrated_context = self._integrate_static_analysis_into_context(str(full_path), optimized_context)

            # åº”ç”¨é…ç½®å‚æ•°åˆ°è¯·æ±‚
            request_params = {
                'file_path': str(full_path),
                'analysis_type': self.conversation_context.analysis_context['analysis_type'],
                'context': integrated_context
            }

            # æ·»åŠ é…ç½®å‚æ•°åˆ°ä¸Šä¸‹æ–‡
            request_params['context']['analysis_config'] = self.analysis_config.copy()

            # å¦‚æœè®¾ç½®äº†å…³æ³¨é¢†åŸŸï¼Œæ·»åŠ åˆ°è¯·æ±‚ä¸­
            if self.analysis_config['focus_areas']:
                request_params['focus_areas'] = self.analysis_config['focus_areas']

            # æ ¹æ®åˆ†ææ·±åº¦è°ƒæ•´è¯·æ±‚
            if self.analysis_config['analysis_depth'] == 'basic':
                request_params['max_tokens'] = min(self.analysis_config['max_tokens'], 2000)
            elif self.analysis_config['analysis_depth'] == 'detailed':
                request_params['max_tokens'] = max(self.analysis_config['max_tokens'], 6000)
            elif self.analysis_config['analysis_depth'] == 'comprehensive':
                request_params['max_tokens'] = max(self.analysis_config['max_tokens'], 8000)

            # è®¾ç½®å…¶ä»–å‚æ•°
            request_params['temperature'] = self.analysis_config['temperature']
            # å°†é¢å¤–çš„é…ç½®å‚æ•°æ·»åŠ åˆ°contextä¸­
            request_params['context']['extra_config'] = {
                'enable_structured_output': self.analysis_config.get('enable_structured_output', False),
                'language_style': self.analysis_config.get('language_style', 'professional'),
                'output_format': self.analysis_config.get('output_format', 'json')
            }

            request = DeepAnalysisRequest(**request_params)

            # æ‰§è¡Œå¼‚æ­¥åˆ†æï¼ˆå¸¦é‡è¯•å’Œé”™è¯¯å¤„ç†ï¼‰
            result = None
            execution_time = 0
            attempt = 1

            while attempt <= self.error_handling_config['max_retry_attempts']:
                try:
                    # è°ƒæ•´è¶…æ—¶æ—¶é—´ï¼ˆå¦‚æœæ˜¯é‡è¯•ä¸”æœ‰è¶…æ—¶é”™è¯¯ï¼‰
                    current_timeout = self.optimization_config['response_timeout']
                    if attempt > 1:
                        current_timeout *= 1.5  # æ¯æ¬¡é‡è¯•å¢åŠ 50%è¶…æ—¶æ—¶é—´

                    result = asyncio.run(
                        asyncio.wait_for(
                            analyzer.analyze_file(request),
                            timeout=current_timeout
                        )
                    )

                    execution_time = time.time() - analysis_start_time
                    break  # æˆåŠŸåˆ™è·³å‡ºé‡è¯•å¾ªç¯

                except asyncio.TimeoutError as e:
                    execution_time = time.time() - analysis_start_time
                    retry_decision = self._handle_analysis_error(e, str(full_path), attempt)

                    if retry_decision == 'retry_with_increased_timeout':
                        # å¢åŠ è¶…æ—¶æ—¶é—´ç»§ç»­é‡è¯•
                        attempt += 1
                        continue
                    elif retry_decision == 'retry':
                        # æ ‡å‡†é‡è¯•
                        attempt += 1
                        continue
                    else:
                        # é™çº§æ¨¡å¼æˆ–å¤±è´¥å¤„ç†
                        if isinstance(retry_decision, dict):
                            result = retry_decision
                            execution_time = time.time() - analysis_start_time
                        break

                except Exception as e:
                    execution_time = time.time() - analysis_start_time
                    retry_decision = self._handle_analysis_error(e, str(full_path), attempt)

                    if retry_decision == 'retry':
                        attempt += 1
                        continue
                    else:
                        # é™çº§æ¨¡å¼æˆ–å¤±è´¥å¤„ç†
                        if isinstance(retry_decision, dict):
                            result = retry_decision
                            execution_time = time.time() - analysis_start_time
                        break

            # å¦‚æœæ²¡æœ‰ä»»ä½•ç»“æœï¼Œè¿”å›None
            if result is None:
                return None

            # æ˜¾ç¤ºåˆ†æç»“æœæ¨ªå¹…
            self._show_analysis_result_banner(result.success, file_name)

            if result.success:
                # æ£€æŸ¥æ˜¯å¦ä¸ºé™çº§æ¨¡å¼
                if hasattr(result, 'fallback_mode'):
                    fallback_mode = getattr(result, 'fallback_mode', '')
                    if fallback_mode:
                        print(f"\nğŸ”„ ä½¿ç”¨äº†é™çº§åˆ†ææ¨¡å¼: {fallback_mode}")
                        self.error_stats['successful_recoveries'] += 1
                else:
                    print(f"\nğŸ‰ åˆ†ææˆåŠŸå®Œæˆï¼")

                print(f"â±ï¸ æ‰§è¡Œæ—¶é—´: {execution_time:.2f}ç§’")

                # æ€§èƒ½ç›‘æ§ï¼šæ›´æ–°ç»Ÿè®¡ä¿¡æ¯
                self._update_performance_stats(execution_time, True)

                if hasattr(result, 'model_used') and result.model_used:
                    print(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹: {result.model_used}")

                # æ˜¾ç¤ºåˆ†æç±»å‹å’ŒçŠ¶æ€
                if hasattr(result, 'analysis_type'):
                    analysis_type = result.analysis_type
                else:
                    analysis_type = self.conversation_context.analysis_context['analysis_type']

                print(f"ğŸ“Š åˆ†æç±»å‹: {analysis_type}")
                print(f"ğŸš€ ç¼“å­˜çŠ¶æ€: {self._get_cache_status()}")
                print("-" * 50)

                # æ ¹æ®ç»“æœç±»å‹æ˜¾ç¤ºä¸åŒçš„å†…å®¹
                if hasattr(result, 'fallback_mode'):
                    if result.fallback_mode == 'static_analysis':
                        self._show_static_analysis_fallback_result(result)
                    elif result.fallback_mode == 'basic_info':
                        self._show_basic_file_info_result(result)
                else:
                    # æ­£å¸¸åˆ†æç»“æœ
                    if hasattr(result, 'structured_analysis') and result.structured_analysis and result.structured_analysis.get('structured'):
                        self._show_enhanced_structured_result(result.structured_analysis)
                    else:
                        # æ˜¾ç¤ºæ–‡æœ¬ç»“æœçš„æ‘˜è¦
                        content = getattr(result, 'content', '')
                        if content:
                            self._show_text_result_preview(content)

                # åªæœ‰éé™çº§æ¨¡å¼æ‰ç¼“å­˜ç»“æœ
                if not hasattr(result, 'fallback_mode'):
                    self._cache_result(cache_key, result.to_dict())

                # æ·»åŠ åˆ°ä¸Šä¸‹æ–‡ç®¡ç†å™¨
                self.conversation_context.add_analysis_result(
                    str(full_path),
                    analysis_type,
                    result.to_dict(),
                    execution_time
                )

                print(f"\nğŸ’¡ æç¤º: ä½¿ç”¨ 'summary' æŸ¥çœ‹ä¼šè¯æ€»ç»“")
                print(f"ğŸ’¡ æç¤º: ä½¿ç”¨ 'export <filename>' å¯¼å‡ºå¯¹è¯å†å²")
                print(f"ğŸ’¡ æç¤º: ä½¿ç”¨ 'stats' æŸ¥çœ‹æ€§èƒ½ç»Ÿè®¡")
                print(f"ğŸ’¡ æç¤º: ä½¿ç”¨ 'errors' æŸ¥çœ‹é”™è¯¯ç»Ÿè®¡")

                return result.to_dict()
            else:
                print(f"\nâŒ åˆ†æå¤±è´¥")
                error_msg = getattr(result, 'error', 'æœªçŸ¥é”™è¯¯')
                print(f"ğŸ”´ é”™è¯¯ä¿¡æ¯: {error_msg}")
                print(f"â±ï¸ è€—æ—¶: {execution_time:.2f}ç§’")

                # æ€§èƒ½ç›‘æ§ï¼šæ›´æ–°ç»Ÿè®¡ä¿¡æ¯
                self._update_performance_stats(execution_time, False)

                # è®°å½•å¤±è´¥çš„åˆ†æåˆ°ä¸Šä¸‹æ–‡
                self.conversation_context.add_analysis_result(
                    str(full_path),
                    self.conversation_context.analysis_context['analysis_type'],
                    {'success': False, 'error': error_msg},
                    execution_time
                )

                print(f"\nğŸ’¡ å»ºè®®:")
                print(f"  â€¢ æ£€æŸ¥æ–‡ä»¶æ˜¯å¦ä¸ºæœ‰æ•ˆçš„Pythonä»£ç ")
                print(f"  â€¢ ç¡®è®¤ç½‘ç»œè¿æ¥æ­£å¸¸")
                print(f"  â€¢ å°è¯•æ›´æ¢åˆ†æç±»å‹")
                print(f"  â€¢ ä½¿ç”¨ 'errors' æŸ¥çœ‹è¯¦ç»†é”™è¯¯ä¿¡æ¯")

                return None

        except Exception as e:
            print(f"\nâŒ åˆ†æè¿‡ç¨‹ä¸­å‡ºç°å¼‚å¸¸")
            print(f"ğŸ”´ å¼‚å¸¸ä¿¡æ¯: {e}")
            self.logger.error(f"Interactive analysis failed: {e}")

            # è®°å½•å¼‚å¸¸åˆ°ä¸Šä¸‹æ–‡
            self.conversation_context.add_message(
                f"analyze {file_path}",
                f"åˆ†æå¼‚å¸¸: {e}",
                'file_analysis'
            )

            print(f"\nğŸ’¡ æ•…éšœæ’é™¤:")
            print(f"  â€¢ æ£€æŸ¥æ–‡ä»¶è·¯å¾„æ˜¯å¦æ­£ç¡®")
            print(f"  â€¢ ç¡®è®¤æ–‡ä»¶æƒé™å¯è¯»")
            print(f"  â€¢ æ£€æŸ¥ç³»ç»Ÿèµ„æºæ˜¯å¦å……è¶³")

            return None

    def _generate_cache_key(self, file_path: str, analysis_type: str) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        import hashlib
        import os

        # ä½¿ç”¨æ–‡ä»¶è·¯å¾„ã€åˆ†æç±»å‹å’Œä¿®æ”¹æ—¶é—´ç”Ÿæˆç¼“å­˜é”®
        try:
            mtime = os.path.getmtime(file_path)
            cache_data = f"{file_path}:{analysis_type}:{mtime}"
            return hashlib.md5(cache_data.encode()).hexdigest()
        except OSError:
            # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨æˆ–æ— æ³•è·å–ä¿®æ”¹æ—¶é—´ï¼Œä½¿ç”¨ç®€å•ç¼“å­˜é”®
            cache_data = f"{file_path}:{analysis_type}"
            return hashlib.md5(cache_data.encode()).hexdigest()

    def _get_cached_result(self, cache_key: str) -> Optional[dict]:
        """è·å–ç¼“å­˜ç»“æœ"""
        if not self.enable_caching:
            return None

        cache_entry = self.analysis_cache.get(cache_key)
        if not cache_entry:
            return None

        # æ£€æŸ¥ç¼“å­˜æ˜¯å¦è¿‡æœŸ
        import time
        current_time = time.time()
        if current_time - cache_entry['timestamp'] > self.cache_ttl:
            del self.analysis_cache[cache_key]
            return None

        self.performance_stats['total_cache_hits'] += 1
        return cache_entry

    def _cache_result(self, cache_key: str, result: dict):
        """ç¼“å­˜åˆ†æç»“æœ"""
        if not self.enable_caching:
            return

        import time
        cache_entry = {
            'result': result,
            'timestamp': time.time(),
            'cached_at': self._get_current_time(),
            'cache_key': cache_key
        }

        self.analysis_cache[cache_key] = cache_entry

        # æ¸…ç†è¿‡æœŸç¼“å­˜
        self._cleanup_expired_cache()

        # é™åˆ¶ç¼“å­˜å¤§å°
        if len(self.analysis_cache) > 100:  # æœ€å¤šç¼“å­˜100ä¸ªç»“æœ
            # åˆ é™¤æœ€æ—§çš„ç¼“å­˜é¡¹
            oldest_key = min(self.analysis_cache.keys(),
                           key=lambda k: self.analysis_cache[k]['timestamp'])
            del self.analysis_cache[oldest_key]

    def _cleanup_expired_cache(self):
        """æ¸…ç†è¿‡æœŸç¼“å­˜"""
        import time
        current_time = time.time()
        expired_keys = []

        for cache_key, cache_entry in self.analysis_cache.items():
            if current_time - cache_entry['timestamp'] > self.cache_ttl:
                expired_keys.append(cache_key)

        for key in expired_keys:
            del self.analysis_cache[key]

    def _optimize_context_for_analysis(self) -> dict:
        """ä¼˜åŒ–åˆ†æä¸Šä¸‹æ–‡"""
        if not self.conversation_context:
            return {}

        # è·å–åŸå§‹ä¸Šä¸‹æ–‡
        original_context = self.conversation_context.analysis_context.copy()

        # æ™ºèƒ½ä¸Šä¸‹æ–‡ä¿®å‰ª
        if self.optimization_config['smart_context_trimming']:
            context_str = str(original_context)
            if len(context_str) > self.optimization_config['context_length_limit']:
                # ä¿ç•™é‡è¦ä¿¡æ¯ï¼Œåˆ é™¤å†—ä½™å†…å®¹
                optimized_context = {
                    'target': original_context.get('target', ''),
                    'analysis_type': original_context.get('analysis_type', 'comprehensive'),
                    'current_file': original_context.get('current_file', ''),
                    'previous_results': original_context.get('previous_results', [])[-3:],  # åªä¿ç•™æœ€è¿‘3ä¸ªç»“æœ
                    'preferences': original_context.get('preferences', {}),
                    'session_stats': {
                        'total_analyses': original_context.get('session_stats', {}).get('total_analyses', 0),
                        'successful_analyses': original_context.get('session_stats', {}).get('successful_analyses', 0),
                        'most_used_analysis_type': self.conversation_context.get_session_stats().get('most_used_analysis_type', 'comprehensive')
                    }
                }
                return optimized_context

        return original_context

    def _update_performance_stats(self, execution_time: float, success: bool):
        """æ›´æ–°æ€§èƒ½ç»Ÿè®¡"""
        if not self.enable_performance_monitoring:
            return

        self.performance_stats['total_analysis_time'] += execution_time
        self.performance_stats['total_cache_misses'] += 1

        # æ›´æ–°å¹³å‡å“åº”æ—¶é—´
        if self.performance_stats['analysis_count'] > 0:
            self.performance_stats['avg_response_time'] = (
                self.performance_stats['total_analysis_time'] / self.performance_stats['analysis_count']
            )

        # è®°å½•æ…¢è¯·æ±‚
        if execution_time > 30:  # è¶…è¿‡30ç§’çš„è¯·æ±‚
            slow_request = {
                'timestamp': self._get_current_time(),
                'execution_time': execution_time,
                'success': success
            }
            self.performance_stats['slow_requests'].append(slow_request)

            # åªä¿ç•™æœ€è¿‘10ä¸ªæ…¢è¯·æ±‚è®°å½•
            if len(self.performance_stats['slow_requests']) > 10:
                self.performance_stats['slow_requests'] = self.performance_stats['slow_requests'][-10:]

    def _get_cache_status(self) -> str:
        """è·å–ç¼“å­˜çŠ¶æ€ä¿¡æ¯"""
        if not self.enable_caching:
            return "å·²ç¦ç”¨"

        cache_hits = self.performance_stats['total_cache_hits']
        cache_misses = self.performance_stats['total_cache_misses']
        total_requests = cache_hits + cache_misses

        if total_requests == 0:
            return f"ç¼“å­˜: {len(self.analysis_cache)} é¡¹"

        hit_rate = (cache_hits / total_requests) * 100
        return f"ç¼“å­˜: {len(self.analysis_cache)} é¡¹, å‘½ä¸­ç‡: {hit_rate:.1f}%"

    def _show_performance_stats(self):
        """æ˜¾ç¤ºæ€§èƒ½ç»Ÿè®¡ä¿¡æ¯"""
        if not self.enable_performance_monitoring:
            print("âŒ æ€§èƒ½ç›‘æ§å·²ç¦ç”¨")
            return

        print("\nğŸ“Š æ€§èƒ½ç»Ÿè®¡æŠ¥å‘Š")
        print("=" * 50)

        stats = self.performance_stats
        print(f"ğŸ” æ€»åˆ†ææ¬¡æ•°: {stats['analysis_count']}")
        print(f"â±ï¸ æ€»åˆ†ææ—¶é—´: {stats['total_analysis_time']:.2f}ç§’")
        print(f"ğŸ“ˆ å¹³å‡å“åº”æ—¶é—´: {stats['avg_response_time']:.2f}ç§’")

        if stats['total_cache_hits'] + stats['total_cache_misses'] > 0:
            total_requests = stats['total_cache_hits'] + stats['total_cache_misses']
            hit_rate = (stats['total_cache_hits'] / total_requests) * 100
            print(f"ğŸš€ ç¼“å­˜å‘½ä¸­ç‡: {hit_rate:.1f}% ({stats['total_cache_hits']}/{total_requests})")

        print(f"ğŸ’¾ ç¼“å­˜å¤§å°: {len(self.analysis_cache)} é¡¹")

        # æ…¢è¯·æ±‚åˆ†æ
        if stats['slow_requests']:
            print(f"\nâš ï¸ æ…¢è¯·æ±‚è®°å½• ({len(stats['slow_requests'])} ä¸ª):")
            for i, req in enumerate(stats['slow_requests'][-5:], 1):  # æ˜¾ç¤ºæœ€è¿‘5ä¸ª
                print(f"  {i}. {req['timestamp']} - {req['execution_time']:.1f}ç§’ {'âœ…' if req['success'] else 'âŒ'}")

        # ä¼˜åŒ–å»ºè®®
        print(f"\nğŸ’¡ æ€§èƒ½ä¼˜åŒ–å»ºè®®:")
        if stats['avg_response_time'] > 20:
            print(f"  â€¢ å¹³å‡å“åº”æ—¶é—´è¾ƒé•¿ï¼Œå»ºè®®å¯ç”¨ç¼“å­˜åŠŸèƒ½")
        if stats.get('total_cache_hits', 0) + stats.get('total_cache_misses', 0) > 0:
            hit_rate = stats['total_cache_hits'] / (stats['total_cache_hits'] + stats['total_cache_misses']) * 100
            if hit_rate < 30:
                print(f"  â€¢ ç¼“å­˜å‘½ä¸­ç‡è¾ƒä½ï¼Œå»ºè®®åˆ†æç›¸ä¼¼æ–‡ä»¶ä»¥æé«˜ç¼“å­˜æ•ˆç‡")
        if len(stats['slow_requests']) > 3:
            print(f"  â€¢ æ…¢è¯·æ±‚è¾ƒå¤šï¼Œå»ºè®®æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–ä½¿ç”¨è¾ƒå°çš„æ–‡ä»¶")

        print()

    def _configure_analysis_settings(self, config_key: str, config_value: str) -> bool:
        """é…ç½®åˆ†æè®¾ç½®"""
        try:
            config_key = config_key.strip()
            config_value = config_value.strip()

            if config_key in ['model', 'model_selection']:
                valid_models = ['auto', 'glm-4.5', 'glm-4.6', 'gpt-4', 'claude-3']
                if config_value.lower() in valid_models:
                    self.analysis_config['model_selection'] = config_value.lower()
                    print(f"âœ… æ¨¡å‹å·²è®¾ç½®ä¸º: {config_value}")
                    return True
                else:
                    print(f"âŒ ä¸æ”¯æŒçš„æ¨¡å‹: {config_value}")
                    print(f"æ”¯æŒçš„æ¨¡å‹: {', '.join(valid_models)}")
                    return False

            elif config_key in ['depth', 'analysis_depth']:
                valid_depths = ['basic', 'standard', 'detailed', 'comprehensive']
                if config_value.lower() in valid_depths:
                    self.analysis_config['analysis_depth'] = config_value.lower()
                    print(f"âœ… åˆ†ææ·±åº¦å·²è®¾ç½®ä¸º: {config_value}")
                    return True
                else:
                    print(f"âŒ ä¸æ”¯æŒçš„æ·±åº¦çº§åˆ«: {config_value}")
                    print(f"æ”¯æŒçš„æ·±åº¦: {', '.join(valid_depths)}")
                    return False

            elif config_key == 'temperature':
                try:
                    temp = float(config_value)
                    if 0.0 <= temp <= 1.0:
                        self.analysis_config['temperature'] = temp
                        print(f"âœ… åˆ›é€ æ€§å‚æ•°å·²è®¾ç½®ä¸º: {temp}")
                        return True
                    else:
                        print(f"âŒ æ¸©åº¦å€¼å¿…é¡»åœ¨0.0-1.0ä¹‹é—´")
                        return False
                except ValueError:
                    print(f"âŒ æ— æ•ˆçš„æ¸©åº¦å€¼: {config_value}")
                    return False

            elif config_key == 'max_tokens':
                try:
                    tokens = int(config_value)
                    if tokens > 0:
                        self.analysis_config['max_tokens'] = tokens
                        print(f"âœ… æœ€å¤§tokenæ•°å·²è®¾ç½®ä¸º: {tokens}")
                        return True
                    else:
                        print(f"âŒ tokenæ•°å¿…é¡»å¤§äº0")
                        return False
                except ValueError:
                    print(f"âŒ æ— æ•ˆçš„tokenæ•°: {config_value}")
                    return False

            elif config_key in ['style', 'language_style']:
                valid_styles = ['casual', 'professional', 'technical']
                if config_value.lower() in valid_styles:
                    self.analysis_config['language_style'] = config_value.lower()
                    print(f"âœ… è¯­è¨€é£æ ¼å·²è®¾ç½®ä¸º: {config_value}")
                    return True
                else:
                    print(f"âŒ ä¸æ”¯æŒçš„è¯­è¨€é£æ ¼: {config_value}")
                    print(f"æ”¯æŒçš„é£æ ¼: {', '.join(valid_styles)}")
                    return False

            elif config_key in ['format', 'output_format']:
                valid_formats = ['concise', 'standard', 'comprehensive']
                if config_value.lower() in valid_formats:
                    self.analysis_config['output_format'] = config_value.lower()
                    print(f"âœ… è¾“å‡ºæ ¼å¼å·²è®¾ç½®ä¸º: {config_value}")
                    return True
                else:
                    print(f"âŒ ä¸æ”¯æŒçš„è¾“å‡ºæ ¼å¼: {config_value}")
                    print(f"æ”¯æŒçš„æ ¼å¼: {', '.join(valid_formats)}")
                    return False

            elif config_key == 'structured_output':
                if config_value.lower() in ['true', 'on', 'enable', '1']:
                    self.analysis_config['enable_structured_output'] = True
                    print(f"âœ… ç»“æ„åŒ–è¾“å‡ºå·²å¯ç”¨")
                    return True
                elif config_value.lower() in ['false', 'off', 'disable', '0']:
                    self.analysis_config['enable_structured_output'] = False
                    print(f"âœ… ç»“æ„åŒ–è¾“å‡ºå·²ç¦ç”¨")
                    return True
                else:
                    print(f"âŒ æ— æ•ˆçš„å€¼: {config_value} (ä½¿ç”¨ true/false)")
                    return False

            elif config_key == 'focus_areas':
                areas = [area.strip() for area in config_value.split(',')]
                valid_areas = ['security', 'performance', 'architecture', 'code_quality', 'best_practices', 'testing']
                valid_area_list = []
                for area in areas:
                    if area.lower() in valid_areas:
                        valid_area_list.append(area.lower())
                    else:
                        print(f"âš ï¸ è·³è¿‡æ— æ•ˆçš„å…³æ³¨é¢†åŸŸ: {area}")

                if valid_area_list:
                    self.analysis_config['focus_areas'] = valid_area_list
                    print(f"âœ… å…³æ³¨é¢†åŸŸå·²è®¾ç½®ä¸º: {', '.join(valid_area_list)}")
                    return True
                else:
                    print(f"âŒ æ²¡æœ‰æœ‰æ•ˆçš„å…³æ³¨é¢†åŸŸ")
                    print(f"æ”¯æŒçš„é¢†åŸŸ: {', '.join(valid_areas)}")
                    return False

            else:
                print(f"âŒ ä¸æ”¯æŒçš„é…ç½®é¡¹: {config_key}")
                self._show_available_configs()
                return False

        except Exception as e:
            print(f"âŒ é…ç½®è®¾ç½®å¤±è´¥: {e}")
            return False

    def _show_current_config(self):
        """æ˜¾ç¤ºå½“å‰é…ç½®"""
        print("\nâš™ï¸ å½“å‰åˆ†æé…ç½®")
        print("=" * 50)

        print(f"ğŸ¤– AIæ¨¡å‹: {self.analysis_config['model_selection']}")
        print(f"ğŸ” åˆ†ææ·±åº¦: {self.analysis_config['analysis_depth']}")
        print(f"ğŸ¨ åˆ›é€ æ€§å‚æ•°: {self.analysis_config['temperature']}")
        print(f"ğŸ“ æœ€å¤§tokens: {self.analysis_config['max_tokens']}")
        print(f"ğŸ—ï¸ ç»“æ„åŒ–è¾“å‡º: {'å¯ç”¨' if self.analysis_config['enable_structured_output'] else 'ç¦ç”¨'}")
        print(f"ğŸ’¬ è¯­è¨€é£æ ¼: {self.analysis_config['language_style']}")
        print(f"ğŸ“‹ è¾“å‡ºæ ¼å¼: {self.analysis_config['output_format']}")

        if self.analysis_config['focus_areas']:
            print(f"ğŸ¯ å…³æ³¨é¢†åŸŸ: {', '.join(self.analysis_config['focus_areas'])}")
        else:
            print(f"ğŸ¯ å…³æ³¨é¢†åŸŸ: å…¨éƒ¨")

        if self.analysis_config['custom_prompt_template']:
            print(f"ğŸ“„ è‡ªå®šä¹‰æç¤ºè¯: å·²è®¾ç½®")
        else:
            print(f"ğŸ“„ è‡ªå®šä¹‰æç¤ºè¯: æœªè®¾ç½®")

        print()

    def _show_available_configs(self):
        """æ˜¾ç¤ºå¯ç”¨çš„é…ç½®é€‰é¡¹"""
        print("\nâš™ï¸ å¯ç”¨é…ç½®é€‰é¡¹")
        print("=" * 50)
        print("é…ç½®æ ¼å¼: config <é€‰é¡¹> <å€¼>")
        print()
        print("æ¨¡å‹é€‰æ‹©:")
        print("  config model auto|glm-4.5|glm-4.6|gpt-4|claude-3")
        print()
        print("åˆ†ææ·±åº¦:")
        print("  config depth basic|standard|detailed|comprehensive")
        print()
        print("å“åº”å‚æ•°:")
        print("  config temperature <0.0-1.0>")
        print("  config max_tokens <æ•°å­—>")
        print("  config structured_output true|false")
        print()
        print("è¾“å‡ºæ ¼å¼:")
        print("  config style casual|professional|technical")
        print("  config format concise|standard|comprehensive")
        print()
        print("å…³æ³¨é¢†åŸŸ:")
        print("  config focus_areas security,performance,architecture")
        print()
        print("ç¤ºä¾‹:")
        print("  config model glm-4.6")
        print("  config depth comprehensive")
        print("  config temperature 0.7")
        print("  config focus_areas security,performance")
        print()

    def _reset_config_to_defaults(self):
        """é‡ç½®é…ç½®ä¸ºé»˜è®¤å€¼"""
        self.analysis_config = {
            'model_selection': 'auto',
            'analysis_depth': 'standard',
            'custom_prompt_template': None,
            'temperature': 0.3,
            'max_tokens': 4000,
            'enable_structured_output': True,
            'focus_areas': [],
            'exclude_patterns': [],
            'include_patterns': [],
            'language_style': 'professional',
            'output_format': 'comprehensive'
        }
        print("âœ… é…ç½®å·²é‡ç½®ä¸ºé»˜è®¤å€¼")

    def _handle_analysis_error(self, error: Exception, file_path: str, attempt: int = 1) -> Optional[dict]:
        """å¤„ç†åˆ†æé”™è¯¯å¹¶å°è¯•æ¢å¤"""
        import time
        import traceback
        from datetime import datetime

        # è®°å½•é”™è¯¯ä¿¡æ¯
        error_info = {
            'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            'file_path': file_path,
            'attempt': attempt,
            'error_type': type(error).__name__,
            'error_message': str(error),
            'traceback': traceback.format_exc()
        }

        # æ›´æ–°é”™è¯¯ç»Ÿè®¡
        self._update_error_stats(error_info)

        # åˆ¤æ–­é”™è¯¯ç±»å‹å¹¶é‡‡å–ç›¸åº”ç­–ç•¥
        error_type = self._classify_error(error)
        recovery_strategy = self.error_handling_config['error_recovery_strategies'].get(error_type)

        print(f"\nâš ï¸ åˆ†æå‡ºç°é”™è¯¯ (ç¬¬{attempt}æ¬¡å°è¯•)")
        print(f"ğŸ”´ é”™è¯¯ç±»å‹: {error_type}")
        print(f"ğŸ“ é”™è¯¯ä¿¡æ¯: {str(error)}")

        # æ ¹æ®é”™è¯¯ç±»å‹æä¾›è§£å†³å»ºè®®
        suggestions = self._get_error_suggestions(error_type, error)
        if suggestions:
            print("ğŸ’¡ å»ºè®®è§£å†³æ–¹æ¡ˆ:")
            for i, suggestion in enumerate(suggestions, 1):
                print(f"  {i}. {suggestion}")

        # æ£€æŸ¥æ˜¯å¦åº”è¯¥é‡è¯•
        if attempt < self.error_handling_config['max_retry_attempts']:
            if recovery_strategy == 'retry_with_backoff':
                delay = self.error_handling_config['retry_delay'] * (2 ** (attempt - 1))
                print(f"ğŸ”„ {delay}ç§’åè‡ªåŠ¨é‡è¯•...")
                time.sleep(delay)
                return 'retry'
            elif recovery_strategy == 'exponential_backoff':
                delay = min(self.error_handling_config['retry_delay'] * (2 ** (attempt - 1)), 30)
                print(f"ğŸ”„ {delay}ç§’åè‡ªåŠ¨é‡è¯•...")
                time.sleep(delay)
                return 'retry'
            elif recovery_strategy == 'increase_timeout':
                print("ğŸ”„ å¢åŠ è¶…æ—¶æ—¶é—´åé‡è¯•...")
                return 'retry_with_increased_timeout'

        # å¦‚æœè¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°æˆ–é”™è¯¯ä¸å¯é‡è¯•ï¼Œè¿”å›None
        print(f"âŒ å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°({self.error_handling_config['max_retry_attempts']})æˆ–é”™è¯¯ä¸å¯é‡è¯•")

        # å°è¯•é™çº§æ¨¡å¼
        if self.error_handling_config['enable_fallback_mode']:
            return self._try_fallback_analysis(file_path, error_info)

        return None

    def _classify_error(self, error: Exception) -> str:
        """åˆ†ç±»é”™è¯¯ç±»å‹"""
        error_msg = str(error).lower()
        error_type_name = type(error).__name__.lower()

        # ç½‘ç»œç›¸å…³é”™è¯¯
        if any(keyword in error_msg for keyword in ['connection', 'network', 'timeout', 'unreachable', 'dns']):
            return 'network_error'

        # APIç›¸å…³é”™è¯¯
        if any(keyword in error_msg for keyword in ['api', 'authentication', 'authorization', 'quota', 'limit']):
            return 'api_error'

        # è¶…æ—¶é”™è¯¯
        if any(keyword in error_msg for keyword in ['timeout', 'timed out']) or 'timeout' in error_type_name:
            return 'timeout_error'

        # é™æµé”™è¯¯
        if any(keyword in error_msg for keyword in ['rate limit', 'too many requests', 'quota exceeded']):
            return 'rate_limit_error'

        # æ–‡ä»¶ç›¸å…³é”™è¯¯
        if any(keyword in error_msg for keyword in ['file', 'not found', 'permission', 'access']) or 'file' in error_type_name:
            return 'file_error'

        # å…¶ä»–é”™è¯¯
        return 'unknown_error'

    def _get_error_suggestions(self, error_type: str, error: Exception) -> list:
        """æ ¹æ®é”™è¯¯ç±»å‹æä¾›è§£å†³å»ºè®®"""
        suggestions = []

        if error_type == 'network_error':
            suggestions = [
                "æ£€æŸ¥ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸",
                "å°è¯•åˆ‡æ¢åˆ°å…¶ä»–ç½‘ç»œç¯å¢ƒ",
                "æ£€æŸ¥é˜²ç«å¢™è®¾ç½®æ˜¯å¦é˜»æ­¢äº†è¿æ¥",
                "ç¨åé‡è¯•ï¼Œå¯èƒ½æ˜¯ä¸´æ—¶ç½‘ç»œé—®é¢˜"
            ]
        elif error_type == 'api_error':
            suggestions = [
                "æ£€æŸ¥APIå¯†é’¥æ˜¯å¦æ­£ç¡®é…ç½®",
                "ç¡®è®¤APIé…é¢æ˜¯å¦å……è¶³",
                "å°è¯•åˆ‡æ¢åˆ°å…¶ä»–AIæ¨¡å‹",
                "æ£€æŸ¥è®¤è¯ä¿¡æ¯æ˜¯å¦è¿‡æœŸ"
            ]
        elif error_type == 'timeout_error':
            suggestions = [
                "å°è¯•åˆ†æè¾ƒå°çš„æ–‡ä»¶",
                "å¢åŠ è¶…æ—¶æ—¶é—´è®¾ç½®",
                "æ£€æŸ¥ç½‘ç»œå»¶è¿Ÿæ˜¯å¦è¿‡é«˜",
                "ç®€åŒ–åˆ†æå‚æ•°"
            ]
        elif error_type == 'rate_limit_error':
            suggestions = [
                "ç­‰å¾…ä¸€æ®µæ—¶é—´åé‡è¯•",
                "é™ä½è¯·æ±‚é¢‘ç‡",
                "å°è¯•ä½¿ç”¨å…¶ä»–AIæ¨¡å‹",
                "å‡çº§APIé…é¢"
            ]
        elif error_type == 'file_error':
            suggestions = [
                "æ£€æŸ¥æ–‡ä»¶è·¯å¾„æ˜¯å¦æ­£ç¡®",
                "ç¡®è®¤æ–‡ä»¶æƒé™æ˜¯å¦å¯è¯»",
                "æ£€æŸ¥æ–‡ä»¶æ˜¯å¦ä¸ºæœ‰æ•ˆçš„Pythonä»£ç ",
                "å°è¯•åˆ†æå…¶ä»–æ–‡ä»¶"
            ]
        else:
            suggestions = [
                "æŸ¥çœ‹è¯¦ç»†é”™è¯¯æ—¥å¿—",
                "å°è¯•é‡å¯åº”ç”¨ç¨‹åº",
                "è”ç³»æŠ€æœ¯æ”¯æŒ",
                "æ£€æŸ¥ç³»ç»Ÿèµ„æºæ˜¯å¦å……è¶³"
            ]

        return suggestions

    def _update_error_stats(self, error_info: dict):
        """æ›´æ–°é”™è¯¯ç»Ÿè®¡"""
        self.error_stats['total_errors'] += 1

        error_type = error_info['error_type'].lower()
        if 'network' in error_type or 'connection' in error_type:
            self.error_stats['network_errors'] += 1
        elif 'api' in error_type or 'auth' in error_type:
            self.error_stats['api_errors'] += 1
        elif 'timeout' in error_type:
            self.error_stats['timeout_errors'] += 1
        elif 'file' in error_type:
            self.error_stats['file_errors'] += 1

        # ä¿å­˜æœ€è¿‘çš„é”™è¯¯ä¿¡æ¯
        self.error_stats['recent_errors'].append(error_info)
        if len(self.error_stats['recent_errors']) > 10:
            self.error_stats['recent_errors'] = self.error_stats['recent_errors'][-10:]

    def _try_fallback_analysis(self, file_path: str, error_info: dict) -> Optional[dict]:
        """å°è¯•é™çº§åˆ†ææ¨¡å¼"""
        print("\nğŸ”„ å°è¯•é™çº§åˆ†ææ¨¡å¼...")

        try:
            # å¦‚æœæœ‰é™æ€åˆ†æç»“æœï¼Œæä¾›åŸºæœ¬çš„é™æ€åˆ†æ
            if self.error_handling_config['offline_mode_available']:
                print("ğŸ“Š å¯ç”¨ç¦»çº¿é™æ€åˆ†ææ¨¡å¼")
                return self._perform_static_analysis_fallback(file_path)

            # æä¾›åŸºæœ¬çš„æ–‡ä»¶ä¿¡æ¯åˆ†æ
            print("ğŸ“‹ æä¾›åŸºæœ¬æ–‡ä»¶ä¿¡æ¯åˆ†æ")
            return self._perform_basic_file_analysis(file_path)

        except Exception as fallback_error:
            print(f"âŒ é™çº§æ¨¡å¼ä¹Ÿå¤±è´¥: {fallback_error}")
            self.error_stats['failed_recoveries'] += 1
            return None

    def _perform_static_analysis_fallback(self, file_path: str) -> dict:
        """æ‰§è¡Œé™æ€åˆ†æé™çº§æ¨¡å¼"""
        try:
            from .static_coordinator import StaticAnalysisCoordinator
            from pathlib import Path

            # ä½¿ç”¨é™æ€åˆ†æå·¥å…·
            coordinator = StaticAnalysisCoordinator()
            result = coordinator.analyze_file(file_path)

            return {
                'success': True,
                'fallback_mode': 'static_analysis',
                'file_path': file_path,
                'analysis_type': 'static_fallback',
                'issues': [issue.to_dict() for issue in result.issues] if result.issues else [],
                'execution_time': result.execution_time if hasattr(result, 'execution_time') else 0,
                'message': f"ä½¿ç”¨äº†é™æ€åˆ†æé™çº§æ¨¡å¼ï¼Œå‘ç° {len(result.issues) if result.issues else 0} ä¸ªé—®é¢˜"
            }

        except Exception as e:
            return {
                'success': False,
                'fallback_mode': 'static_analysis',
                'error': str(e),
                'message': "é™æ€åˆ†æé™çº§æ¨¡å¼ä¹Ÿå¤±è´¥"
            }

    def _perform_basic_file_analysis(self, file_path: str) -> dict:
        """æ‰§è¡ŒåŸºæœ¬æ–‡ä»¶ä¿¡æ¯åˆ†æ"""
        try:
            from pathlib import Path
            import os

            file_path_obj = Path(file_path)
            if not file_path_obj.exists():
                return {
                    'success': False,
                    'fallback_mode': 'basic_info',
                    'error': 'File not found',
                    'message': "æ–‡ä»¶ä¸å­˜åœ¨"
                }

            # è·å–åŸºæœ¬æ–‡ä»¶ä¿¡æ¯
            stat = file_path_obj.stat()
            file_size = stat.st_size

            # è¯»å–æ–‡ä»¶å†…å®¹è¿›è¡ŒåŸºæœ¬åˆ†æ
            try:
                with open(file_path_obj, 'r', encoding='utf-8') as f:
                    content = f.read()
            except UnicodeDecodeError:
                # å¦‚æœUTF-8è§£ç å¤±è´¥ï¼Œå°è¯•å…¶ä»–ç¼–ç 
                try:
                    with open(file_path_obj, 'r', encoding='gbk') as f:
                        content = f.read()
                except:
                    content = ""

            line_count = len(content.splitlines()) if content else 0
            char_count = len(content) if content else 0

            # åŸºæœ¬çš„ä»£ç ç»“æ„åˆ†æ
            import_count = content.count('import ')
            class_count = content.count('class ')
            function_count = content.count('def ')

            return {
                'success': True,
                'fallback_mode': 'basic_info',
                'file_path': str(file_path_obj),
                'analysis_type': 'basic_fallback',
                'basic_stats': {
                    'file_size': file_size,
                    'line_count': line_count,
                    'character_count': char_count,
                    'import_count': import_count,
                    'class_count': class_count,
                    'function_count': function_count
                },
                'message': f"åŸºæœ¬æ–‡ä»¶ä¿¡æ¯: {line_count} è¡Œ, {class_count} ä¸ªç±», {function_count} ä¸ªå‡½æ•°"
            }

        except Exception as e:
            return {
                'success': False,
                'fallback_mode': 'basic_info',
                'error': str(e),
                'message': "åŸºæœ¬æ–‡ä»¶ä¿¡æ¯åˆ†æå¤±è´¥"
            }

    def _show_error_statistics(self):
        """æ˜¾ç¤ºé”™è¯¯ç»Ÿè®¡ä¿¡æ¯"""
        print("\nğŸš¨ é”™è¯¯ç»Ÿè®¡æŠ¥å‘Š")
        print("=" * 50)

        stats = self.error_stats
        print(f"ğŸ“Š æ€»é”™è¯¯æ•°: {stats['total_errors']}")
        print(f"ğŸŒ ç½‘ç»œé”™è¯¯: {stats['network_errors']}")
        print(f"ğŸ”Œ APIé”™è¯¯: {stats['api_errors']}")
        print(f"â±ï¸ è¶…æ—¶é”™è¯¯: {stats['timeout_errors']}")
        print(f"ğŸ“ æ–‡ä»¶é”™è¯¯: {stats['file_errors']}")
        print(f"âœ… æˆåŠŸæ¢å¤: {stats['successful_recoveries']}")
        print(f"âŒ æ¢å¤å¤±è´¥: {stats['failed_recoveries']}")

        if stats['total_errors'] > 0:
            recovery_rate = (stats['successful_recoveries'] / stats['total_errors']) * 100
            print(f"ğŸ“ˆ æ¢å¤æˆåŠŸç‡: {recovery_rate:.1f}%")

        # æ˜¾ç¤ºæœ€è¿‘çš„é”™è¯¯
        if stats['recent_errors']:
            print(f"\nğŸ“‹ æœ€è¿‘é”™è¯¯è®°å½•:")
            for i, error in enumerate(stats['recent_errors'][-3:], 1):
                print(f"  {i}. {error['timestamp']} - {error['error_type']}: {error['file_path']}")

        print()

    def _show_static_analysis_fallback_result(self, result: dict):
        """æ˜¾ç¤ºé™æ€åˆ†æé™çº§ç»“æœ"""
        print(f"\nğŸ“Š é™æ€åˆ†æç»“æœ (é™çº§æ¨¡å¼)")
        print("-" * 50)

        if hasattr(result, 'message'):
            print(f"ğŸ’¬ {result.message}")

        if hasattr(result, 'issues') and result.issues:
            print(f"\nğŸ” å‘ç°é—®é¢˜ ({len(result.issues)}ä¸ª):")

            # æŒ‰ä¸¥é‡ç¨‹åº¦åˆ†ç»„
            severity_groups = {}
            for issue in result.issues:
                severity = issue.get('severity', 'info')
                if severity not in severity_groups:
                    severity_groups[severity] = []
                severity_groups[severity].append(issue)

            for severity in ['error', 'warning', 'info']:
                if severity in severity_groups:
                    emoji = {'error': 'ğŸ”´', 'warning': 'ğŸŸ¡', 'info': 'ğŸ”µ'}[severity]
                    print(f"\n{emoji} {severity.upper()}çº§åˆ«é—®é¢˜ ({len(severity_groups[severity])}ä¸ª):")

                    for i, issue in enumerate(severity_groups[severity][:5], 1):
                        message = issue.get('message', 'No message')
                        line = issue.get('line', 'N/A')
                        tool = issue.get('tool', 'Unknown')
                        print(f"  {i}. ç¬¬{line}è¡Œ [{tool}]: {message}")

                    if len(severity_groups[severity]) > 5:
                        print(f"     ... è¿˜æœ‰ {len(severity_groups[severity]) - 5} ä¸ª{severity}çº§åˆ«é—®é¢˜")

        print(f"\nğŸ’¡ è¿™æ˜¯é™æ€åˆ†æç»“æœï¼Œå¯èƒ½ä¸å¦‚AIåˆ†æè¯¦ç»†ã€‚")
        print(f"ğŸ’¡ ç½‘ç»œæ¢å¤åå¯é‡æ–°åˆ†æè·å¾—æ›´æ·±å…¥çš„ç»“æœã€‚")
        print()

    def _show_basic_file_info_result(self, result: dict):
        """æ˜¾ç¤ºåŸºæœ¬æ–‡ä»¶ä¿¡æ¯ç»“æœ"""
        print(f"\nğŸ“‹ åŸºæœ¬æ–‡ä»¶ä¿¡æ¯ (é™çº§æ¨¡å¼)")
        print("-" * 50)

        if hasattr(result, 'message'):
            print(f"ğŸ’¬ {result.message}")

        if hasattr(result, 'basic_stats'):
            stats = result.basic_stats
            print(f"\nğŸ“Š æ–‡ä»¶ç»Ÿè®¡:")
            print(f"  ğŸ“ æ–‡ä»¶å¤§å°: {self._format_file_size(stats.get('file_size', 0))}")
            print(f"  ğŸ“ ä»£ç è¡Œæ•°: {stats.get('line_count', 0)}")
            print(f"  ğŸ”¤ å­—ç¬¦æ•°é‡: {stats.get('character_count', 0)}")
            print(f"  ğŸ“¦ å¯¼å…¥è¯­å¥: {stats.get('import_count', 0)}")
            print(f"  ğŸ—ï¸ ç±»å®šä¹‰: {stats.get('class_count', 0)}")
            print(f"  âš™ï¸ å‡½æ•°å®šä¹‰: {stats.get('function_count', 0)}")

            # åŸºæœ¬åˆ†æå»ºè®®
            if stats.get('function_count', 0) > 20:
                print(f"\nğŸ’¡ å»ºè®®: å‡½æ•°æ•°é‡è¾ƒå¤š({stats.get('function_count', 0)})ï¼Œå»ºè®®è€ƒè™‘æ¨¡å—åŒ–é‡æ„")
            if stats.get('class_count', 0) > 10:
                print(f"ğŸ’¡ å»ºè®®: ç±»æ•°é‡è¾ƒå¤š({stats.get('class_count', 0)})ï¼Œå»ºè®®æ£€æŸ¥å•ä¸€èŒè´£åŸåˆ™")
            if stats.get('import_count', 0) > 15:
                print(f"ğŸ’¡ å»ºè®®: å¯¼å…¥è¯­å¥è¾ƒå¤š({stats.get('import_count', 0)})ï¼Œå¯èƒ½å­˜åœ¨ä¾èµ–è€¦åˆ")

        print(f"\nğŸ’¡ è¿™æ˜¯åŸºæœ¬æ–‡ä»¶ä¿¡æ¯ï¼Œä»…æä¾›ä»£ç ç»“æ„ç»Ÿè®¡ã€‚")
        print(f"ğŸ’¡ ç½‘ç»œæ¢å¤åå¯é‡æ–°åˆ†æè·å¾—æ·±å…¥çš„ä»£ç è´¨é‡åˆ†æã€‚")
        print()

    def _load_static_analysis_reports(self, target_path: str) -> list:
        """åŠ è½½é™æ€åˆ†ææŠ¥å‘Š"""
        import glob
        import os
        from datetime import datetime
        from pathlib import Path

        if not self.static_analysis_integration['auto_load_reports']:
            return []

        reports = []
        current_time = datetime.now()

        # æœç´¢é™æ€åˆ†ææŠ¥å‘Š
        for pattern in self.static_analysis_integration['report_search_paths']:
            search_path = Path(target_path) / pattern if not os.path.isabs(pattern) else Path(pattern)

            if '*' in pattern:
                # é€šé…ç¬¦æœç´¢
                matching_files = list(Path(search_path.parent).glob(search_path.name))
            else:
                # ç›´æ¥è·¯å¾„
                if search_path.exists():
                    matching_files = [search_path]
                else:
                    matching_files = []

            for report_file in matching_files:
                try:
                    # æ£€æŸ¥æ–‡ä»¶å¹´é¾„
                    file_mtime = datetime.fromtimestamp(report_file.stat().st_mtime)
                    age_days = (current_time - file_mtime).days

                    if age_days <= self.static_analysis_integration['max_report_age_days']:
                        # è¯»å–æŠ¥å‘Šå†…å®¹
                        with open(report_file, 'r', encoding='utf-8') as f:
                            report_data = json.load(f)

                        # éªŒè¯æŠ¥å‘Šæ ¼å¼
                        if self._is_valid_static_report(report_data):
                            report_data['file_path'] = str(report_file)
                            report_data['age_days'] = age_days
                            reports.append(report_data)

                            # ç¼“å­˜æŠ¥å‘Š
                            self.static_analysis_integration['report_cache'][str(report_file)] = report_data

                except Exception as e:
                    self.logger.warning(f"Failed to load static report {report_file}: {e}")

        # æŒ‰å¹´é¾„æ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
        reports.sort(key=lambda x: x.get('age_days', float('inf')))

        self.static_analysis_integration['integrated_reports'] = reports
        return reports

    def _is_valid_static_report(self, report_data: dict) -> bool:
        """éªŒè¯æ˜¯å¦ä¸ºæœ‰æ•ˆçš„é™æ€åˆ†ææŠ¥å‘Š"""
        required_fields = ['target', 'files_analyzed', 'total_issues', 'files']

        if not all(field in report_data for field in required_fields):
            return False

        # æ£€æŸ¥æ–‡ä»¶ç»“æ„
        if 'files' in report_data and isinstance(report_data['files'], list):
            for file_info in report_data['files']:
                if not isinstance(file_info, dict):
                    return False
                if 'file_path' not in file_info:
                    return False

        return True

    def _get_static_analysis_for_file(self, file_path: str) -> Optional[dict]:
        """è·å–æŒ‡å®šæ–‡ä»¶çš„é™æ€åˆ†æç»“æœ"""
        target_path = Path(file_path)

        # ä»å·²åŠ è½½çš„æŠ¥å‘Šä¸­æŸ¥æ‰¾
        for report in self.static_analysis_integration['integrated_reports']:
            for file_info in report.get('files', []):
                report_file_path = Path(file_info.get('file_path', ''))

                # æ£€æŸ¥æ˜¯å¦ä¸ºåŒä¸€æ–‡ä»¶ï¼ˆç›¸å¯¹è·¯å¾„æˆ–ç»å¯¹è·¯å¾„åŒ¹é…ï¼‰
                if (report_file_path.name == target_path.name or
                    str(report_file_path) == str(target_path.resolve())):

                    # è¿”å›åŒ¹é…çš„æ–‡ä»¶ä¿¡æ¯
                    file_result = file_info.copy()
                    file_result['report_metadata'] = {
                        'report_target': report.get('target'),
                        'report_age_days': report.get('age_days', 0),
                        'total_issues_in_report': report.get('total_issues', 0)
                    }
                    return file_result

        return None

    def _integrate_static_analysis_into_context(self, file_path: str, context: dict) -> dict:
        """å°†é™æ€åˆ†æç»“æœé›†æˆåˆ°åˆ†æä¸Šä¸‹æ–‡ä¸­"""
        static_result = self._get_static_analysis_for_file(file_path)

        if not static_result:
            return context

        # åˆ›å»ºé›†æˆçš„ä¸Šä¸‹æ–‡
        integrated_context = context.copy()

        # æ·»åŠ é™æ€åˆ†æä¿¡æ¯åˆ°ä¸Šä¸‹æ–‡
        if 'static_analysis' not in integrated_context:
            integrated_context['static_analysis'] = {}

        integrated_context['static_analysis'][file_path] = {
            'issues_count': static_result.get('issues_count', 0),
            'execution_time': static_result.get('execution_time', 0),
            'issues_summary': static_result.get('summary', {}),
            'total_issues_in_report': static_result['report_metadata']['total_issues_in_report']
        }

        # æ·»åŠ é—®é¢˜åˆ—è¡¨ï¼ˆå¦‚æœæœ‰ï¼‰
        if 'issues' in static_result:
            # è¿‡æ»¤é«˜ä¼˜å…ˆçº§é—®é¢˜
            all_issues = static_result['issues']
            high_priority_issues = []

            for issue in all_issues:
                severity = issue.get('severity', 'info')
                if severity in ['error', 'warning']:
                    high_priority_issues.append(issue)

            # åªä¿ç•™å‰10ä¸ªé«˜ä¼˜å…ˆçº§é—®é¢˜
            integrated_context['static_analysis'][file_path]['high_priority_issues'] = high_priority_issues[:10]

        return integrated_context

    def _show_static_analysis_summary(self, file_path: str):
        """æ˜¾ç¤ºé™æ€åˆ†ææ‘˜è¦"""
        static_result = self._get_static_analysis_for_file(file_path)

        if not static_result:
            return

        print(f"\nğŸ“Š é™æ€åˆ†ææ‘˜è¦")
        print("-" * 40)

        issues_count = static_result.get('issues_count', 0)
        execution_time = static_result.get('execution_time', 0)
        total_report_issues = static_result['report_metadata']['total_issues_in_report']

        print(f"ğŸ“ æ–‡ä»¶: {Path(file_path).name}")
        print(f"ğŸ” å‘ç°é—®é¢˜: {issues_count} ä¸ª")
        print(f"â±ï¸ åˆ†æè€—æ—¶: {execution_time:.2f}ç§’")
        print(f"ğŸ“‹ æŠ¥å‘Šæ€»é—®é¢˜: {total_report_issues} ä¸ª")

        # æ˜¾ç¤ºé—®é¢˜ä¸¥é‡ç¨‹åº¦åˆ†å¸ƒ
        if 'summary' in static_result and 'severity_distribution' in static_result['summary']:
            severity_dist = static_result['summary']['severity_distribution']
            if severity_dist:
                print(f"\nğŸ“ˆ é—®é¢˜ä¸¥é‡ç¨‹åº¦:")
                for severity, count in severity_dist.items():
                    emoji = {'error': 'ğŸ”´', 'warning': 'ğŸŸ¡', 'info': 'ğŸ”µ'}.get(severity, 'âšª')
                    print(f"  {emoji} {severity}: {count} ä¸ª")

        # æ˜¾ç¤ºå‰å‡ ä¸ªé«˜ä¼˜å…ˆçº§é—®é¢˜
        if 'high_priority_issues' in static_result.get('static_analysis', {}).get(file_path, {}):
            high_issues = static_result['static_analysis'][file_path]['high_priority_issues']
            if high_issues:
                print(f"\nâš ï¸  é«˜ä¼˜å…ˆçº§é—®é¢˜:")
                for i, issue in enumerate(high_issues[:5], 1):
                    line = issue.get('line', 'N/A')
                    message = issue.get('message', 'No message')
                    tool = issue.get('tool', 'Unknown')
                    print(f"  {i}. ç¬¬{line}è¡Œ [{tool}]: {message}")

        print(f"\nğŸ’¡ AIå°†åŸºäºè¿™äº›é™æ€åˆ†æç»“æœæä¾›æ·±åº¦å»ºè®®")
        print()

    def _analyze_file_interactive(self, analyzer, file_path: str, context: dict, history: list) -> dict:
        """äº¤äº’å¼æ–‡ä»¶åˆ†æ"""
        from pathlib import Path
        import asyncio
        import time

        try:
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            full_path = Path(context['target']) / file_path
            if not full_path.exists():
                full_path = Path(file_path)  # å°è¯•ç»å¯¹è·¯å¾„

            if not full_path.exists():
                print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                print("ğŸ’¡ è¯·æ£€æŸ¥æ–‡ä»¶è·¯å¾„æ˜¯å¦æ­£ç¡®")
                return None

            # æ˜¾ç¤ºæ–‡ä»¶ä¿¡æ¯
            file_name = full_path.name
            file_size = self._format_file_size(full_path.stat().st_size)

            print(f"\nğŸ” å‡†å¤‡åˆ†ææ–‡ä»¶: {file_name}")
            print(f"ğŸ“„ æ–‡ä»¶å¤§å°: {file_size}")
            print(f"ğŸ“Š åˆ†æç±»å‹: {context['analysis_type']}")
            print(f"ğŸ• å¼€å§‹æ—¶é—´: {self._get_current_time()}")
            print("-" * 50)

            # æ˜¾ç¤ºåˆ†æè¿›åº¦
            self._show_analyzing_animation("AIæ­£åœ¨æ·±åº¦åˆ†æä»£ç ç»“æ„")

            start_time = time.time()

            # åˆ›å»ºåˆ†æè¯·æ±‚
            request = DeepAnalysisRequest(
                file_path=str(full_path),
                analysis_type=context['analysis_type'],
                context=context
            )

            # æ‰§è¡Œå¼‚æ­¥åˆ†æ
            result = asyncio.run(analyzer.analyze_file(request))

            execution_time = time.time() - start_time

            # æ˜¾ç¤ºåˆ†æç»“æœæ¨ªå¹…
            self._show_analysis_result_banner(result.success, file_name)

            if result.success:
                print(f"\nğŸ‰ åˆ†ææˆåŠŸå®Œæˆï¼")
                print(f"â±ï¸ æ‰§è¡Œæ—¶é—´: {execution_time:.2f}ç§’")

                if hasattr(result, 'model_used') and result.model_used:
                    print(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹: {result.model_used}")

                print(f"ğŸ“Š åˆ†æç±»å‹: {context['analysis_type']}")
                print("-" * 50)

                # æ˜¾ç¤ºåˆ†æç»“æœæ‘˜è¦
                if result.structured_analysis and result.structured_analysis.get('structured'):
                    self._show_enhanced_structured_result(result.structured_analysis)
                else:
                    # æ˜¾ç¤ºæ–‡æœ¬ç»“æœçš„æ‘˜è¦
                    self._show_text_result_preview(result.content)

                # æ›´æ–°ä¸Šä¸‹æ–‡
                context['current_file'] = str(full_path)
                context['previous_results'].append({
                    'file_path': str(full_path),
                    'analysis_type': context['analysis_type'],
                    'execution_time': execution_time,
                    'success': True,
                    'timestamp': self._get_current_time()
                })

                # è®°å½•åˆ°å¯¹è¯å†å²
                history.append({
                    'timestamp': self._get_current_time(),
                    'type': 'file_analysis',
                    'file_path': str(full_path),
                    'analysis_type': context['analysis_type'],
                    'result': result.to_dict(),
                    'execution_time': execution_time
                })

                print(f"\nğŸ’¡ æç¤º: ä½¿ç”¨ 'summary' æŸ¥çœ‹ä¼šè¯æ€»ç»“")
                print(f"ğŸ’¡ æç¤º: ä½¿ç”¨ 'export <filename>' å¯¼å‡ºå¯¹è¯å†å²")

                return result.to_dict()
            else:
                print(f"\nâŒ åˆ†æå¤±è´¥")
                error_msg = getattr(result, 'error', 'æœªçŸ¥é”™è¯¯')
                print(f"ğŸ”´ é”™è¯¯ä¿¡æ¯: {error_msg}")
                print(f"â±ï¸ è€—æ—¶: {execution_time:.2f}ç§’")

                # è®°å½•å¤±è´¥çš„åˆ†æ
                history.append({
                    'timestamp': self._get_current_time(),
                    'type': 'file_analysis',
                    'file_path': str(full_path),
                    'analysis_type': context['analysis_type'],
                    'success': False,
                    'error': error_msg,
                    'execution_time': execution_time
                })

                print(f"\nğŸ’¡ å»ºè®®:")
                print(f"  â€¢ æ£€æŸ¥æ–‡ä»¶æ˜¯å¦ä¸ºæœ‰æ•ˆçš„Pythonä»£ç ")
                print(f"  â€¢ ç¡®è®¤ç½‘ç»œè¿æ¥æ­£å¸¸")
                print(f"  â€¢ å°è¯•æ›´æ¢åˆ†æç±»å‹")

                return None

        except Exception as e:
            print(f"\nâŒ åˆ†æè¿‡ç¨‹ä¸­å‡ºç°å¼‚å¸¸")
            print(f"ğŸ”´ å¼‚å¸¸ä¿¡æ¯: {e}")
            self.logger.error(f"Interactive analysis failed: {e}")

            print(f"\nğŸ’¡ æ•…éšœæ’é™¤:")
            print(f"  â€¢ æ£€æŸ¥æ–‡ä»¶è·¯å¾„æ˜¯å¦æ­£ç¡®")
            print(f"  â€¢ ç¡®è®¤æ–‡ä»¶æƒé™å¯è¯»")
            print(f"  â€¢ æ£€æŸ¥ç³»ç»Ÿèµ„æºæ˜¯å¦å……è¶³")

            return None

    def _show_enhanced_structured_result(self, structured_result: dict):
        """æ˜¾ç¤ºå¢å¼ºçš„ç»“æ„åŒ–åˆ†æç»“æœ"""
        print("\nğŸ“Š ç»“æ„åŒ–åˆ†æç»“æœ:")
        print("ğŸ¯" * 35)
        print()

        # æ˜¾ç¤ºæ‘˜è¦
        if 'summary' in structured_result:
            print(f"ğŸ“ åˆ†ææ‘˜è¦:")
            print(f"   {structured_result['summary']}")
            print()

        # æ˜¾ç¤ºä»£ç è´¨é‡è¯„åˆ†
        if 'quality_score' in structured_result:
            score = structured_result['quality_score']
            emoji = self._get_score_emoji(score)
            print(f"ğŸ“ˆ ä»£ç è´¨é‡è¯„åˆ†: {emoji} {score}/10")
            print(f"   {self._get_score_description(score)}")
            print()

        # æ˜¾ç¤ºå¤æ‚åº¦åˆ†æ
        if 'complexity' in structured_result:
            complexity = structured_result['complexity']
            print(f"ğŸ”€ å¤æ‚åº¦åˆ†æ:")
            print(f"   åœˆå¤æ‚åº¦: {complexity.get('cyclomatic', 'N/A')}")
            print(f"   è®¤çŸ¥å¤æ‚åº¦: {complexity.get('cognitive', 'N/A')}")
            print(f"   å¤æ‚åº¦ç­‰çº§: {self._get_complexity_level(complexity.get('cyclomatic', 0))}")
            print()

        # æ˜¾ç¤ºé—®é¢˜åˆ†æ
        if 'issues' in structured_result and structured_result['issues']:
            issues = structured_result['issues']
            print(f"ğŸ” å‘ç°é—®é¢˜ ({len(issues)}ä¸ª):")
            print("-" * 40)

            # æŒ‰ä¸¥é‡ç¨‹åº¦åˆ†ç»„
            severity_groups = {}
            for issue in issues:
                severity = issue.get('severity', 'info')
                if severity not in severity_groups:
                    severity_groups[severity] = []
                severity_groups[severity].append(issue)

            for severity in ['error', 'warning', 'info']:
                if severity in severity_groups:
                    emoji = {'error': 'ğŸ”´', 'warning': 'ğŸŸ¡', 'info': 'ğŸ”µ'}[severity]
                    print(f"\n{emoji} {severity.upper()}çº§åˆ«é—®é¢˜ ({len(severity_groups[severity])}ä¸ª):")

                    for i, issue in enumerate(severity_groups[severity][:5], 1):
                        message = issue.get('message', 'No message')
                        line = issue.get('line', 'N/A')
                        print(f"  {i}. ç¬¬{line}è¡Œ: {message}")

                        if issue.get('suggestion'):
                            print(f"     ğŸ’¡ å»ºè®®: {issue['suggestion']}")

                    if len(severity_groups[severity]) > 5:
                        print(f"     ... è¿˜æœ‰ {len(severity_groups[severity]) - 5} ä¸ª{severity}çº§åˆ«é—®é¢˜")

        # æ˜¾ç¤ºæ”¹è¿›å»ºè®®
        if 'recommendations' in structured_result and structured_result['recommendations']:
            recommendations = structured_result['recommendations']
            print(f"\nğŸ’¡ æ”¹è¿›å»ºè®® ({len(recommendations)}æ¡):")
            print("-" * 40)

            for i, rec in enumerate(recommendations[:5], 1):
                priority = rec.get('priority', 'medium')
                priority_emoji = {'high': 'ğŸ”´', 'medium': 'ğŸŸ¡', 'low': 'ğŸŸ¢'}.get(priority, 'âšª')
                print(f"  {i}. {priority_emoji} {rec.get('text', rec)}")

                if rec.get('effort'):
                    effort = rec.get('effort')
                    print(f"     å®æ–½éš¾åº¦: {effort}")

            if len(recommendations) > 5:
                print(f"     ... è¿˜æœ‰ {len(recommendations) - 5} æ¡å»ºè®®")

        print()

    def _show_text_result_preview(self, content: str):
        """æ˜¾ç¤ºæ–‡æœ¬ç»“æœé¢„è§ˆ"""
        print(f"\nğŸ“‹ AIåˆ†æç»“æœ:")
        print("-" * 50)

        if not content:
            print("âš ï¸ åˆ†æç»“æœä¸ºç©º")
            return

        # æ˜¾ç¤ºå‰500å­—ç¬¦çš„é¢„è§ˆ
        preview_length = 500
        if len(content) <= preview_length:
            print(content)
        else:
            preview = content[:preview_length]
            print(preview)
            print(f"\n... (è¿˜æœ‰ {len(content) - preview_length} å­—ç¬¦ï¼Œè¯¦è§å®Œæ•´æŠ¥å‘Š)")

        print("-" * 50)

    def _get_score_emoji(self, score: float) -> str:
        """æ ¹æ®è¯„åˆ†è·å–emoji"""
        if score >= 8.5:
            return "ğŸŸ¢"
        elif score >= 7.0:
            return "ğŸŸ¡"
        elif score >= 5.0:
            return "ğŸŸ "
        else:
            return "ğŸ”´"

    def _get_score_description(self, score: float) -> str:
        """è·å–è¯„åˆ†æè¿°"""
        if score >= 9.0:
            return "ä¼˜ç§€ - ä»£ç è´¨é‡éå¸¸é«˜"
        elif score >= 8.0:
            return "è‰¯å¥½ - ä»£ç è´¨é‡è¾ƒé«˜"
        elif score >= 7.0:
            return "ä¸­ç­‰ - ä»£ç è´¨é‡ä¸€èˆ¬"
        elif score >= 6.0:
            return "è¾ƒå·® - éœ€è¦ä¸€äº›æ”¹è¿›"
        else:
            return "å¾ˆå·® - éœ€è¦é‡å¤§æ”¹è¿›"

    def _get_complexity_level(self, complexity: int) -> str:
        """è·å–å¤æ‚åº¦ç­‰çº§"""
        if complexity <= 5:
            return "ç®€å•"
        elif complexity <= 10:
            return "ä¸­ç­‰"
        elif complexity <= 15:
            return "å¤æ‚"
        else:
            return "éå¸¸å¤æ‚"

    def _show_structured_result(self, structured_result: dict):
        """æ˜¾ç¤ºç»“æ„åŒ–åˆ†æç»“æœï¼ˆä¿ç•™åŸæ–¹æ³•å…¼å®¹æ€§ï¼‰"""
        self._show_enhanced_structured_result(structured_result)

    def _show_analysis_summary(self, context: dict):
        """æ˜¾ç¤ºåˆ†ææ€»ç»“"""
        print("\nğŸ“Š åˆ†ææ€»ç»“")
        print("=" * 40)
        print(f"ğŸ“ ç›®æ ‡è·¯å¾„: {context['target']}")
        print(f"ğŸ” å½“å‰åˆ†æç±»å‹: {context['analysis_type']}")
        print(f"ğŸ“„ å·²åˆ†ææ–‡ä»¶: {len(context['previous_results'])}")

        if context['previous_results']:
            total_time = sum(r.get('execution_time', 0) for r in context['previous_results'])
            successful_files = len([r for r in context['previous_results'] if r.get('success', False)])
            print(f"âœ… æˆåŠŸåˆ†æ: {successful_files}/{len(context['previous_results'])} æ–‡ä»¶")
            print(f"â±ï¸ æ€»è€—æ—¶: {total_time:.2f}ç§’")

            # æ˜¾ç¤ºæ–‡ä»¶åˆ—è¡¨
            print("\nğŸ“‹ å·²åˆ†ææ–‡ä»¶åˆ—è¡¨:")
            for i, result in enumerate(context['previous_results'], 1):
                file_path = result.get('file_path', 'Unknown')
                success = result.get('success', False)
                status = "âœ…" if success else "âŒ"
                print(f"  {i}. {status} {Path(file_path).name}")

        print()

    def _export_conversation(self, history: list, export_file: str):
        """å¯¼å‡ºå¯¹è¯å†å²"""
        try:
            export_path = Path(export_file)
            if not export_path.suffix:
                export_path = export_path.with_suffix('.json')

            export_data = {
                'export_time': self._get_current_time(),
                'total_entries': len(history),
                'conversation_history': history
            }

            with open(export_path, 'w', encoding='utf-8') as f:
                json.dump(export_data, f, indent=2, ensure_ascii=False)

            print(f"âœ… å¯¹è¯å†å²å·²å¯¼å‡ºåˆ°: {export_path}")

        except Exception as e:
            print(f"âŒ å¯¼å‡ºå¤±è´¥: {e}")

    def _generate_contextual_response(self, user_input: str) -> str:
        """ç”Ÿæˆä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„å¯¹è¯å“åº”"""
        user_lower = user_input.lower()

        # è·å–ä¸Šä¸‹æ–‡ä¿¡æ¯
        context_summary = self.conversation_context.get_context_summary()
        recent_context = self.conversation_context.get_recent_context(3)
        session_stats = self.conversation_context.get_session_stats()

        # æ£€æŸ¥æ˜¯å¦åœ¨è¯¢é—®åˆ†æç›¸å…³çš„é—®é¢˜
        if any(keyword in user_lower for keyword in ['å¦‚ä½•åˆ†æ', 'æ€ä¹ˆåˆ†æ', 'åˆ†æä»€ä¹ˆ', 'å¦‚ä½•ä½¿ç”¨']):
            current_type = self.conversation_context.analysis_context['analysis_type']
            return f"æ‚¨å¯ä»¥ä½¿ç”¨ 'analyze <æ–‡ä»¶è·¯å¾„>' å‘½ä»¤æ¥åˆ†ææŒ‡å®šæ–‡ä»¶ã€‚å½“å‰åˆ†æç±»å‹æ˜¯ {current_type}ï¼Œå¯ä»¥ä½¿ç”¨ 'type <ç±»å‹>' å‘½ä»¤æ›´æ”¹ã€‚{context_summary}"

        # æ£€æŸ¥æ˜¯å¦åœ¨è¯¢é—®è¿›åº¦ç›¸å…³çš„é—®é¢˜
        elif any(keyword in user_lower for keyword in ['è¿›åº¦', 'å¦‚ä½•äº†', 'çŠ¶æ€', 'ç»Ÿè®¡']):
            if session_stats['total_analyses'] > 0:
                success_rate = (session_stats['successful_analyses'] / session_stats['total_analyses']) * 100
                return f"ä¼šè¯ç»Ÿè®¡: å·²åˆ†æ {session_stats['files_analyzed']} ä¸ªæ–‡ä»¶ï¼ŒæˆåŠŸç‡ {success_rate:.1f}%ï¼Œæ€»è€—æ—¶ {session_stats['total_time']:.1f}ç§’ã€‚{context_summary}"
            else:
                return f"è¿˜æ²¡æœ‰å¼€å§‹åˆ†æã€‚ä½¿ç”¨ 'analyze <æ–‡ä»¶è·¯å¾„>' å‘½ä»¤å¼€å§‹åˆ†ææ–‡ä»¶ã€‚"

        # æ£€æŸ¥æ˜¯å¦åœ¨è¯¢é—®æ–‡ä»¶ç›¸å…³çš„é—®é¢˜
        elif any(keyword in user_lower for keyword in ['æ–‡ä»¶', 'ä»£ç ', 'project', 'åˆšæ‰']):
            if session_stats['total_analyses'] > 0:
                most_used_type = session_stats['most_used_analysis_type']
                return f"æˆ‘ä»¬å·²ç»åˆ†æäº† {session_stats['files_analyzed']} ä¸ªæ–‡ä»¶ï¼Œæœ€å¸¸ç”¨çš„åˆ†æç±»å‹æ˜¯ {most_used_type}ã€‚{context_summary}"
            else:
                return "è¿˜æ²¡æœ‰åˆ†æä»»ä½•æ–‡ä»¶ã€‚ä½¿ç”¨ 'analyze <æ–‡ä»¶è·¯å¾„>' å‘½ä»¤å¼€å§‹åˆ†æã€‚"

        # æ£€æŸ¥æ˜¯å¦åœ¨è¯¢é—®ç±»å‹ç›¸å…³çš„é—®é¢˜
        elif any(keyword in user_lower for keyword in ['ç±»å‹', 'type', 'åˆ†æç±»å‹', 'å“ªç§åˆ†æ']):
            current_type = self.conversation_context.analysis_context['analysis_type']
            most_used_type = session_stats['most_used_analysis_type']
            return f"å½“å‰åˆ†æç±»å‹æ˜¯ {current_type}ï¼Œæ‚¨æœ€å¸¸ç”¨çš„ç±»å‹æ˜¯ {most_used_type}ã€‚æ”¯æŒçš„ç±»å‹åŒ…æ‹¬: comprehensive, security, performance, architecture, code_review, refactoringã€‚"

        # æ£€æŸ¥æ˜¯å¦åœ¨è¯¢é—®å†å²ç›¸å…³çš„é—®é¢˜
        elif any(keyword in user_lower for keyword in ['å†å²', 'è®°å½•', 'ä¹‹å‰', 'åˆšæ‰è¯´ä»€ä¹ˆ']):
            if recent_context:
                last_entry = recent_context[-1]
                if last_entry.get('type') == 'file_analysis':
                    file_name = Path(last_entry.get('file_path', 'unknown')).name
                    return f"æœ€è¿‘æˆ‘ä»¬åˆ†æäº† {file_name}ã€‚ä½¿ç”¨ 'summary' æŸ¥çœ‹å®Œæ•´ä¼šè¯æ€»ç»“ã€‚"
                else:
                    return f"æˆ‘ä»¬åˆšæ‰è®¨è®ºäº†: {last_entry.get('user_input', 'æŸä¸ªè¯é¢˜')}"
            else:
                return "è¿™æ˜¯æˆ‘ä»¬å¯¹è¯çš„å¼€å§‹ã€‚ä½¿ç”¨ 'analyze <æ–‡ä»¶è·¯å¾„>' å¼€å§‹åˆ†ææ–‡ä»¶ã€‚"

        # æ£€æŸ¥æ˜¯å¦åœ¨è¯¢é—®å»ºè®®ç›¸å…³çš„é—®é¢˜
        elif any(keyword in user_lower for keyword in ['å»ºè®®', 'æ¨è', 'åº”è¯¥', 'ä¸‹ä¸€æ­¥']):
            if session_stats['total_analyses'] > 0:
                return f"åŸºäºæ‚¨çš„åˆ†æå†å²ï¼Œå»ºè®®æ‚¨: 1) ç»§ç»­åˆ†æå…¶ä»–é‡è¦æ–‡ä»¶ 2) å°è¯•ä¸åŒçš„åˆ†æç±»å‹ 3) ä½¿ç”¨ 'summary' æŸ¥çœ‹ä¼šè¯æ€»ç»“ 4) ä½¿ç”¨ 'export' ä¿å­˜å¯¹è¯å†å²ã€‚{context_summary}"
            else:
                return "å»ºè®®æ‚¨å…ˆåˆ†æä¸€äº›å…³é”®æ–‡ä»¶ï¼Œæ¯”å¦‚ä¸»ç¨‹åºæ–‡ä»¶æˆ–æ ¸å¿ƒæ¨¡å—ã€‚ä½¿ç”¨ 'analyze <æ–‡ä»¶è·¯å¾„>' å¼€å§‹ã€‚"

        # æ£€æŸ¥æ„Ÿè°¢æˆ–ç»“æŸç›¸å…³çš„è¯é¢˜
        elif any(keyword in user_lower for keyword in ['è°¢è°¢', 'æ„Ÿè°¢', 'å¥½çš„', 'å¯ä»¥', 'æ˜ç™½äº†']):
            return "ä¸å®¢æ°”ï¼å¦‚æœæ‚¨éœ€è¦æ›´å¤šå¸®åŠ©ï¼Œéšæ—¶å‘Šè¯‰æˆ‘ã€‚æˆ‘å¯ä»¥å¸®æ‚¨åˆ†æä»£ç ã€è§£ç­”é—®é¢˜æˆ–æä¾›å»ºè®®ã€‚"

        # é»˜è®¤å“åº”ï¼ˆåŒ…å«ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼‰
        else:
            return f"æˆ‘æ˜¯AIä»£ç åˆ†æåŠ©æ‰‹ï¼Œå¯ä»¥å¸®åŠ©æ‚¨è¿›è¡Œæ·±åº¦ä»£ç åˆ†æã€‚{context_summary} è¾“å…¥ 'help' æŸ¥çœ‹å¯ç”¨å‘½ä»¤ï¼Œæˆ–ç›´æ¥ä½¿ç”¨ 'analyze <æ–‡ä»¶è·¯å¾„>' å¼€å§‹åˆ†ææ–‡ä»¶ã€‚"

    def _export_conversation_with_context(self, export_file: str):
        """ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨å¯¼å‡ºå¯¹è¯å†å²"""
        try:
            export_path = Path(export_file)

            # æ ¹æ®æ–‡ä»¶æ‰©å±•åç¡®å®šå¯¼å‡ºæ ¼å¼
            if not export_path.suffix:
                # é»˜è®¤å¯¼å‡ºä¸ºJSON
                export_path = export_path.with_suffix('.json')
                format_type = 'json'
            else:
                format_type = export_path.suffix.lower().lstrip('.')

            # æ”¯æŒçš„å¯¼å‡ºæ ¼å¼
            if format_type not in ['json', 'md', 'markdown', 'txt', 'html']:
                print(f"âŒ ä¸æ”¯æŒçš„å¯¼å‡ºæ ¼å¼: {format_type}")
                print("ğŸ’¡ æ”¯æŒçš„æ ¼å¼: json, md/markdown, txt, html")
                return

            # ç¡®ä¿ç›®å½•å­˜åœ¨
            export_path.parent.mkdir(parents=True, exist_ok=True)

            # æ ¹æ®æ ¼å¼è°ƒç”¨ç›¸åº”çš„å¯¼å‡ºæ–¹æ³•
            if format_type == 'json':
                self._export_as_json(export_path)
            elif format_type in ['md', 'markdown']:
                self._export_as_markdown(export_path)
            elif format_type == 'txt':
                self._export_as_text(export_path)
            elif format_type == 'html':
                self._export_as_html(export_path)

            print(f"âœ… å¯¹è¯å†å²å·²å¯¼å‡ºåˆ°: {export_path}")
            print(f"ğŸ“Š å¯¼å‡ºæ ¼å¼: {format_type.upper()}")
            print(f"ğŸ“ è®°å½•æ•°é‡: {len(self.conversation_context.conversation_history)} æ¡")

        except Exception as e:
            print(f"âŒ å¯¼å‡ºå¤±è´¥: {e}")
            self.logger.error(f"Export failed: {e}")

    def _export_as_json(self, export_path: Path):
        """å¯¼å‡ºä¸ºJSONæ ¼å¼"""
        export_data = {
            'export_info': {
                'export_time': self._get_current_time(),
                'target': self.conversation_context.target,
                'total_entries': len(self.conversation_context.conversation_history),
                'session_duration': self.conversation_context._get_session_duration(),
                'version': '2.0',
                'format': 'json'
            },
            'session_stats': self.conversation_context.get_session_stats(),
            'analysis_context': self.conversation_context.analysis_context,
            'conversation_history': self.conversation_context.conversation_history,
            'user_preferences': self.conversation_context.analysis_context['preferences'],
            'user_patterns': self.conversation_context.analysis_context['user_patterns'],
            'metadata': {
                'generator': 'AIDefectDetector Deep Analysis',
                'platform': 'CLI Interactive Mode',
                'max_context_length': self.conversation_context.max_context_length
            }
        }

        with open(export_path, 'w', encoding='utf-8') as f:
            json.dump(export_data, f, indent=2, ensure_ascii=False, default=str)

    def _export_as_markdown(self, export_path: Path):
        """å¯¼å‡ºä¸ºMarkdownæ ¼å¼"""
        with open(export_path, 'w', encoding='utf-8') as f:
            f.write("# AIæ·±åº¦åˆ†æå¯¹è¯å†å²\n\n")

            # å¯¼å‡ºä¿¡æ¯
            f.write("## å¯¼å‡ºä¿¡æ¯\n\n")
            f.write(f"- **å¯¼å‡ºæ—¶é—´**: {self._get_current_time()}\n")
            f.write(f"- **åˆ†æç›®æ ‡**: `{self.conversation_context.target}`\n")
            f.write(f"- **ä¼šè¯æ—¶é•¿**: {self._format_duration(self.conversation_context._get_session_duration())}\n")
            f.write(f"- **å¯¹è¯è®°å½•**: {len(self.conversation_context.conversation_history)} æ¡\n\n")

            # ä¼šè¯ç»Ÿè®¡
            stats = self.conversation_context.get_session_stats()
            f.write("## ä¼šè¯ç»Ÿè®¡\n\n")
            f.write(f"- **åˆ†ææ–‡ä»¶æ•°**: {stats['files_analyzed']}\n")
            f.write(f"- **æˆåŠŸåˆ†æ**: {stats['successful_analyses']}\n")
            f.write(f"- **å¤±è´¥åˆ†æ**: {stats['failed_analyses']}\n")
            f.write(f"- **æ€»è€—æ—¶**: {stats['total_time']:.2f}ç§’\n")
            f.write(f"- **æœ€å¸¸ç”¨ç±»å‹**: {stats['most_used_analysis_type']}\n\n")

            # å¯¹è¯å†å²
            f.write("## å¯¹è¯å†å²\n\n")

            for i, entry in enumerate(self.conversation_context.conversation_history, 1):
                f.write(f"### å¯¹è¯ {i} - {entry.get('timestamp', 'N/A')}\n\n")

                if entry.get('type') == 'file_analysis':
                    file_path = entry.get('file_path', 'Unknown')
                    analysis_type = entry.get('analysis_type', 'Unknown')
                    success = entry.get('success', False)
                    exec_time = entry.get('execution_time', 0)

                    f.write(f"**ç±»å‹**: æ–‡ä»¶åˆ†æ\n")
                    f.write(f"**æ–‡ä»¶**: `{file_path}`\n")
                    f.write(f"**åˆ†æç±»å‹**: {analysis_type}\n")
                    f.write(f"**ç»“æœ**: {'âœ… æˆåŠŸ' if success else 'âŒ å¤±è´¥'}\n")
                    f.write(f"**è€—æ—¶**: {exec_time:.2f}ç§’\n\n")

                    if entry.get('result') and entry['result'].get('success'):
                        result = entry['result']
                        if result.get('content'):
                            content = result['content']
                            preview = content[:300] + "..." if len(content) > 300 else content
                            f.write("**åˆ†æç»“æœé¢„è§ˆ**:\n")
                            f.write("```\n")
                            f.write(preview)
                            f.write("\n```\n\n")

                    if entry.get('result') and not entry['result'].get('success'):
                        error_msg = entry['result'].get('error', 'æœªçŸ¥é”™è¯¯')
                        f.write(f"**é”™è¯¯ä¿¡æ¯**: {error_msg}\n\n")

                else:
                    # æ™®é€šå¯¹è¯
                    user_input = entry.get('user_input', '')
                    ai_response = entry.get('ai_response', '')

                    f.write("**ç”¨æˆ·**: ")
                    f.write(f"{user_input}\n\n")
                    f.write("**AI**: ")
                    f.write(f"{ai_response}\n\n")

                f.write("---\n\n")

    def _export_as_text(self, export_path: Path):
        """å¯¼å‡ºä¸ºçº¯æ–‡æœ¬æ ¼å¼"""
        with open(export_path, 'w', encoding='utf-8') as f:
            f.write("AIæ·±åº¦åˆ†æå¯¹è¯å†å²\n")
            f.write("=" * 50 + "\n\n")

            # å¯¼å‡ºä¿¡æ¯
            f.write("å¯¼å‡ºä¿¡æ¯:\n")
            f.write(f"  å¯¼å‡ºæ—¶é—´: {self._get_current_time()}\n")
            f.write(f"  åˆ†æç›®æ ‡: {self.conversation_context.target}\n")
            f.write(f"  ä¼šè¯æ—¶é•¿: {self._format_duration(self.conversation_context._get_session_duration())}\n")
            f.write(f"  å¯¹è¯è®°å½•: {len(self.conversation_context.conversation_history)} æ¡\n\n")

            # ä¼šè¯ç»Ÿè®¡
            stats = self.conversation_context.get_session_stats()
            f.write("ä¼šè¯ç»Ÿè®¡:\n")
            f.write(f"  åˆ†ææ–‡ä»¶æ•°: {stats['files_analyzed']}\n")
            f.write(f"  æˆåŠŸåˆ†æ: {stats['successful_analyses']}\n")
            f.write(f"  å¤±è´¥åˆ†æ: {stats['failed_analyses']}\n")
            f.write(f"  æ€»è€—æ—¶: {stats['total_time']:.2f}ç§’\n")
            f.write(f"  æœ€å¸¸ç”¨ç±»å‹: {stats['most_used_analysis_type']}\n\n")

            # å¯¹è¯å†å²
            f.write("å¯¹è¯å†å²:\n")
            f.write("-" * 50 + "\n\n")

            for i, entry in enumerate(self.conversation_context.conversation_history, 1):
                f.write(f"[{i}] {entry.get('timestamp', 'N/A')}\n")

                if entry.get('type') == 'file_analysis':
                    f.write(f"  ç±»å‹: æ–‡ä»¶åˆ†æ\n")
                    f.write(f"  æ–‡ä»¶: {entry.get('file_path', 'Unknown')}\n")
                    f.write(f"  åˆ†æç±»å‹: {entry.get('analysis_type', 'Unknown')}\n")
                    f.write(f"  ç»“æœ: {'æˆåŠŸ' if entry.get('success', False) else 'å¤±è´¥'}\n")
                    f.write(f"  è€—æ—¶: {entry.get('execution_time', 0):.2f}ç§’\n")
                else:
                    f.write(f"  ç”¨æˆ·: {entry.get('user_input', '')}\n")
                    f.write(f"  AI: {entry.get('ai_response', '')}\n")

                f.write("\n")

    def _export_as_html(self, export_path: Path):
        """å¯¼å‡ºä¸ºHTMLæ ¼å¼"""
        html_template = """<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AIæ·±åº¦åˆ†æå¯¹è¯å†å²</title>
    <style>
        body {{ font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif; line-height: 1.6; margin: 0; padding: 20px; background: #f5f5f5; }}
        .container {{ max-width: 800px; margin: 0 auto; background: white; padding: 30px; border-radius: 10px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); }}
        h1 {{ color: #333; border-bottom: 3px solid #007bff; padding-bottom: 10px; }}
        h2 {{ color: #007bff; margin-top: 30px; }}
        h3 {{ color: #495057; margin-top: 25px; }}
        .stats {{ background: #f8f9fa; padding: 15px; border-radius: 5px; margin: 20px 0; }}
        .conversation {{ margin: 20px 0; }}
        .message {{ padding: 15px; margin: 10px 0; border-radius: 8px; }}
        .user {{ background: #e3f2fd; border-left: 4px solid #2196f3; }}
        .ai {{ background: #f3e5f5; border-left: 4px solid #9c27b0; }}
        .analysis {{ background: #e8f5e8; border-left: 4px solid #4caf50; }}
        .file-info {{ background: #fff3cd; padding: 10px; border-radius: 5px; margin: 10px 0; }}
        .success {{ color: #28a745; }}
        .failure {{ color: #dc3545; }}
        code {{ background: #f1f1f1; padding: 2px 4px; border-radius: 3px; }}
        pre {{ background: #f8f9fa; padding: 15px; border-radius: 5px; overflow-x: auto; }}
        .timestamp {{ color: #6c757d; font-size: 0.9em; }}
    </style>
</head>
<body>
    <div class="container">
        <h1>ğŸ§  AIæ·±åº¦åˆ†æå¯¹è¯å†å²</h1>

        <div class="stats">
            <h2>ğŸ“Š ä¼šè¯ç»Ÿè®¡</h2>
            <p><strong>å¯¼å‡ºæ—¶é—´</strong>: {export_time}</p>
            <p><strong>åˆ†æç›®æ ‡</strong>: <code>{target}</code></p>
            <p><strong>ä¼šè¯æ—¶é•¿</strong>: {session_duration}</p>
            <p><strong>å¯¹è¯è®°å½•</strong>: {total_entries} æ¡</p>
            <p><strong>åˆ†ææ–‡ä»¶æ•°</strong>: {files_analyzed}</p>
            <p><strong>æˆåŠŸåˆ†æ</strong>: {successful_analyses}</p>
            <p><strong>å¤±è´¥åˆ†æ</strong>: {failed_analyses}</p>
            <p><strong>æ€»è€—æ—¶</strong>: {total_time:.2f}ç§’</p>
        </div>

        <div class="conversation">
            <h2>ğŸ’¬ å¯¹è¯å†å²</h2>
            {conversation_content}
        </div>
    </div>
</body>
</html>"""

        # ç”Ÿæˆå¯¹è¯å†…å®¹
        conversation_html = ""
        for i, entry in enumerate(self.conversation_context.conversation_history, 1):
            timestamp = entry.get('timestamp', 'N/A')

            if entry.get('type') == 'file_analysis':
                file_path = entry.get('file_path', 'Unknown')
                analysis_type = entry.get('analysis_type', 'Unknown')
                success = entry.get('success', False)
                exec_time = entry.get('execution_time', 0)

                conversation_html += f"""
                <div class="message analysis">
                    <h3>ğŸ“Š æ–‡ä»¶åˆ†æ #{i} <span class="timestamp">({timestamp})</span></h3>
                    <div class="file-info">
                        <p><strong>æ–‡ä»¶</strong>: <code>{file_path}</code></p>
                        <p><strong>åˆ†æç±»å‹</strong>: {analysis_type}</p>
                        <p><strong>ç»“æœ</strong>: <span class="{'success' if success else 'failure'}">{'âœ… æˆåŠŸ' if success else 'âŒ å¤±è´¥'}</span></p>
                        <p><strong>è€—æ—¶</strong>: {exec_time:.2f}ç§’</p>
                    </div>
                </div>"""

                if entry.get('result') and entry['result'].get('success'):
                    result = entry['result']
                    if result.get('content'):
                        content = result['content'][:500] + "..." if len(result['content']) > 500 else result['content']
                        conversation_html += f"""
                        <div class="message ai">
                            <p><strong>AIåˆ†æç»“æœ</strong>:</p>
                            <pre>{content}</pre>
                        </div>"""

            else:
                user_input = entry.get('user_input', '')
                ai_response = entry.get('ai_response', '')

                conversation_html += f"""
                <div class="conversation">
                    <div class="message user">
                        <p><strong>ç”¨æˆ·</strong> <span class="timestamp">({timestamp})</span>:</p>
                        <p>{user_input}</p>
                    </div>
                    <div class="message ai">
                        <p><strong>AIåŠ©æ‰‹</strong>:</p>
                        <p>{ai_response}</p>
                    </div>
                </div>"""

        # å¡«å……æ¨¡æ¿
        stats = self.conversation_context.get_session_stats()
        html_content = html_template.format(
            export_time=self._get_current_time(),
            target=self.conversation_context.target,
            session_duration=self._format_duration(self.conversation_context._get_session_duration()),
            total_entries=len(self.conversation_context.conversation_history),
            files_analyzed=stats['files_analyzed'],
            successful_analyses=stats['successful_analyses'],
            failed_analyses=stats['failed_analyses'],
            total_time=stats['total_time'],
            conversation_content=conversation_html
        )

        with open(export_path, 'w', encoding='utf-8') as f:
            f.write(html_content)

    def _format_duration(self, seconds: float) -> str:
        """æ ¼å¼åŒ–æ—¶é•¿æ˜¾ç¤º"""
        if seconds < 60:
            return f"{seconds:.1f}ç§’"
        elif seconds < 3600:
            minutes = seconds / 60
            return f"{minutes:.1f}åˆ†é’Ÿ"
        else:
            hours = seconds / 3600
            return f"{hours:.1f}å°æ—¶"

    def _generate_conversational_response(self, user_input: str, context: dict) -> str:
        """ç”Ÿæˆå¯¹è¯å“åº”ï¼ˆä¿ç•™åŸæ–¹æ³•å…¼å®¹æ€§ï¼‰"""
        return self._generate_contextual_response(user_input)

    def _save_deep_analysis_result(self, result: dict, output_file: str):
        """ä¿å­˜æ·±åº¦åˆ†æç»“æœ"""
        try:
            output_path = Path(output_file)
            output_path.parent.mkdir(parents=True, exist_ok=True)

            if output_file.endswith('.json'):
                with open(output_path, 'w', encoding='utf-8') as f:
                    json.dump(result, f, indent=2, ensure_ascii=False, default=str)
            elif output_file.endswith('.md'):
                self._save_deep_analysis_markdown(result, output_path)
            else:
                # ç®€å•æ–‡æœ¬æ ¼å¼
                with open(output_path, 'w', encoding='utf-8') as f:
                    self._save_deep_analysis_text(result, f)

            print(f"ğŸ“„ åˆ†æç»“æœå·²ä¿å­˜åˆ°: {output_path}")

        except Exception as e:
            print(f"âŒ ä¿å­˜ç»“æœå¤±è´¥: {e}")

    def _save_deep_analysis_text(self, result: dict, file):
        """ä¿å­˜æ·±åº¦åˆ†ææ–‡æœ¬ç»“æœ"""
        file.write(f"æ·±åº¦åˆ†ææŠ¥å‘Š\n")
        file.write("=" * 50 + "\n")
        file.write(f"ç›®æ ‡è·¯å¾„: {result['target']}\n")
        file.write(f"åˆ†ææ¨¡å¼: {result['mode']}\n")
        file.write(f"åˆ†ææ–‡ä»¶æ•°: {result['files_analyzed']}\n")
        file.write(f"æ€»æ‰§è¡Œæ—¶é—´: {result['total_execution_time']:.2f}ç§’\n")
        file.write(f"å¯¹è¯è½®æ¬¡: {len(result.get('conversation_history', []))}\n\n")

        if result.get('conversation_history'):
            file.write("å¯¹è¯å†å²:\n")
            file.write("-" * 30 + "\n")
            for i, entry in enumerate(result['conversation_history'], 1):
                file.write(f"[{entry.get('timestamp', 'N/A')}]\n")
                if entry.get('type') == 'file_analysis':
                    file.write(f"åˆ†ææ–‡ä»¶: {entry.get('file_path', 'Unknown')}\n")
                    file.write(f"åˆ†æç±»å‹: {entry.get('analysis_type', 'Unknown')}\n")
                    file.write(f"ç»“æœ: {'æˆåŠŸ' if entry.get('result', {}).get('success') else 'å¤±è´¥'}\n")
                else:
                    file.write(f"ç”¨æˆ·: {entry.get('user_input', 'N/A')}\n")
                    file.write(f"AI: {entry.get('ai_response', 'N/A')}\n")
                file.write("-" * 30 + "\n")

    def _save_deep_analysis_markdown(self, result: dict, file_path: Path):
        """ä¿å­˜æ·±åº¦åˆ†æMarkdownç»“æœ"""
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write("# æ·±åº¦åˆ†ææŠ¥å‘Š\n\n")
            f.write(f"**ç›®æ ‡è·¯å¾„**: `{result['target']}`\n")
            f.write(f"**åˆ†ææ¨¡å¼**: {result['mode']}\n")
            f.write(f"**åˆ†ææ–‡ä»¶æ•°**: {result['files_analyzed']}\n")
            f.write(f"**æ€»æ‰§è¡Œæ—¶é—´**: {result['total_execution_time']:.2f}ç§’\n")
            f.write(f"**å¯¹è¯è½®æ¬¡**: {len(result.get('conversation_history', []))}\n\n")

            if result.get('conversation_history'):
                f.write("## å¯¹è¯å†å²\n\n")
                for i, entry in enumerate(result['conversation_history'], 1):
                    f.write(f"### å¯¹è¯ {i} - {entry.get('timestamp', 'N/A')}\n\n")
                    if entry.get('type') == 'file_analysis':
                        f.write(f"**æ–‡ä»¶**: `{entry.get('file_path', 'Unknown')}`\n")
                        f.write(f"**åˆ†æç±»å‹**: {entry.get('analysis_type', 'Unknown')}\n")
                        f.write(f"**ç»“æœ**: {'âœ… æˆåŠŸ' if entry.get('result', {}).get('success') else 'âŒ å¤±è´¥'}\n")
                        if entry.get('result', {}).get('content'):
                            content = entry['result']['content']
                            preview = content[:200] + "..." if len(content) > 200 else content
                            f.write(f"**å†…å®¹é¢„è§ˆ**:\n```\n{preview}\n```\n")
                    else:
                        f.write(f"**ç”¨æˆ·**: {entry.get('user_input', 'N/A')}\n")
                        f.write(f"**AI**: {entry.get('ai_response', 'N/A')}\n")
                    f.write("\n")

    def _show_enhanced_startup_banner(self, target: str):
        """æ˜¾ç¤ºå¢å¼ºçš„å¯åŠ¨æ¨ªå¹…"""
        from pathlib import Path
        import os

        # æ¸…å±ï¼ˆå¯é€‰ï¼‰
        if os.name == 'posix':  # Unix/Linux/macOS
            os.system('clear')
        else:  # Windows
            os.system('cls')

        print("\n" + "="*70)
        print("ğŸ§  AIæ·±åº¦åˆ†æåŠ©æ‰‹ - äº¤äº’å¼å¯¹è¯æ¨¡å¼".center(70))
        print("="*70)
        print()

        # æ˜¾ç¤ºç›®æ ‡ä¿¡æ¯
        target_path = Path(target)
        print(f"ğŸ“ åˆ†æç›®æ ‡: {target_path.resolve()}")
        print(f"ğŸ“Š ç›®æ ‡ç±»å‹: {'æ–‡ä»¶' if target_path.is_file() else 'ç›®å½•'}")

        if target_path.is_file():
            print(f"ğŸ“„ æ–‡ä»¶å¤§å°: {self._format_file_size(target_path.stat().st_size)}")
        elif target_path.is_dir():
            # ç»Ÿè®¡Pythonæ–‡ä»¶æ•°é‡
            try:
                py_files = list(target_path.rglob("*.py"))
                print(f"ğŸ Pythonæ–‡ä»¶: {len(py_files)} ä¸ª")
            except:
                print("ğŸ Pythonæ–‡ä»¶: æ— æ³•ç»Ÿè®¡")

        print(f"ğŸ• å¯åŠ¨æ—¶é—´: {self._get_current_time()}")
        print()

    def _show_initialization_progress(self):
        """æ˜¾ç¤ºåˆå§‹åŒ–è¿›åº¦"""
        import sys
        import time

        steps = [
            "ğŸ”§ åˆå§‹åŒ–AIæ¨¡å‹...",
            "ğŸ”— è¿æ¥åˆ°åˆ†ææœåŠ¡...",
            "ğŸ“š åŠ è½½çŸ¥è¯†åº“...",
            "ğŸ¯ é…ç½®åˆ†æå¼•æ“...",
            "âœ… ç³»ç»Ÿå°±ç»ªï¼"
        ]

        print("ğŸš€ ç³»ç»Ÿåˆå§‹åŒ–ä¸­:")
        for i, step in enumerate(steps, 1):
            # æ˜¾ç¤ºè¿›åº¦æ¡
            progress_bar = self._create_progress_bar(i, len(steps), width=30)
            print(f"  [{progress_bar}] {step}", end='\r')
            sys.stdout.flush()
            time.sleep(0.3)  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´

        print()  # æ¢è¡Œ
        print()

    def _create_progress_bar(self, current: int, total: int, width: int = 40) -> str:
        """åˆ›å»ºè¿›åº¦æ¡"""
        filled = int(width * current / total)
        bar = 'â–ˆ' * filled + 'â–‘' * (width - filled)
        return bar

    def _format_file_size(self, size_bytes: int) -> str:
        """æ ¼å¼åŒ–æ–‡ä»¶å¤§å°"""
        for unit in ['B', 'KB', 'MB', 'GB']:
            if size_bytes < 1024.0:
                return f"{size_bytes:.1f} {unit}"
            size_bytes /= 1024.0
        return f"{size_bytes:.1f} TB"

    def _show_enhanced_session_info(self, context: ConversationContext, analyzer):
        """æ˜¾ç¤ºå¢å¼ºçš„ä¼šè¯ä¿¡æ¯"""
        from pathlib import Path

        print("ğŸ“Š ä¼šè¯é…ç½®ä¿¡æ¯:")
        print("-" * 50)
        print(f"  ğŸ¯ é»˜è®¤åˆ†æç±»å‹: {context.analysis_context['analysis_type']}")
        print(f"  ğŸ“ åˆ†æç›®æ ‡: {context.target}")
        print(f"  ğŸ’¬ å¯¹è¯æ¨¡å¼: æ™ºèƒ½ä¸Šä¸‹æ–‡æ„ŸçŸ¥")
        print(f"  ğŸ§  ä¸Šä¸‹æ–‡å®¹é‡: {context.max_context_length} è½®å¯¹è¯")
        print(f"  ğŸ’¾ è‡ªåŠ¨ä¿å­˜: å¯ç”¨")

        if self.output_file:
            print(f"  ğŸ“„ è¾“å‡ºæ–‡ä»¶: {Path(self.output_file).name}")
        else:
            print(f"  ğŸ“„ è¾“å‡ºæ–‡ä»¶: è‡ªåŠ¨ç”Ÿæˆæ—¶é—´æˆ³æ–‡ä»¶")

        try:
            supported_types = analyzer.get_supported_analysis_types()
            print(f"  ğŸ”§ æ”¯æŒçš„åˆ†æç±»å‹: {', '.join(supported_types)}")
        except:
            print(f"  ğŸ”§ æ”¯æŒçš„åˆ†æç±»å‹: comprehensive, security, performance, architecture, code_review, refactoring")

        print(f"  ğŸ• ä¼šè¯å¼€å§‹: {context.session_start}")
        print()

    def _show_session_info(self, context: dict, analyzer):
        """æ˜¾ç¤ºä¼šè¯ä¿¡æ¯ï¼ˆä¿ç•™åŸæ–¹æ³•å…¼å®¹æ€§ï¼‰"""
        # å°†æ—§æ ¼å¼è½¬æ¢ä¸ºæ–°çš„ConversationContextæ ¼å¼
        temp_context = ConversationContext(context.get('target', 'unknown'))
        temp_context.analysis_context = context
        self._show_enhanced_session_info(temp_context, analyzer)

    def _show_analyzing_animation(self, message: str = "AIæ­£åœ¨åˆ†æ"):
        """æ˜¾ç¤ºåˆ†æåŠ¨ç”»"""
        import sys
        import time
        from threading import Thread

        def animate():
            chars = "â ‹â ™â ¹â ¸â ¼â ´â ¦â §â ‡â "
            for char in chars:
                sys.stdout.write(f'\r{char} {message}...')
                sys.stdout.flush()
                time.sleep(0.1)

        # åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œåº”è¯¥å¯åŠ¨åŠ¨ç”»çº¿ç¨‹
        # ä¸ºäº†ç®€åŒ–ï¼Œæˆ‘ä»¬åªæ˜¾ç¤ºé™æ€æ¶ˆæ¯
        print(f"â³ {message}...")

    def _show_analysis_result_banner(self, success: bool, file_name: str = ""):
        """æ˜¾ç¤ºåˆ†æç»“æœæ¨ªå¹…"""
        if success:
            print(f"\nâœ… åˆ†æå®Œæˆ: {file_name}")
            print("ğŸ‰" * 20)
        else:
            print(f"\nâŒ åˆ†æå¤±è´¥: {file_name}")
            print("ğŸ’¥" * 20)

    def _get_current_time(self) -> str:
        """è·å–å½“å‰æ—¶é—´å­—ç¬¦ä¸²"""
        from datetime import datetime
        return datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    def _show_fix_analysis_help(self):
        """æ˜¾ç¤ºä¿®å¤åˆ†æå¸®åŠ©ä¿¡æ¯"""
        print("\nğŸ“– ä¿®å¤åˆ†æå¸®åŠ©")
        print("-" * 40)
        print("å¯ç”¨å‘½ä»¤:")
        print("  help                    - æ˜¾ç¤ºæ­¤å¸®åŠ©ä¿¡æ¯")
        print("  scan                    - æ‰«æç›®æ ‡è·¯å¾„ä¸­çš„é—®é¢˜")
        print("  fix <file_path>         - ä¿®å¤æŒ‡å®šæ–‡ä»¶")
        print("  batch fix               - æ‰¹é‡ä¿®å¤æ‰€æœ‰æ–‡ä»¶")
        print("  auto confirm            - åˆ‡æ¢è‡ªåŠ¨ç¡®è®¤æ¨¡å¼")
        print("  status                  - æ˜¾ç¤ºå½“å‰çŠ¶æ€")
        print("  history                 - æ˜¾ç¤ºä¿®å¤å†å²")
        print("  export <filename>       - å¯¼å‡ºä¿®å¤ä¼šè¯")
        print("  quit/exit/q             - é€€å‡ºä¿®å¤æ¨¡å¼")
        print("\nä¿®å¤æµç¨‹:")
        print("  1. ä½¿ç”¨ 'scan' æ‰«ææ–‡ä»¶é—®é¢˜")
        print("  2. ä½¿ç”¨ 'fix <file>' ä¿®å¤æŒ‡å®šæ–‡ä»¶")
        print("  3. æˆ–è€…ä½¿ç”¨ 'batch fix' æ‰¹é‡ä¿®å¤")
        print("  4. ä½¿ç”¨ 'auto confirm' å¯ç”¨è‡ªåŠ¨ç¡®è®¤")
        print("  5. ä½¿ç”¨ 'history' æŸ¥çœ‹ä¿®å¤å†å²")
        print("\nç¤ºä¾‹:")
        print("  scan")
        print("  fix src/main.py")
        print("  batch fix")
        print("  export fix_session.json")
        print()

    def _scan_files_for_issues(self, static_coordinator, target: str, session: dict):
        """æ‰«ææ–‡ä»¶é—®é¢˜"""
        from pathlib import Path

        print(f"ğŸ” å¼€å§‹æ‰«æ: {target}")
        print("â³ æ­£åœ¨åˆ†ææ–‡ä»¶...")

        try:
            target_path = Path(target)
            if not target_path.exists():
                print(f"âŒ ç›®æ ‡è·¯å¾„ä¸å­˜åœ¨: {target}")
                return

            # æ”¶é›†Pythonæ–‡ä»¶
            python_files = []
            if target_path.is_file() and target_path.suffix == '.py':
                python_files.append(str(target_path))
            elif target_path.is_dir():
                python_files.extend([str(f) for f in target_path.rglob("*.py")])

            if not python_files:
                print("âš ï¸ æœªæ‰¾åˆ°Pythonæ–‡ä»¶")
                return

            print(f"ğŸ“ æ‰¾åˆ° {len(python_files)} ä¸ªPythonæ–‡ä»¶")

            # æ‰«æé—®é¢˜
            session['scanned_files'] = python_files
            session['identified_issues'] = {}
            total_issues = 0

            for i, file_path in enumerate(python_files, 1):
                print(f"ğŸ” [{i}/{len(python_files)}] æ‰«æ: {Path(file_path).name}")

                try:
                    result = static_coordinator.analyze_file(file_path)
                    issues = result.issues if result.issues else []

                    if issues:
                        session['identified_issues'][file_path] = [issue.to_dict() for issue in issues]
                        total_issues += len(issues)
                        print(f"  å‘ç° {len(issues)} ä¸ªé—®é¢˜")
                    else:
                        print(f"  âœ… æ— é—®é¢˜")

                except Exception as e:
                    print(f"  âŒ æ‰«æå¤±è´¥: {e}")

            print(f"\nğŸ“Š æ‰«æå®Œæˆ:")
            print(f"  ğŸ“„ æ‰«ææ–‡ä»¶: {len(python_files)}")
            print(f"  ğŸ” æœ‰é—®é¢˜æ–‡ä»¶: {len(session['identified_issues'])}")
            print(f"  âš ï¸ æ€»é—®é¢˜æ•°: {total_issues}")

            if session['identified_issues']:
                print(f"\nğŸ“‹ é—®é¢˜æ–‡ä»¶åˆ—è¡¨:")
                for file_path, issues in list(session['identified_issues'].items())[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                    print(f"  ğŸ“„ {Path(file_path).name}: {len(issues)} ä¸ªé—®é¢˜")
                if len(session['identified_issues']) > 5:
                    print(f"  ... è¿˜æœ‰ {len(session['identified_issues']) - 5} ä¸ªæ–‡ä»¶")

        except Exception as e:
            print(f"âŒ æ‰«æå¤±è´¥: {e}")
            self.logger.error(f"Scan failed: {e}")

    def _fix_file_interactive(self, fix_coordinator, file_path: str, session: dict) -> dict:
        """äº¤äº’å¼æ–‡ä»¶ä¿®å¤"""
        from pathlib import Path
        import asyncio

        try:
            # æ£€æŸ¥æ–‡ä»¶è·¯å¾„
            full_path = Path(session['target']) / file_path
            if not full_path.exists():
                full_path = Path(file_path)  # å°è¯•ç»å¯¹è·¯å¾„

            if not full_path.exists():
                print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                return None

            # è·å–é—®é¢˜åˆ—è¡¨
            file_key = str(full_path)
            if file_key not in session['identified_issues']:
                print(f"âš ï¸ æ–‡ä»¶ {file_path} æ²¡æœ‰æ‰«æåˆ°é—®é¢˜ï¼Œä½¿ç”¨ 'scan' å…ˆæ‰«æ")
                return None

            issues = session['identified_issues'][file_key]
            print(f"ğŸ”§ å¼€å§‹ä¿®å¤æ–‡ä»¶: {full_path}")
            print(f"ğŸ“‹ å‘ç°é—®é¢˜: {len(issues)} ä¸ª")

            # æ˜¾ç¤ºé—®é¢˜æ‘˜è¦
            for i, issue in enumerate(issues[:3], 1):
                message = issue.get('message', 'No message')
                line = issue.get('line', 'N/A')
                severity = issue.get('severity', 'unknown')
                print(f"  {i}. [{severity}] ç¬¬{line}è¡Œ: {message}")
            if len(issues) > 3:
                print(f"  ... è¿˜æœ‰ {len(issues) - 3} ä¸ªé—®é¢˜")

            # åˆ›å»ºä¿®å¤è¯·æ±‚
            request = FixAnalysisRequest(
                file_path=str(full_path),
                issues=issues,
                analysis_type="security",
                confirmation_required=not session['auto_confirm'],
                backup_enabled=True,
                auto_fix=session['auto_confirm']
            )

            print("â³ AIæ­£åœ¨ç”Ÿæˆä¿®å¤æ–¹æ¡ˆ...")

            # æ‰§è¡Œä¿®å¤
            result = asyncio.run(fix_coordinator.process_fix_request(request))

            if result.success:
                print(f"âœ… ä¿®å¤å®Œæˆ (è€—æ—¶: {result.total_time:.2f}ç§’)")
                print(f"ğŸ”§ å®Œæˆé˜¶æ®µ: {', '.join(result.stages_completed)}")

                if result.execution_result:
                    applied_fixes = len(result.execution_result.applied_suggestions)
                    print(f"ğŸ“ åº”ç”¨ä¿®å¤: {applied_fixes} ä¸ª")

                return result.to_dict()
            else:
                print(f"âŒ ä¿®å¤å¤±è´¥: {result.error_message}")
                return result.to_dict() if result else None

        except Exception as e:
            print(f"âŒ ä¿®å¤è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            self.logger.error(f"Interactive fix failed: {e}")
            return None

    def _batch_fix_interactive(self, fix_coordinator, session: dict) -> dict:
        """æ‰¹é‡ä¿®å¤äº¤äº’"""
        import asyncio

        if not session['identified_issues']:
            print("âš ï¸ æ²¡æœ‰æ‰«æåˆ°é—®é¢˜ï¼Œè¯·å…ˆä½¿ç”¨ 'scan' å‘½ä»¤")
            return None

        total_files = len(session['identified_issues'])
        total_issues = sum(len(issues) for issues in session['identified_issues'].values())

        print(f"ğŸ”§ å‡†å¤‡æ‰¹é‡ä¿®å¤:")
        print(f"ğŸ“ æ–‡ä»¶æ•°é‡: {total_files}")
        print(f"âš ï¸ é—®é¢˜æ€»æ•°: {total_issues}")
        print(f"ğŸ”§ è‡ªåŠ¨ç¡®è®¤: {'å¯ç”¨' if session['auto_confirm'] else 'ç¦ç”¨'}")

        if not session['auto_confirm']:
            print("\nâš ï¸ æ‰¹é‡ä¿®å¤å°†ä¿®æ”¹å¤šä¸ªæ–‡ä»¶ï¼Œå»ºè®®å…ˆå¯ç”¨ 'auto confirm'")
            choice = input("æ˜¯å¦ç»§ç»­? (y/n): ").strip().lower()
            if choice not in ['y', 'yes']:
                print("âŒ å–æ¶ˆæ‰¹é‡ä¿®å¤")
                return None

        print("\nâ³ å¼€å§‹æ‰¹é‡ä¿®å¤...")

        try:
            # åˆ›å»ºæ‰¹é‡ä¿®å¤è¯·æ±‚
            requests = []
            for file_path, issues in session['identified_issues'].items():
                request = FixAnalysisRequest(
                    file_path=file_path,
                    issues=issues,
                    analysis_type="security",
                    confirmation_required=not session['auto_confirm'],
                    backup_enabled=True,
                    auto_fix=session['auto_confirm']
                )
                requests.append(request)

            # æ‰§è¡Œæ‰¹é‡ä¿®å¤
            result = fix_coordinator.process_batch_fix_requests(requests)

            # æ˜¾ç¤ºç»“æœ
            print(f"\nğŸ“Š æ‰¹é‡ä¿®å¤å®Œæˆ:")
            print(f"âœ… æˆåŠŸæ–‡ä»¶: {result.successful_files}/{result.total_files}")
            print(f"â±ï¸ æ€»è€—æ—¶: {result.total_time:.2f}ç§’")
            print(f"ğŸ“ æ‘˜è¦: {result.summary}")

            if result.process_results:
                print(f"\nğŸ“‹ è¯¦ç»†ç»“æœ:")
                for process_result in result.process_results[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                    file_name = Path(process_result.file_path).name
                    status = "âœ…" if process_result.success else "âŒ"
                    print(f"  {status} {file_name} ({process_result.total_time:.2f}s)")
                if len(result.process_results) > 5:
                    print(f"  ... è¿˜æœ‰ {len(result.process_results) - 5} ä¸ªæ–‡ä»¶")

            return result.__dict__ if hasattr(result, '__dict__') else {'batch_result': str(result)}

        except Exception as e:
            print(f"âŒ æ‰¹é‡ä¿®å¤å¤±è´¥: {e}")
            self.logger.error(f"Batch fix failed: {e}")
            return None

    def _show_fix_status(self, session: dict):
        """æ˜¾ç¤ºä¿®å¤çŠ¶æ€"""
        print("\nğŸ“Š ä¿®å¤ä¼šè¯çŠ¶æ€")
        print("=" * 40)
        print(f"ğŸ“ ç›®æ ‡è·¯å¾„: {session['target']}")
        print(f"ğŸ” å·²æ‰«ææ–‡ä»¶: {len(session['scanned_files'])}")
        print(f"âš ï¸ å‘ç°é—®é¢˜æ–‡ä»¶: {len(session['identified_issues'])}")
        print(f"ğŸ”§ ä¿®å¤å°è¯•: {len(session['fix_history'])}")
        print(f"âœ… æˆåŠŸä¿®å¤: {len([f for f in session['fix_history'] if f.get('success', False)])}")
        print(f"ğŸ¤– è‡ªåŠ¨ç¡®è®¤: {'å¯ç”¨' if session['auto_confirm'] else 'ç¦ç”¨'}")

        if session['identified_issues']:
            total_issues = sum(len(issues) for issues in session['identified_issues'].values())
            print(f"\nğŸ“‹ é—®é¢˜ç»Ÿè®¡:")
            print(f"  ğŸ” æ€»é—®é¢˜æ•°: {total_issues}")
            print(f"  ğŸ“ æœ‰é—®é¢˜æ–‡ä»¶: {len(session['identified_issues'])}")

            # æ˜¾ç¤ºæ–‡ä»¶åˆ—è¡¨
            print(f"\nğŸ“„ å¾…ä¿®å¤æ–‡ä»¶:")
            for file_path, issues in list(session['identified_issues'].items())[:5]:
                if not any(f.get('file_path') == file_path for f in session['fix_history']):
                    file_name = Path(file_path).name
                    print(f"  ğŸ”§ {file_name}: {len(issues)} ä¸ªé—®é¢˜")
            if len(session['identified_issues']) > 5:
                print(f"  ... è¿˜æœ‰ {len(session['identified_issues']) - 5} ä¸ªæ–‡ä»¶")

        print()

    def _show_fix_history(self, session: dict):
        """æ˜¾ç¤ºä¿®å¤å†å²"""
        print("\nğŸ“œ ä¿®å¤å†å²")
        print("=" * 40)

        if not session['fix_history']:
            print("ğŸ“ æš‚æ— ä¿®å¤è®°å½•")
            print()

        for i, fix_record in enumerate(session['fix_history'], 1):
            file_name = Path(fix_record.get('file_path', 'Unknown')).name
            success = fix_record.get('success', False)
            time_taken = fix_record.get('total_time', 0)
            stages = fix_record.get('stages_completed', [])

            print(f"{i}. {'âœ…' if success else 'âŒ'} {file_name}")
            print(f"   è€—æ—¶: {time_taken:.2f}ç§’")
            print(f"   é˜¶æ®µ: {', '.join(stages)}")

            if not success and fix_record.get('error_message'):
                print(f"   é”™è¯¯: {fix_record['error_message']}")

            print()

    def _export_fix_session(self, session: dict, export_file: str):
        """å¯¼å‡ºä¿®å¤ä¼šè¯"""
        try:
            export_path = Path(export_file)
            if not export_path.suffix:
                export_path = export_path.with_suffix('.json')

            export_data = {
                'export_time': self._get_current_time(),
                'session': session,
                'summary': {
                    'target': session['target'],
                    'files_scanned': len(session['scanned_files']),
                    'issues_found': sum(len(issues) for issues in session['identified_issues'].values()),
                    'fixes_attempted': len(session['fix_history']),
                    'successful_fixes': len([f for f in session['fix_history'] if f.get('success', False)])
                }
            }

            with open(export_path, 'w', encoding='utf-8') as f:
                json.dump(export_data, f, indent=2, ensure_ascii=False, default=str)

            print(f"âœ… ä¿®å¤ä¼šè¯å·²å¯¼å‡ºåˆ°: {export_path}")

        except Exception as e:
            print(f"âŒ å¯¼å‡ºå¤±è´¥: {e}")

    def _generate_fix_response(self, user_input: str, session: dict) -> str:
        """ç”Ÿæˆä¿®å¤å“åº”"""
        user_lower = user_input.lower()

        # æ£€æŸ¥æ˜¯å¦åœ¨è¯¢é—®ä¿®å¤ç›¸å…³çš„é—®é¢˜
        if any(keyword in user_lower for keyword in ['å¦‚ä½•ä¿®å¤', 'æ€ä¹ˆä¿®å¤', 'ä¿®å¤ä»€ä¹ˆ']):
            if session['identified_issues']:
                return f"å‘ç°äº† {len(session['identified_issues'])} ä¸ªæ–‡ä»¶æœ‰é—®é¢˜ã€‚ä½¿ç”¨ 'fix <æ–‡ä»¶è·¯å¾„>' ä¿®å¤æŒ‡å®šæ–‡ä»¶ï¼Œæˆ– 'batch fix' æ‰¹é‡ä¿®å¤ã€‚"
            else:
                return "è¿˜æ²¡æœ‰æ‰«æåˆ°é—®é¢˜ã€‚è¯·å…ˆä½¿ç”¨ 'scan' å‘½ä»¤æ‰«ææ–‡ä»¶ã€‚"

        # æ£€æŸ¥æ˜¯å¦åœ¨è¯¢é—®çŠ¶æ€ç›¸å…³çš„é—®é¢˜
        elif any(keyword in user_lower for keyword in ['çŠ¶æ€', 'è¿›åº¦', 'å¦‚ä½•äº†']):
            scanned = len(session['scanned_files'])
            issues = len(session['identified_issues'])
            fixed = len([f for f in session['fix_history'] if f.get('success', False)])
            return f"å·²æ‰«æ {scanned} ä¸ªæ–‡ä»¶ï¼Œå‘ç° {issues} ä¸ªæ–‡ä»¶æœ‰é—®é¢˜ï¼ŒæˆåŠŸä¿®å¤ {fixed} ä¸ªæ–‡ä»¶ã€‚"

        # æ£€æŸ¥æ˜¯å¦åœ¨è¯¢é—®è‡ªåŠ¨ç¡®è®¤ç›¸å…³çš„é—®é¢˜
        elif any(keyword in user_lower for keyword in ['è‡ªåŠ¨', 'auto', 'ç¡®è®¤']):
            status = "å¯ç”¨" if session['auto_confirm'] else "ç¦ç”¨"
            return f"è‡ªåŠ¨ç¡®è®¤å½“å‰{status}ã€‚ä½¿ç”¨ 'auto confirm' å‘½ä»¤å¯ä»¥åˆ‡æ¢çŠ¶æ€ã€‚"

        # é»˜è®¤å“åº”
        else:
            return "æˆ‘æ˜¯AIä»£ç ä¿®å¤åŠ©æ‰‹ï¼Œå¯ä»¥å¸®åŠ©æ‚¨æ‰«æå’Œä¿®å¤ä»£ç é—®é¢˜ã€‚è¾“å…¥ 'help' æŸ¥çœ‹å¯ç”¨å‘½ä»¤ï¼Œæˆ–ä½¿ç”¨ 'scan' å¼€å§‹æ‰«æé—®é¢˜ã€‚"

    def _save_fix_analysis_result(self, result: dict, output_file: str):
        """ä¿å­˜ä¿®å¤åˆ†æç»“æœ"""
        try:
            output_path = Path(output_file)
            output_path.parent.mkdir(parents=True, exist_ok=True)

            if output_file.endswith('.json'):
                with open(output_path, 'w', encoding='utf-8') as f:
                    json.dump(result, f, indent=2, ensure_ascii=False, default=str)
            elif output_file.endswith('.md'):
                self._save_fix_analysis_markdown(result, output_path)
            else:
                # ç®€å•æ–‡æœ¬æ ¼å¼
                with open(output_path, 'w', encoding='utf-8') as f:
                    self._save_fix_analysis_text(result, f)

            print(f"ğŸ“„ ä¿®å¤ç»“æœå·²ä¿å­˜åˆ°: {output_path}")

        except Exception as e:
            print(f"âŒ ä¿å­˜ç»“æœå¤±è´¥: {e}")

    def _save_fix_analysis_text(self, result: dict, file):
        """ä¿å­˜ä¿®å¤åˆ†ææ–‡æœ¬ç»“æœ"""
        file.write(f"ä¿®å¤åˆ†ææŠ¥å‘Š\n")
        file.write("=" * 50 + "\n")
        file.write(f"ç›®æ ‡è·¯å¾„: {result['target']}\n")
        file.write(f"åˆ†ææ¨¡å¼: {result['mode']}\n")
        file.write(f"æ‰«ææ–‡ä»¶æ•°: {result['files_scanned']}\n")
        file.write(f"å‘ç°é—®é¢˜æ•°: {result['total_issues_found']}\n")
        file.write(f"ä¿®å¤å°è¯•æ•°: {result['fixes_attempted']}\n")
        file.write(f"æˆåŠŸä¿®å¤æ•°: {result['successful_fixes']}\n\n")

        session = result.get('fix_session', {})
        if session.get('fix_history'):
            file.write("ä¿®å¤å†å²:\n")
            file.write("-" * 30 + "\n")
            for i, fix_record in enumerate(session['fix_history'], 1):
                file_path = fix_record.get('file_path', 'Unknown')
                success = fix_record.get('success', False)
                file.write(f"{i}. {'âœ…' if success else 'âŒ'} {Path(file_path).name}\n")
                file.write(f"   çŠ¶æ€: {'æˆåŠŸ' if success else 'å¤±è´¥'}\n")
                if not success and fix_record.get('error_message'):
                    file.write(f"   é”™è¯¯: {fix_record['error_message']}\n")
                file.write("-" * 30 + "\n")

    def _save_fix_analysis_markdown(self, result: dict, file_path: Path):
        """ä¿å­˜ä¿®å¤åˆ†æMarkdownç»“æœ"""
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write("# ä¿®å¤åˆ†ææŠ¥å‘Š\n\n")
            f.write(f"**ç›®æ ‡è·¯å¾„**: `{result['target']}`\n")
            f.write(f"**åˆ†ææ¨¡å¼**: {result['mode']}\n")
            f.write(f"**æ‰«ææ–‡ä»¶æ•°**: {result['files_scanned']}\n")
            f.write(f"**å‘ç°é—®é¢˜æ•°**: {result['total_issues_found']}\n")
            f.write(f"**ä¿®å¤å°è¯•æ•°**: {result['fixes_attempted']}\n")
            f.write(f"**æˆåŠŸä¿®å¤æ•°**: {result['successful_fixes']}\n\n")

            session = result.get('fix_session', {})

            # é—®é¢˜ç»Ÿè®¡
            if session.get('identified_issues'):
                f.write("## å‘ç°é—®é¢˜ç»Ÿè®¡\n\n")
                f.write("| æ–‡ä»¶ | é—®é¢˜æ•° |\n")
                f.write("|------|--------|\n")
                for file_path, issues in session['identified_issues'].items():
                    file_name = Path(file_path).name
                    f.write(f"| `{file_name}` | {len(issues)} |\n")
                f.write("\n")

            # ä¿®å¤å†å²
            if session.get('fix_history'):
                f.write("## ä¿®å¤å†å²\n\n")
                for i, fix_record in enumerate(session['fix_history'], 1):
                    file_name = Path(fix_record.get('file_path', 'Unknown')).name
                    success = fix_record.get('success', False)
                    status_icon = "âœ…" if success else "âŒ"

                    f.write(f"### {i}. {status_icon} {file_name}\n\n")
                    f.write(f"- **çŠ¶æ€**: {'æˆåŠŸ' if success else 'å¤±è´¥'}\n")
                    f.write(f"- **è€—æ—¶**: {fix_record.get('total_time', 0):.2f}ç§’\n")
                    f.write(f"- **å®Œæˆé˜¶æ®µ**: {', '.join(fix_record.get('stages_completed', []))}\n")

                    if not success and fix_record.get('error_message'):
                        f.write(f"- **é”™è¯¯**: {fix_record['error_message']}\n")

                    f.write("\n")

    # ===== T026-010: é«˜çº§æ·±åº¦åˆ†æç¼“å­˜æœºåˆ¶ =====

    def initialize_advanced_cache(self):
        """åˆå§‹åŒ–é«˜çº§ç¼“å­˜ç³»ç»Ÿ"""
        try:
            # åŠ è½½æŒä¹…åŒ–ç¼“å­˜
            if self.advanced_cache_config['enable_persistent_cache']:
                self._load_persistent_cache()

            # åˆå§‹åŒ–ç¼“å­˜å±‚æ¬¡ç»“æ„
            self._initialize_cache_hierarchy()

            # é¢„çƒ­å¸¸è§é—®é¢˜ç¼“å­˜
            self._preload_common_qa_cache()

            self.logger.info("é«˜çº§ç¼“å­˜ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")

        except Exception as e:
            self.logger.error(f"é«˜çº§ç¼“å­˜ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")

    def _load_persistent_cache(self):
        """åŠ è½½æŒä¹…åŒ–ç¼“å­˜"""
        import os
        import json
        from pathlib import Path

        cache_file = Path(self.advanced_cache_config['cache_file_path'])

        if cache_file.exists():
            try:
                with open(cache_file, 'r', encoding='utf-8') as f:
                    cache_data = json.load(f)

                # æ¢å¤ç¼“å­˜æ•°æ®
                self.analysis_cache.update(cache_data.get('analysis_cache', {}))
                self.semantic_cache['cache_entries'].update(cache_data.get('semantic_cache', {}))
                self.common_qa_cache['qa_pairs'].update(cache_data.get('qa_cache', {}))

                # æ›´æ–°ç¼“å­˜ç»Ÿè®¡
                self.cache_stats['cache_size_bytes'] = cache_data.get('cache_size_bytes', 0)

                self.logger.info(f"åŠ è½½æŒä¹…åŒ–ç¼“å­˜: {len(self.analysis_cache)} é¡¹")

            except Exception as e:
                self.logger.warning(f"åŠ è½½æŒä¹…åŒ–ç¼“å­˜å¤±è´¥: {e}")

    def _save_persistent_cache(self):
        """ä¿å­˜æŒä¹…åŒ–ç¼“å­˜"""
        if not self.advanced_cache_config['enable_persistent_cache']:
            return

        try:
            import json
            from pathlib import Path

            cache_file = Path(self.advanced_cache_config['cache_file_path'])
            cache_file.parent.mkdir(parents=True, exist_ok=True)

            cache_data = {
                'analysis_cache': self.analysis_cache,
                'semantic_cache': self.semantic_cache['cache_entries'],
                'qa_cache': self.common_qa_cache['qa_pairs'],
                'cache_size_bytes': self.cache_stats['cache_size_bytes'],
                'saved_at': self._get_current_time(),
                'version': '1.0'
            }

            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, indent=2, ensure_ascii=False)

            self.logger.debug("æŒä¹…åŒ–ç¼“å­˜å·²ä¿å­˜")

        except Exception as e:
            self.logger.warning(f"ä¿å­˜æŒä¹…åŒ–ç¼“å­˜å¤±è´¥: {e}")

    def _initialize_cache_hierarchy(self):
        """åˆå§‹åŒ–ç¼“å­˜å±‚æ¬¡ç»“æ„"""
        # L1 å†…å­˜ç¼“å­˜ (å·²åœ¨ __init__ ä¸­åˆå§‹åŒ–ä¸º self.analysis_cache)

        # L2 ç£ç›˜ç¼“å­˜
        self.disk_cache_path = '.aidefect_l2_cache'

        # L3 è¯­ä¹‰ç¼“å­˜
        self.semantic_similarity_cache = {}

        # æ¸…ç†è¿‡æœŸçš„æŒä¹…åŒ–ç¼“å­˜
        self._cleanup_expired_persistent_cache()

    def _preload_common_qa_cache(self):
        """é¢„åŠ è½½å¸¸è§é—®ç­”ç¼“å­˜"""
        # é¢„å®šä¹‰ä¸€äº›å¸¸è§çš„é—®ç­”å¯¹
        common_qa = {
            "é«˜å¤æ‚åº¦å‡½æ•°": {
                "question": "å¦‚ä½•å¤„ç†é«˜å¤æ‚åº¦å‡½æ•°ï¼Ÿ",
                "answer": "é«˜å¤æ‚åº¦å‡½æ•°åº”è¯¥è¿›è¡Œé‡æ„ï¼Œè€ƒè™‘ä»¥ä¸‹ç­–ç•¥ï¼š\n1. å°†å¤§å‡½æ•°æ‹†åˆ†ä¸ºå¤šä¸ªå°å‡½æ•°\n2. ä½¿ç”¨è®¾è®¡æ¨¡å¼ç®€åŒ–é€»è¾‘\n3. æå–å…¬å…±é€»è¾‘åˆ°ç‹¬ç«‹æ–¹æ³•\n4. è€ƒè™‘ä½¿ç”¨ç­–ç•¥æ¨¡å¼æˆ–çŠ¶æ€æ¨¡å¼",
                "category": "é‡æ„å»ºè®®",
                "priority": "high"
            },
            "ä»£ç é‡å¤": {
                "question": "å¦‚ä½•æ¶ˆé™¤ä»£ç é‡å¤ï¼Ÿ",
                "answer": "æ¶ˆé™¤ä»£ç é‡å¤çš„æ–¹æ³•ï¼š\n1. æå–å…¬å…±å‡½æ•°æˆ–æ–¹æ³•\n2. ä½¿ç”¨ç»§æ‰¿å’Œå¤šæ€\n3. åˆ›å»ºå·¥å…·ç±»æˆ–è¾…åŠ©å‡½æ•°\n4. ä½¿ç”¨æ¨¡æ¿æ–¹æ³•æ¨¡å¼\n5. è€ƒè™‘ä½¿ç”¨è£…é¥°å™¨",
                "category": "é‡æ„å»ºè®®",
                "priority": "medium"
            },
            "å®‰å…¨æ¼æ´": {
                "question": "å¦‚ä½•ä¿®å¤å¸¸è§å®‰å…¨æ¼æ´ï¼Ÿ",
                "answer": "å¸¸è§å®‰å…¨æ¼æ´ä¿®å¤æ–¹æ³•ï¼š\n1. SQLæ³¨å…¥ï¼šä½¿ç”¨å‚æ•°åŒ–æŸ¥è¯¢\n2. XSSæ”»å‡»ï¼šè¾“å…¥éªŒè¯å’Œè¾“å‡ºç¼–ç \n3. CSRFï¼šä½¿ç”¨ä»¤ç‰ŒéªŒè¯\n4. æƒé™é—®é¢˜ï¼šå®æ–½æœ€å°æƒé™åŸåˆ™\n5. æ•æ„Ÿæ•°æ®ï¼šåŠ å¯†å­˜å‚¨å’Œä¼ è¾“",
                "category": "å®‰å…¨ä¿®å¤",
                "priority": "critical"
            },
            "æ€§èƒ½é—®é¢˜": {
                "question": "å¦‚ä½•ä¼˜åŒ–ä»£ç æ€§èƒ½ï¼Ÿ",
                "answer": "ä»£ç æ€§èƒ½ä¼˜åŒ–ç­–ç•¥ï¼š\n1. å‡å°‘ä¸å¿…è¦çš„è®¡ç®—å’ŒI/Oæ“ä½œ\n2. ä½¿ç”¨ç¼“å­˜æœºåˆ¶\n3. ä¼˜åŒ–ç®—æ³•å’Œæ•°æ®ç»“æ„\n4. å¼‚æ­¥å¤„ç†é•¿æ—¶é—´æ“ä½œ\n5. å‡å°‘å†…å­˜åˆ†é…å’Œåƒåœ¾å›æ”¶\n6. ä½¿ç”¨æ€§èƒ½åˆ†æå·¥å…·å®šä½ç“¶é¢ˆ",
                "category": "æ€§èƒ½ä¼˜åŒ–",
                "priority": "high"
            }
        }

        self.common_qa_cache['qa_pairs'] = common_qa

    def get_smart_cache_result(self, file_path: str, analysis_type: str, user_context: str = "") -> Optional[dict]:
        """æ™ºèƒ½ç¼“å­˜è·å– - æ”¯æŒå¤šçº§ç¼“å­˜å’Œè¯­ä¹‰åŒ¹é…"""
        import time
        start_time = time.time()

        # L1: å†…å­˜ç¼“å­˜æŸ¥æ‰¾
        l1_result = self._get_l1_cache_result(file_path, analysis_type)
        if l1_result:
            self.cache_stats['L1_memory_hits'] += 1
            self._update_cache_retrieval_time(time.time() - start_time)
            return l1_result

        self.cache_stats['L1_memory_misses'] += 1

        # L2: ç£ç›˜ç¼“å­˜æŸ¥æ‰¾
        l2_result = self._get_l2_cache_result(file_path, analysis_type)
        if l2_result:
            self.cache_stats['L2_disk_hits'] += 1
            # æå‡åˆ°L1ç¼“å­˜
            self._cache_result(file_path, analysis_type, l2_result)
            self._update_cache_retrieval_time(time.time() - start_time)
            return l2_result

        self.cache_stats['L2_disk_misses'] += 1

        # L3: è¯­ä¹‰ç¼“å­˜æŸ¥æ‰¾
        if self.advanced_cache_config['semantic_cache_enabled'] and user_context:
            l3_result = self._get_semantic_cache_result(file_path, user_context)
            if l3_result:
                self.cache_stats['L3_semantic_hits'] += 1
                self.cache_stats['semantic_matches'] += 1
                self._update_cache_retrieval_time(time.time() - start_time)
                return l3_result

        self.cache_stats['L3_semantic_misses'] += 1
        self._update_cache_retrieval_time(time.time() - start_time)
        return None

    def _get_l1_cache_result(self, file_path: str, analysis_type: str) -> Optional[dict]:
        """è·å–L1å†…å­˜ç¼“å­˜ç»“æœ"""
        cache_key = self._generate_smart_cache_key(file_path, analysis_type)
        return self._get_cached_result(cache_key)

    def _get_l2_cache_result(self, file_path: str, analysis_type: str) -> Optional[dict]:
        """è·å–L2ç£ç›˜ç¼“å­˜ç»“æœ"""
        import os
        import json
        from pathlib import Path

        try:
            cache_key = self._generate_smart_cache_key(file_path, analysis_type)
            cache_file = Path(self.disk_cache_path) / f"{cache_key}.json"

            if cache_file.exists():
                # æ£€æŸ¥æ–‡ä»¶ä¿®æ”¹æ—¶é—´
                file_mtime = cache_file.stat().st_mtime
                import time
                if time.time() - file_mtime <= self.advanced_cache_config['cache_hierarchy']['L2_disk']['ttl']:

                    with open(cache_file, 'r', encoding='utf-8') as f:
                        cache_data = json.load(f)

                    return cache_data.get('result')
                else:
                    # åˆ é™¤è¿‡æœŸç¼“å­˜
                    cache_file.unlink()

        except Exception as e:
            self.logger.warning(f"L2ç¼“å­˜è¯»å–å¤±è´¥: {e}")

        return None

    def _get_semantic_cache_result(self, file_path: str, user_context: str) -> Optional[dict]:
        """è·å–è¯­ä¹‰ç¼“å­˜ç»“æœ"""
        if not self.semantic_cache['enabled']:
            return None

        try:
            # è®¡ç®—ç”¨æˆ·è¾“å…¥çš„è¯­ä¹‰æŒ‡çº¹
            context_fingerprint = self._generate_semantic_fingerprint(user_context)

            # åœ¨è¯­ä¹‰ç¼“å­˜ä¸­æŸ¥æ‰¾ç›¸ä¼¼æ¡ç›®
            for cached_fingerprint, cached_data in self.semantic_cache['cache_entries'].items():
                similarity = self._calculate_semantic_similarity(context_fingerprint, cached_fingerprint)

                if similarity >= self.semantic_cache['similarity_threshold']:
                    return cached_data['result']

        except Exception as e:
            self.logger.warning(f"è¯­ä¹‰ç¼“å­˜æŸ¥æ‰¾å¤±è´¥: {e}")

        return None

    def cache_smart_result(self, file_path: str, analysis_type: str, result: dict, user_context: str = ""):
        """æ™ºèƒ½ç¼“å­˜å­˜å‚¨ - å¤šçº§ç¼“å­˜ç­–ç•¥"""
        import time

        # ç”Ÿæˆæ™ºèƒ½ç¼“å­˜é”®
        cache_key = self._generate_smart_cache_key(file_path, analysis_type)

        # L1: å†…å­˜ç¼“å­˜
        self._cache_result(cache_key, result)

        # L2: ç£ç›˜ç¼“å­˜ï¼ˆå¼‚æ­¥ï¼‰
        if len(str(result)) < 1024 * 1024:  # å°äº1MBçš„ç»“æœæ‰å­˜ç£ç›˜
            self._cache_to_l2_disk(cache_key, result)

        # L3: è¯­ä¹‰ç¼“å­˜
        if self.advanced_cache_config['semantic_cache_enabled'] and user_context:
            self._cache_to_semantic(user_context, result)

        # æ›´æ–°ç»Ÿè®¡
        self.cache_stats['total_cache_writes'] += 1
        self.cache_stats['cache_size_bytes'] += len(str(result))

        # å®šæœŸä¿å­˜æŒä¹…åŒ–ç¼“å­˜
        if self.cache_stats['total_cache_writes'] % 10 == 0:
            self._save_persistent_cache()

    def _generate_smart_cache_key(self, file_path: str, analysis_type: str) -> str:
        """ç”Ÿæˆæ™ºèƒ½ç¼“å­˜é”®"""
        import hashlib
        import os

        # åŸºç¡€ä¿¡æ¯
        key_components = [
            file_path,
            analysis_type,
            str(self.analysis_config.get('analysis_depth', 'standard')),
            str(self.analysis_config.get('model_selection', 'auto'))
        ]

        # æ–‡ä»¶ä¿®æ”¹æ—¶é—´ï¼ˆå¦‚æœå¯ç”¨æ™ºèƒ½å¤±æ•ˆï¼‰
        if self.cache_invalidation_config['auto_invalidate_on_file_change']:
            try:
                mtime = os.path.getmtime(file_path)
                key_components.append(str(mtime))
            except OSError:
                pass

        # åˆ†æé…ç½®å“ˆå¸Œ
        config_hash = hashlib.md5(str(sorted(self.analysis_config.items())).encode()).hexdigest()[:8]
        key_components.append(config_hash)

        # ç”Ÿæˆæœ€ç»ˆç¼“å­˜é”®
        cache_data = ":".join(key_components)
        return hashlib.md5(cache_data.encode()).hexdigest()

    def _cache_to_l2_disk(self, cache_key: str, result: dict):
        """ç¼“å­˜åˆ°L2ç£ç›˜"""
        import json
        import os
        import time
        from pathlib import Path

        try:
            cache_dir = Path(self.disk_cache_path)
            cache_dir.mkdir(exist_ok=True)

            cache_file = cache_dir / f"{cache_key}.json"

            cache_data = {
                'result': result,
                'timestamp': time.time(),
                'cache_key': cache_key,
                'analysis_config': self.analysis_config
            }

            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, indent=2, ensure_ascii=False)

            # æ£€æŸ¥ç¼“å­˜å¤§å°é™åˆ¶
            self._enforce_disk_cache_size_limit()

        except Exception as e:
            self.logger.warning(f"L2ç£ç›˜ç¼“å­˜å†™å…¥å¤±è´¥: {e}")

    def _cache_to_semantic(self, user_context: str, result: dict):
        """ç¼“å­˜åˆ°è¯­ä¹‰ç¼“å­˜"""
        try:
            import time

            # ç”Ÿæˆè¯­ä¹‰æŒ‡çº¹
            context_fingerprint = self._generate_semantic_fingerprint(user_context)

            # å­˜å‚¨åˆ°è¯­ä¹‰ç¼“å­˜
            cache_entry = {
                'result': result,
                'user_context': user_context[:self.semantic_cache['max_text_length']],
                'timestamp': time.time(),
                'access_count': 1
            }

            self.semantic_cache['cache_entries'][context_fingerprint] = cache_entry

            # é™åˆ¶è¯­ä¹‰ç¼“å­˜å¤§å°
            max_size = self.advanced_cache_config['cache_hierarchy']['L3_semantic']['size_limit']
            if len(self.semantic_cache['cache_entries']) > max_size:
                # åˆ é™¤æœ€æ—§çš„æ¡ç›®
                oldest_fingerprint = min(
                    self.semantic_cache['cache_entries'].keys(),
                    key=lambda k: self.semantic_cache['cache_entries'][k]['timestamp']
                )
                del self.semantic_cache['cache_entries'][oldest_fingerprint]
                self.cache_stats['cache_evictions'] += 1

        except Exception as e:
            self.logger.warning(f"è¯­ä¹‰ç¼“å­˜å†™å…¥å¤±è´¥: {e}")

    def _generate_semantic_fingerprint(self, text: str) -> str:
        """ç”Ÿæˆè¯­ä¹‰æŒ‡çº¹"""
        import hashlib

        # æ–‡æœ¬é¢„å¤„ç†
        processed_text = text.lower().strip()

        # æå–å…³é”®è¯
        keywords = []
        for issue in self.common_qa_cache['common_issues']:
            if issue in processed_text:
                keywords.append(issue)

        # å¦‚æœæ²¡æœ‰åŒ¹é…çš„å…³é”®è¯ï¼Œä½¿ç”¨æ–‡æœ¬å“ˆå¸Œ
        if not keywords:
            return hashlib.md5(processed_text.encode()).hexdigest()[:16]

        # ç”ŸæˆåŸºäºå…³é”®è¯çš„æŒ‡çº¹
        keyword_fingerprint = ":".join(sorted(keywords))
        return hashlib.md5(keyword_fingerprint.encode()).hexdigest()[:16]

    def _calculate_semantic_similarity(self, fingerprint1: str, fingerprint2: str) -> float:
        """è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦"""
        if fingerprint1 == fingerprint2:
            return 1.0

        # ç®€å•çš„ç›¸ä¼¼åº¦è®¡ç®—ï¼ˆå¯ä»¥åç»­å‡çº§ä¸ºæ›´å¤æ‚çš„ç®—æ³•ï¼‰
        common_chars = set(fingerprint1) & set(fingerprint2)
        total_chars = set(fingerprint1) | set(fingerprint2)

        if not total_chars:
            return 0.0

        return len(common_chars) / len(total_chars)

    def _enforce_disk_cache_size_limit(self):
        """å¼ºåˆ¶æ‰§è¡Œç£ç›˜ç¼“å­˜å¤§å°é™åˆ¶"""
        try:
            import os
            from pathlib import Path

            cache_dir = Path(self.disk_cache_path)
            if not cache_dir.exists():
                return

            # è®¡ç®—å½“å‰ç¼“å­˜å¤§å°
            total_size = sum(f.stat().st_size for f in cache_dir.rglob('*.json') if f.is_file())
            max_size_bytes = self.advanced_cache_config['max_cache_size_mb'] * 1024 * 1024

            if total_size > max_size_bytes:
                # åˆ é™¤æœ€æ—§çš„ç¼“å­˜æ–‡ä»¶
                cache_files = list(cache_dir.glob('*.json'))
                cache_files.sort(key=lambda f: f.stat().st_mtime)

                for cache_file in cache_files:
                    try:
                        cache_file.unlink()
                        total_size -= cache_file.stat().st_size
                        self.cache_stats['cache_evictions'] += 1

                        if total_size <= max_size_bytes * 0.8:  # åˆ é™¤åˆ°80%å®¹é‡
                            break
                    except OSError:
                        continue

        except Exception as e:
            self.logger.warning(f"ç£ç›˜ç¼“å­˜å¤§å°é™åˆ¶æ‰§è¡Œå¤±è´¥: {e}")

    def _cleanup_expired_persistent_cache(self):
        """æ¸…ç†è¿‡æœŸçš„æŒä¹…åŒ–ç¼“å­˜"""
        import time
        from pathlib import Path

        try:
            cache_dir = Path(self.disk_cache_path)
            if not cache_dir.exists():
                return

            current_time = time.time()
            l2_ttl = self.advanced_cache_config['cache_hierarchy']['L2_disk']['ttl']

            for cache_file in cache_dir.glob('*.json'):
                try:
                    file_mtime = cache_file.stat().st_mtime
                    if current_time - file_mtime > l2_ttl:
                        cache_file.unlink()
                        self.cache_stats['cache_evictions'] += 1
                except OSError:
                    continue

        except Exception as e:
            self.logger.warning(f"æ¸…ç†è¿‡æœŸç¼“å­˜å¤±è´¥: {e}")

    def _update_cache_retrieval_time(self, retrieval_time: float):
        """æ›´æ–°ç¼“å­˜æ£€ç´¢æ—¶é—´ç»Ÿè®¡"""
        current_avg = self.cache_stats['average_cache_retrieval_time']
        count = self.cache_stats['L1_memory_hits'] + self.cache_stats['L2_disk_hits'] + self.cache_stats['L3_semantic_hits']

        if count > 0:
            self.cache_stats['average_cache_retrieval_time'] = (current_avg * (count - 1) + retrieval_time) / count

    def get_common_qa_suggestion(self, user_input: str) -> Optional[dict]:
        """è·å–å¸¸è§é—®ç­”å»ºè®®"""
        user_lower = user_input.lower()

        # æŸ¥æ‰¾åŒ¹é…çš„å¸¸è§é—®é¢˜
        for issue in self.common_qa_cache['common_issues']:
            if issue in user_lower:
                qa_pair = self.common_qa_cache['qa_pairs'].get(issue)
                if qa_pair:
                    return {
                        'question': qa_pair['question'],
                        'answer': qa_pair['answer'],
                        'category': qa_pair['category'],
                        'priority': qa_pair['priority'],
                        'match_type': 'exact'
                    }

        return None

    def invalidate_cache_for_file(self, file_path: str):
        """ä¸ºç‰¹å®šæ–‡ä»¶å¤±æ•ˆç¼“å­˜"""
        import os

        try:
            # å¤±æ•ˆå†…å­˜ç¼“å­˜
            cache_keys_to_remove = []
            for cache_key, cache_entry in self.analysis_cache.items():
                # ç®€å•çš„æ–‡ä»¶è·¯å¾„åŒ¹é…æ£€æŸ¥
                if file_path in str(cache_entry.get('result', {})):
                    cache_keys_to_remove.append(cache_key)

            for cache_key in cache_keys_to_remove:
                del self.analysis_cache[cache_key]

            # å¤±æ•ˆç£ç›˜ç¼“å­˜
            from pathlib import Path
            import json
            cache_dir = Path(self.disk_cache_path)
            if cache_dir.exists():
                for cache_file in cache_dir.glob('*.json'):
                    try:
                        with open(cache_file, 'r', encoding='utf-8') as f:
                            cache_data = json.load(f)

                        if file_path in str(cache_data.get('result', {})):
                            cache_file.unlink()
                    except:
                        continue

            self.logger.info(f"å·²ä¸ºæ–‡ä»¶ {file_path} å¤±æ•ˆç¼“å­˜")

        except Exception as e:
            self.logger.warning(f"ç¼“å­˜å¤±æ•ˆå¤±è´¥: {e}")

    def get_comprehensive_cache_stats(self) -> dict:
        """è·å–ç»¼åˆç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        total_requests = (
            self.cache_stats['L1_memory_hits'] + self.cache_stats['L1_memory_misses'] +
            self.cache_stats['L2_disk_hits'] + self.cache_stats['L2_disk_misses'] +
            self.cache_stats['L3_semantic_hits'] + self.cache_stats['L3_semantic_misses']
        )

        l1_hit_rate = 0.0
        if self.cache_stats['L1_memory_hits'] + self.cache_stats['L1_memory_misses'] > 0:
            l1_hit_rate = self.cache_stats['L1_memory_hits'] / (self.cache_stats['L1_memory_hits'] + self.cache_stats['L1_memory_misses']) * 100

        l2_hit_rate = 0.0
        if self.cache_stats['L2_disk_hits'] + self.cache_stats['L2_disk_misses'] > 0:
            l2_hit_rate = self.cache_stats['L2_disk_hits'] / (self.cache_stats['L2_disk_hits'] + self.cache_stats['L2_disk_misses']) * 100

        l3_hit_rate = 0.0
        if self.cache_stats['L3_semantic_hits'] + self.cache_stats['L3_semantic_misses'] > 0:
            l3_hit_rate = self.cache_stats['L3_semantic_hits'] / (self.cache_stats['L3_semantic_hits'] + self.cache_stats['L3_semantic_misses']) * 100

        overall_hit_rate = 0.0
        if total_requests > 0:
            overall_hit_rate = (self.cache_stats['L1_memory_hits'] + self.cache_stats['L2_disk_hits'] + self.cache_stats['L3_semantic_hits']) / total_requests * 100

        from pathlib import Path
        l2_items = len(list(Path(self.disk_cache_path).glob('*.json'))) if Path(self.disk_cache_path).exists() else 0

        return {
            'cache_hierarchy_performance': {
                'L1_memory': {
                    'hits': self.cache_stats['L1_memory_hits'],
                    'misses': self.cache_stats['L1_memory_misses'],
                    'hit_rate_percent': round(l1_hit_rate, 2),
                    'current_items': len(self.analysis_cache)
                },
                'L2_disk': {
                    'hits': self.cache_stats['L2_disk_hits'],
                    'misses': self.cache_stats['L2_disk_misses'],
                    'hit_rate_percent': round(l2_hit_rate, 2),
                    'current_items': l2_items
                },
                'L3_semantic': {
                    'hits': self.cache_stats['L3_semantic_hits'],
                    'misses': self.cache_stats['L3_semantic_misses'],
                    'hit_rate_percent': round(l3_hit_rate, 2),
                    'semantic_matches': self.cache_stats['semantic_matches'],
                    'current_items': len(self.semantic_cache['cache_entries'])
                }
            },
            'overall_stats': {
                'total_requests': total_requests,
                'overall_hit_rate_percent': round(overall_hit_rate, 2),
                'total_cache_writes': self.cache_stats['total_cache_writes'],
                'cache_evictions': self.cache_stats['cache_evictions'],
                'cache_size_mb': round(self.cache_stats['cache_size_bytes'] / (1024 * 1024), 2),
                'average_retrieval_time_ms': round(self.cache_stats['average_cache_retrieval_time'] * 1000, 2)
            },
            'qa_cache_stats': {
                'common_issues_count': len(self.common_qa_cache['common_issues']),
                'qa_pairs_count': len(self.common_qa_cache['qa_pairs']),
                'enabled_features': {
                    'persistent_cache': self.advanced_cache_config['enable_persistent_cache'],
                    'semantic_cache': self.advanced_cache_config['semantic_cache_enabled'],
                    'smart_cache_keys': self.advanced_cache_config['smart_cache_key_generation'],
                    'cache_validation': self.advanced_cache_config['cache_validation_enabled']
                }
            }
        }

    def show_advanced_cache_status(self):
        """æ˜¾ç¤ºé«˜çº§ç¼“å­˜çŠ¶æ€"""
        stats = self.get_comprehensive_cache_stats()

        print("\nğŸ—„ï¸ é«˜çº§ç¼“å­˜ç³»ç»ŸçŠ¶æ€")
        print("=" * 50)

        # æ€»ä½“ç»Ÿè®¡
        overall = stats['overall_stats']
        print(f"ğŸ“Š æ€»ä½“ç»Ÿè®¡:")
        print(f"  æ€»è¯·æ±‚æ•°: {overall['total_requests']}")
        print(f"  æ€»å‘½ä¸­ç‡: {overall['overall_hit_rate_percent']}%")
        print(f"  ç¼“å­˜å†™å…¥: {overall['total_cache_writes']}")
        print(f"  ç¼“å­˜å¤§å°: {overall['cache_size_mb']} MB")
        print(f"  å¹³å‡æ£€ç´¢æ—¶é—´: {overall['average_retrieval_time_ms']} ms")

        # å±‚æ¬¡æ€§èƒ½
        hierarchy = stats['cache_hierarchy_performance']
        print(f"\nğŸ—ï¸ ç¼“å­˜å±‚æ¬¡æ€§èƒ½:")

        print(f"  L1 å†…å­˜ç¼“å­˜:")
        print(f"    å‘½ä¸­ç‡: {hierarchy['L1_memory']['hit_rate_percent']}%")
        print(f"    å½“å‰é¡¹: {hierarchy['L1_memory']['current_items']}")

        print(f"  L2 ç£ç›˜ç¼“å­˜:")
        print(f"    å‘½ä¸­ç‡: {hierarchy['L2_disk']['hit_rate_percent']}%")
        print(f"    å½“å‰é¡¹: {hierarchy['L2_disk']['current_items']}")

        print(f"  L3 è¯­ä¹‰ç¼“å­˜:")
        print(f"    å‘½ä¸­ç‡: {hierarchy['L3_semantic']['hit_rate_percent']}%")
        print(f"    è¯­ä¹‰åŒ¹é…: {hierarchy['L3_semantic']['semantic_matches']}")
        print(f"    å½“å‰é¡¹: {hierarchy['L3_semantic']['current_items']}")

        # å¯ç”¨çš„åŠŸèƒ½
        features = stats['qa_cache_stats']['enabled_features']
        print(f"\nâš™ï¸ å¯ç”¨åŠŸèƒ½:")
        for feature, enabled in features.items():
            status = "âœ…" if enabled else "âŒ"
            feature_name = feature.replace('_', ' ').title()
            print(f"  {status} {feature_name}")

        print()